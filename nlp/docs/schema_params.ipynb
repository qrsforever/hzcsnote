{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T07:57:36.308201Z",
     "start_time": "2020-05-20T07:57:36.280483Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T07:57:36.966456Z",
     "start_time": "2020-05-20T07:57:36.311594Z"
    }
   },
   "outputs": [],
   "source": [
    "from k12libs.utils.nb_easy import k12ai_set_notebook, k12ai_get_tooltips\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T07:57:37.008025Z",
     "start_time": "2020-05-20T07:57:36.972098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k12ai_set_notebook(cellw=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T09:29:16.717113Z",
     "start_time": "2020-05-20T09:29:13.274072Z"
    }
   },
   "outputs": [],
   "source": [
    "tables = k12ai_get_tooltips(framework='k12nlp', task='sentiment_analysis', network='basic_classifier', dataset='sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T09:29:17.459603Z",
     "start_time": "2020-05-20T09:29:17.423857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Key | 属性名 | 类型 | 默认值 | 值范围 | 描述 |\n",
       "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
       "|<img width=200/> | <img width=150/> | <img width=100/> | <img width=100/> | <img width=200/> | <img width=300/> |\n",
       "| trainer.shuffle | Shuffle | 布尔型 | True |  | whether or not to shuffle the instances in the iterator | \n",
       " | \n",
       "| dataset_reader.granularity | granularity | 枚举型 | 5-class | ['2-class', '3-class', '5-class'] | indicate the number of sentiment labels to use | \n",
       " | \n",
       "| dataset_reader.lazy | lazy | 布尔型 | False |  | whether or not instances can be read lazily | \n",
       " | \n",
       "| dataset_reader.use_subtrees | use subtrees | 布尔型 | False |  | whether or not to use sentiment-tagged subtrees | \n",
       " | \n",
       "| dataset_reader.token_indexers.tokens.lowercase_tokens | lowercase | 布尔型 | False |  | token convert to lowercase before getting an index for the token from the vocabulary | \n",
       " | \n",
       "| validation_dataset_reader.granularity | granularity | 枚举型 | 5-class | ['2-class', '3-class', '5-class'] | indicate the number of sentiment labels to use | \n",
       " | \n",
       "| validation_dataset_reader.lazy | lazy | 布尔型 | False |  | whether or not instances can be read lazily | \n",
       " | \n",
       "| validation_dataset_reader.use_subtrees | use subtrees | 布尔型 | False |  | whether or not to use sentiment-tagged subtrees | \n",
       " | \n",
       "| validation_dataset_reader.token_indexers.tokens.lowercase_tokens | lowercase | 布尔型 | False |  | token convert to lowercase before getting an index for the token from the vocabulary | \n",
       " | \n",
       "| iterator.batch_size | batch size | 枚举型 | 32 | ['16', '32', '64', '128', '256'] | the size of each batch of instances yeilded | \n",
       " | \n",
       "| iterator.instances_per_epoch | epoch instances | 整型 | 64 | [16, +inf) | each epoch will consist of precisely this many instances | \n",
       " | \n",
       "| iterator.max_instances_in_memory | max instances | 整型 | 128 | [64, +inf) | load this many instances at a time into an in-memory list | \n",
       " | \n",
       "| iterator.cache_instances | cache instances | 布尔型 | False |  | whether or not to cache the tensorized instances in memory | \n",
       " | \n",
       "| validation_iterator.batch_size | batch size | 枚举型 | 32 | ['16', '32', '64', '128', '256'] | the size of each batch of instances yeilded | \n",
       " | \n",
       "| validation_iterator.instances_per_epoch | epoch instances | 整型 | 64 | [16, +inf) | each epoch will consist of precisely this many instances | \n",
       " | \n",
       "| validation_iterator.max_instances_in_memory | max instances | 整型 | 128 | [64, +inf) | load this many instances at a time into an in-memory list | \n",
       " | \n",
       "| validation_iterator.cache_instances | cache instances | 布尔型 | False |  | whether or not to cache the tensorized instances in memory | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.tokens.pretrained_file | Glove | 枚举型 | 6B.200d | ['6B.50d', '6B.100d', '6B.200d', '6B.300d', '840B.300d'] | pretrained file | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.type | LR Scheduler Type | 枚举型 | StepLR | ['StepLR', 'MultiStepLR', 'ExponentialLR', 'ReduceLROnPlateau'] | learning rate schedulers options for decay the learning rate | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.step_size | Step Size | 整型 | 30 | [1, +inf) | period of learning rate decay | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.gamma | Gamma | 浮点型 | 0.1 | [0.001000, 0.999000] | multiplicative factor of learning rate decay | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.milestones | Milestones | 整型数组 | [90, 120] | [int, int, ...] | value like [int, int, ...], the element is one of of epoch indices and all elements is increasing in turn | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.mode | Mode | 枚举型 | min | ['min', 'max'] | in min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing.  | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.factor | Factor | 浮点型 | 0.1 | [0.001000, 0.999000] | factor by which the learning rate will be reduced. new_lr = lr * factor. | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.patience | Patience | 整型 | 10 | [1, +inf) | number of epochs with no improvement after which learning rate will be reduced | \n",
       " | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '''\n",
    "| Key | 属性名 | 类型 | 默认值 | 值范围 | 描述 |\n",
    "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
    "|<img width=200/> | <img width=150/> | <img width=100/> | <img width=100/> | <img width=200/> | <img width=300/> |\n",
    "'''\n",
    "for key, attr, ty, default, rng, tips in tables:\n",
    "    data += f'| {key} | {attr} | {ty} | {default} | {rng} | {tips} | \\n | \\n'\n",
    "    \n",
    "Markdown(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
