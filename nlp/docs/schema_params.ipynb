{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:21:12.597320Z",
     "start_time": "2020-05-24T09:21:12.569099Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:21:13.247176Z",
     "start_time": "2020-05-24T09:21:12.600905Z"
    }
   },
   "outputs": [],
   "source": [
    "from k12libs.utils.nb_easy import k12ai_set_notebook, k12ai_get_tooltips\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:21:13.286959Z",
     "start_time": "2020-05-24T09:21:13.251235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k12ai_set_notebook(cellw=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:21:17.197123Z",
     "start_time": "2020-05-24T09:21:13.290228Z"
    }
   },
   "outputs": [],
   "source": [
    "tables = k12ai_get_tooltips(framework='k12nlp', task='sentiment_analysis', network='basic_classifier', dataset='sst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:21:17.329905Z",
     "start_time": "2020-05-24T09:21:17.204349Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Key | 属性名 | 类型 | 默认值 | 值范围 | 描述 |\n",
       "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
       "|<img width=200/> | <img width=150/> | <img width=100/> | <img width=100/> | <img width=200/> | <img width=300/> |\n",
       "| dataset_reader.granularity | granularity | 枚举型 | 5-class | ['2-class', '3-class', '5-class'] | indicate the number of sentiment labels to use | \n",
       " | \n",
       "| dataset_reader.lazy | lazy | 布尔型 | False |  | whether or not instances can be read lazily | \n",
       " | \n",
       "| dataset_reader.use_subtrees | use subtrees | 布尔型 | True |  | whether or not to use sentiment-tagged subtrees | \n",
       " | \n",
       "| _k12.dataset_reader.single_id.bool | single id | 布尔型 | False |  | represents tokens as single integers | \n",
       " | \n",
       "| dataset_reader.token_indexers.tokens.lowercase_tokens | lowercase | 布尔型 | False |  | token convert to lowercase before getting an index for the token from the vocabulary | \n",
       " | \n",
       "| _k12.dataset_reader.token_indexers.tokens.token_min_padding_length.bool | token min padding | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| _k12.dataset_reader.token_characters.bool | characters | 布尔型 | False |  | represents tokens as lists of character indices | \n",
       " | \n",
       "| _k12.dataset_reader.token_indexers.token_characters.min_padding_length.bool | min padding length | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| _k12.dataset_reader.token_indexers.token_characters.token_min_padding_length.bool | token min padding | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| validation_dataset_reader.granularity | granularity | 枚举型 | 5-class | ['2-class', '3-class', '5-class'] | indicate the number of sentiment labels to use | \n",
       " | \n",
       "| validation_dataset_reader.lazy | lazy | 布尔型 | False |  | whether or not instances can be read lazily | \n",
       " | \n",
       "| validation_dataset_reader.use_subtrees | use subtrees | 布尔型 | False |  | whether or not to use sentiment-tagged subtrees | \n",
       " | \n",
       "| _k12.validation_dataset_reader.single_id.bool | single id | 布尔型 | False |  | represents tokens as single integers | \n",
       " | \n",
       "| validation_dataset_reader.token_indexers.tokens.lowercase_tokens | lowercase | 布尔型 | False |  | token convert to lowercase before getting an index for the token from the vocabulary | \n",
       " | \n",
       "| _k12.validation_dataset_reader.token_indexers.tokens.token_min_padding_length.bool | token min padding | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| _k12.validation_dataset_reader.token_characters.bool | characters | 布尔型 | False |  | represents tokens as lists of character indices | \n",
       " | \n",
       "| _k12.validation_dataset_reader.token_indexers.token_characters.min_padding_length.bool | min padding length | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| _k12.validation_dataset_reader.token_indexers.token_characters.token_min_padding_length.bool | token min padding | 布尔型 | False |  | minimum padding length required for the TokenIndexer | \n",
       " | \n",
       "| data_loader.num_workers | Num Workers | 整型 | 0 | [0, 8] | specify the subprocesses num for data loading | \n",
       " | \n",
       "| data_loader.batch_sampler.batch_size | batch size | 枚举型 | 32 | ['16', '32', '64', '128', '256'] | the size of each batch of instances yeilded | \n",
       " | \n",
       "| data_loader.batch_sampler.padding_noise | padding noise | 浮点型 | 0.1 | [0.001000, 0.999000] | when sorting by padding length, we add a bit of noise to the lengths | \n",
       " | \n",
       "| data_loader.batch_sampler.drop_last | drop last | 布尔型 | False |  | whether or not drop the last batch if its size would be less the batch_size | \n",
       " | \n",
       "| _k12.data_loader.batch_sampler.sorting_keys | sorting keys | 布尔型 | False |  | value like [str, ...] or [[str, str], ...], to bucket inputs into batches, we want to group the instances by padding length, so that we minimize the amount of padding necessary per batch | \n",
       " | \n",
       "| data_loader.batch_size | batch size | 枚举型 | 64 | ['16', '32', '64', '128', '256'] | the size of each batch of instances yeilded | \n",
       " | \n",
       "| data_loader.drop_last | drop last | 布尔型 | False |  | whether or not drop the last batch if its size would be less the batch_size | \n",
       " | \n",
       "| data_loader.shuffle | shuffle | 布尔型 | False |  | reshuffle the data at every epoch | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.tokens.embedding_dim | embedding dim | 整型 | 200 | (-inf, +inf) | the size of each embedding vector | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.tokens.pretrained_file | glove | 枚举型 | 6B.200d | ['6B.50d', '6B.100d', '6B.200d', '6B.300d', '840B.300d'] | path to a file of word vectors to initialize the embedding matrix | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.tokens.trainable | trainable | 布尔型 | False |  | whether or not to optimize the embedding parameters | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.tokens.sparse | sparse | 布尔型 | False |  | whether or not the Pytorch backend should use a sparse representation of the embedding weight | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim | embedding dim | 整型 | 200 | (-inf, +inf) | the size of each embedding vector | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim | cnn dim | 整型 | 16 | (-inf, +inf) | this is the input dimension to the encoder | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.token_characters.encoder.num_filters | num filters | 整型 | 128 | (-inf, +inf) | this is the output dim for each convolutional layer, which is the number of filters learned by that layer. | \n",
       " | \n",
       "| model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes | ngram sizes | 整型数组 | [2, 3, 4, 5] | [int, int, ...] | This specifies both the number of convolutional layers we will create and their sizes | \n",
       " | \n",
       "| model.seq2vec_encoder.embedding_dim | cnn dim | 整型 | 16 | (-inf, +inf) | this is the input dimension to the encoder | \n",
       " | \n",
       "| model.seq2vec_encoder.num_filters | num filters | 整型 | 128 | (-inf, +inf) | this is the output dim for each convolutional layer, which is the number of filters learned by that layer. | \n",
       " | \n",
       "| model.seq2vec_encoder.ngram_filter_sizes | ngram sizes | 整型数组 | [2, 3, 4, 5] | [int, int, ...] | This specifies both the number of convolutional layers we will create and their sizes | \n",
       " | \n",
       "| model.encoder.embedding_dim | cnn dim | 整型 | 16 | (-inf, +inf) | this is the input dimension to the encoder | \n",
       " | \n",
       "| model.encoder.num_filters | num filters | 整型 | 128 | (-inf, +inf) | this is the output dim for each convolutional layer, which is the number of filters learned by that layer. | \n",
       " | \n",
       "| model.encoder.ngram_filter_sizes | ngram sizes | 整型数组 | [2, 3, 4, 5] | [int, int, ...] | This specifies both the number of convolutional layers we will create and their sizes | \n",
       " | \n",
       "| trainer.validation_metric | validation metric | 枚举型 | +accuracy | ['-loss', '+accuracy', '+f1-measure-overall'] | validation metric to measure for whether to stop training using patience | \n",
       " | \n",
       "| trainer.grad_norm | value | 浮点型 | 5.0 | [0.001000, +inf) | if provided, gradient norms will be rescaled to have a maximum of this value | \n",
       " | \n",
       "| trainer.grad_clipping | value | 浮点型 | 1.0 | [0.001000, +inf) | if provided, gradients will be clipped `during the backward pass` to have an (absolute) maximum of this value. | \n",
       " | \n",
       "| trainer.patience | Patience | 整型 | 10 | (-inf, +inf) | number of epochs to be patient before early stopping | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.type | LR Scheduler Type | 枚举型 | StepLR | ['StepLR', 'MultiStepLR', 'ExponentialLR', 'ReduceLROnPlateau'] | learning rate schedulers options for decay the learning rate | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.step_size | Step Size | 整型 | 30 | [1, +inf) | period of learning rate decay | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.gamma | Gamma | 浮点型 | 0.1 | [0.001000, 0.999000] | multiplicative factor of learning rate decay | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.milestones | Milestones | 整型数组 | [90, 120] | [int, int, ...] | value like [int, int, ...], the element is one of of epoch indices and all elements is increasing in turn | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.mode | Mode | 枚举型 | min | ['min', 'max'] | in min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing.  | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.factor | Factor | 浮点型 | 0.1 | [0.001000, 0.999000] | factor by which the learning rate will be reduced. new_lr = lr * factor. | \n",
       " | \n",
       "| trainer.learning_rate_scheduler.patience | Patience | 整型 | 10 | [1, +inf) | number of epochs with no improvement after which learning rate will be reduced | \n",
       " | \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '''\n",
    "| Key | 属性名 | 类型 | 默认值 | 值范围 | 描述 |\n",
    "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
    "|<img width=200/> | <img width=150/> | <img width=100/> | <img width=100/> | <img width=200/> | <img width=300/> |\n",
    "'''\n",
    "for key, attr, ty, default, rng, tips in tables:\n",
    "    data += f'| {key} | {attr} | {ty} | {default} | {rng} | {tips} | \\n | \\n'\n",
    "    \n",
    "Markdown(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
