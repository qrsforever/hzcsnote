{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-canberra",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:32.118763Z",
     "start_time": "2021-05-21T14:10:31.771794Z"
    },
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%watermark -v -p numpy,sklearn,pandas\n",
    "%watermark -v -p cv2,PIL,matplotlib\n",
    "%watermark -v -p torch,torchvision,torchaudio,pytorch_lightning\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.use_jedi = False\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML, Javascript\n",
    "display(HTML('<style>.container { width:%d%% !important; }</style>' % 90))\n",
    "\n",
    "def _IMPORT_(x):\n",
    "    try:\n",
    "        exec(x, globals())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-production",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-divide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:32.277218Z",
     "start_time": "2021-05-21T14:10:32.121372Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Common ###\n",
    "###\n",
    "\n",
    "import os, io, time, random, math, base64\n",
    "\n",
    "_IMPORT_('import numpy as np')\n",
    "_IMPORT_('import pandas as pd')\n",
    "_IMPORT_('from tqdm.notebook import tqdm')\n",
    "\n",
    "\n",
    "###\n",
    "### Torch ###\n",
    "###\n",
    "\n",
    "_IMPORT_('import torch')\n",
    "_IMPORT_('import torch.nn as nn')\n",
    "_IMPORT_('import torch.nn.functional as F')\n",
    "_IMPORT_('import torch.optim as O')\n",
    "_IMPORT_('from torchvision import models as M')\n",
    "_IMPORT_('from torchvision import transforms as T')\n",
    "_IMPORT_('from torch.utils.data import Dataset, DataLoader')\n",
    "\n",
    "\n",
    "###\n",
    "### Viz Model ###\n",
    "###\n",
    "\n",
    "_IMPORT_('import wandb')\n",
    "_IMPORT_('import hiddenlayer as hl')\n",
    "_IMPORT_('from graphviz import Digraph, Source')\n",
    "_IMPORT_('from torchviz import make_dot')\n",
    "_IMPORT_('from torchsummary import summary')\n",
    "\n",
    "\n",
    "###\n",
    "### Display ###\n",
    "###\n",
    "\n",
    "_IMPORT_('import cv2')\n",
    "_IMPORT_('from PIL import Image')\n",
    "_IMPORT_('from torchvision.utils import make_grid')\n",
    "_IMPORT_('import matplotlib.pyplot as plt')\n",
    "_IMPORT_('import plotly')\n",
    "_IMPORT_('import plotly.graph_objects as go')\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=False)\n",
    "\n",
    "def show_video(video_path, width=None, height=None):\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = 'data:video/mp4;base64,' + base64.b64encode(mp4).decode()\n",
    "    return HTML('<video %s %s controls src=\"%s\" type=\"video/mp4\"/>' % (W, H, data_url))\n",
    "\n",
    "def show_image(image_path, width=None, height=None):\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    img = open(image_path, 'rb').read()\n",
    "    data_url = 'data:image/jpg;base64,' + base64.b64encode(img).decode()\n",
    "    return HTML('<img %s %s src=\"%s\"/>' % (W, H, data_url))\n",
    "\n",
    "###\n",
    "### Random Seed ###\n",
    "###\n",
    "\n",
    "def  set_rng_seed(x):\n",
    "    try:\n",
    "        random.seed(x)\n",
    "        np.random.seed(x)\n",
    "        torch.manual_seed(x)\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "set_rng_seed(888)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-massachusetts",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:32.388032Z",
     "start_time": "2021-05-21T14:10:32.280321Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAME_WIDTH, FRAME_HEIGHT = 112, 112\n",
    "NUM_FRAMES = 64\n",
    "NUM_DMODEL = 512\n",
    "REP_OUT_TIME_RATE = 0.12\n",
    "DATASET_PREFIX = '/data/datasets/cv/countix'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-allowance",
   "metadata": {},
   "source": [
    "## Install Depends Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-career",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T11:08:12.052708Z",
     "start_time": "2021-05-14T11:08:06.992358Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!apt install ffmpeg\n",
    "!pip3 install youtube_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-church",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-census",
   "metadata": {},
   "source": [
    "### Countix Dataset Download and Crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-holly",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```\n",
    "    vs           cs                ce            ve\n",
    "    |             |0.5           0.5|             |\n",
    "    |-------|---------|---------|------|----------|\n",
    "            |         |         |      |            \n",
    "            ks       rs        re      ke\n",
    "    \n",
    "vs: the video start\n",
    "ve: the video end\n",
    "ks: the kinetics start\n",
    "ke: the kinetics end\n",
    "rs: repetition start\n",
    "re: repetition end\n",
    "cs: clip video start\n",
    "ce: clip video end \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-helicopter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T09:33:56.672788Z",
     "start_time": "2021-05-21T09:33:56.463693Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import youtube_dl\n",
    "\n",
    "YOUTUBE_PREFIX = 'https://www.youtube.com/watch?v='\n",
    "\n",
    "SOCKS5_PROXY = 'socks5://127.0.0.1:1881'\n",
    "\n",
    "YDL_OPTS = {\n",
    "    'format': 'mp4',\n",
    "    'proxy': SOCKS5_PROXY,\n",
    "    'quiet': True,\n",
    "    'max_filesize': 30000000, # 30MB\n",
    "}\n",
    "\n",
    "def video_download_crop(vid, fps, wh, ss, to, raw_dir, out_dir, force=False):\n",
    "    raw_file = f'{raw_dir}/{vid}.mp4'\n",
    "    out_file = '%s/%s_%010.6f_%010.6f.mp4' % (out_dir, vid, ss, to)\n",
    "\n",
    "    if os.path.exists(out_file):\n",
    "        if force:\n",
    "            os.remove(out_file)\n",
    "        return out_file\n",
    "\n",
    "    if not os.path.exists(raw_file):\n",
    "        YDL_OPTS['outtmpl'] = raw_file\n",
    "        with youtube_dl.YoutubeDL(YDL_OPTS) as ydl:\n",
    "            ydl.download([f'{YOUTUBE_PREFIX}{vid}'])\n",
    "\n",
    "    if os.path.exists(raw_file):\n",
    "        cmd = 'ffmpeg -i %s -v 0 -r %f -s %s -ss %s -to %s -an %s' % (\n",
    "                raw_file, fps, wh, ss, to, out_file)\n",
    "        subprocess.call(cmd, shell=True)\n",
    "        return out_file\n",
    "\n",
    "    return None\n",
    "\n",
    "def data_preprocess(data_prefix, phase, force=False):\n",
    "    df = pd.read_csv(f'{data_prefix}/countix_{phase}.csv')\n",
    "    raw_dir = f'{data_prefix}/raw/{phase}'\n",
    "    out_dir = f'{data_prefix}/{phase}'\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df['file_name'] = None\n",
    "    df['rep_start_frame'] = 0\n",
    "    df['rep_end_frame'] = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if phase == 'test' or phase == 'sample':\n",
    "            vid, ks, ke, rs, re, count, file_name, rsf, rse = row\n",
    "        else:\n",
    "            vid, _, ks, ke, rs, re, count, file_name, rsf, rse = row\n",
    "\n",
    "        interval = re - rs\n",
    "        cs = float(max([ks, rs - REP_OUT_TIME_RATE * interval]))\n",
    "        ce = float(min([ke, re + REP_OUT_TIME_RATE * interval]))\n",
    "        try:\n",
    "            fps = NUM_FRAMES / (ce - cs)\n",
    "            out_file = video_download_crop(vid, \n",
    "                    fps, '%dx%d' % (FRAME_WIDTH, FRAME_HEIGHT), cs, ce, raw_dir, out_dir, force)\n",
    "            if out_file is not None:\n",
    "                cap = cv2.VideoCapture(out_file)\n",
    "                cnt = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "                if cnt >= NUM_FRAMES:\n",
    "                    print('preprocess file: %s: %d, count: %d' % (out_file, cnt, count))\n",
    "                    rsf = int(fps * (rs - cs))\n",
    "                    ref = int(fps * (re - cs))\n",
    "                    if 1 < count < (ref - rsf) // 2:\n",
    "                        df.loc[idx, 'rep_start_frame'] = rsf\n",
    "                        df.loc[idx, 'rep_end_frame'] = ref\n",
    "                        df.loc[idx, 'file_name'] = os.path.basename(out_file)\n",
    "                    else:\n",
    "                        print('[%s] count:[%d] %d %d' % (vid, count, ref, rsf))\n",
    "                else:\n",
    "                    print(f'frames is less than {NUM_FRAMES}')     \n",
    "            else:\n",
    "                print('download or crop [%s] fail' % vid)\n",
    "        except Exception as err:\n",
    "            print('%s' % err)\n",
    "    sub_df = df[df['file_name'].notnull()]\n",
    "    sub_df.to_csv(f'{data_prefix}/sub_countix_{phase}.csv', index=False, header=True)\n",
    "    return sub_df\n",
    "\n",
    "# data_preprocess(DATASET_PREFIX, 'test')\n",
    "# data_preprocess(DATASET_PREFIX, 'val')\n",
    "# data_preprocess(DATASET_PREFIX, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-arnold",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:02.851307Z",
     "start_time": "2021-05-20T12:23:02.274607Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!ls $DATASET_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-festival",
   "metadata": {},
   "source": [
    "### Countix Dataset Sample Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:02.934004Z",
     "start_time": "2021-05-20T12:23:02.854105Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'{DATASET_PREFIX}/sub_countix_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-distribution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:03.773581Z",
     "start_time": "2021-05-20T12:23:02.937603Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "def calculate_period_length(row):\n",
    "    row.period_length = (row.rep_end_frame - row.rep_start_frame) / row['count']\n",
    "    return row\n",
    "df_train['period_length'] = 0\n",
    "df_train = df_train.apply(calculate_period_length, axis=1, result_type='expand')\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-investigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:03.837831Z",
     "start_time": "2021-05-20T12:23:03.776380Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-bracelet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-14T11:08:13.910146Z",
     "start_time": "2021-05-14T11:08:13.843165Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sample_video_item = df_train.iloc[3]\n",
    "print(sample_video_item)\n",
    "sample_video_path = f'{DATASET_PREFIX}/train/{sample_video_item.file_name}'\n",
    "\n",
    "cap = cv2.VideoCapture(sample_video_path)\n",
    "count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "# vwrite = cv2.VideoWriter('/tmp/t3.mp4', fourcc=fourcc, fps=fps, frameSize=(width, height))\n",
    "print(count, fps)\n",
    "frames = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "    # vwrite.write(frame)\n",
    "cap.release()\n",
    "# vwrite.release()\n",
    "print(len(frames))\n",
    "show_video(sample_video_path, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-heath",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T02:52:58.183979Z",
     "start_time": "2021-05-10T02:52:51.622Z"
    }
   },
   "source": [
    "### Countix Dataset Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-diagram",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:10:48.246086Z",
     "start_time": "2021-05-23T10:10:48.134375Z"
    }
   },
   "outputs": [],
   "source": [
    "class CountixDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, phase, frame_size=112, num_frames=64):\n",
    "        self.data_root = data_root\n",
    "        self.phase = phase\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = (frame_size, frame_size) if isinstance(frame_size, int) else frame_size\n",
    "        self.df = pd.read_csv(f'{data_root}/sub_countix_{phase}.csv')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.df.iloc[index]\n",
    "        start = item.rep_start_frame\n",
    "        end = item.rep_end_frame\n",
    "        count = self.df.loc[index, 'count']\n",
    "        \n",
    "        path = f'{self.data_root}/{self.phase}/{item.file_name}'\n",
    "        \n",
    "        period_length = int((end - start) / count)\n",
    "        \n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        flg = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret is False:\n",
    "                if flg == 0:\n",
    "                    print(path)\n",
    "                break\n",
    "            flg = 1\n",
    "                \n",
    "            img = Image.fromarray(frame)\n",
    "            trans = T.Compose([\n",
    "                T.Resize(self.frame_size),\n",
    "                T.ToTensor(),       \n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])])\n",
    "            frames.append(trans(img).unsqueeze(0))\n",
    "        cap.release()\n",
    "        \n",
    "        X = frames[:self.num_frames]\n",
    "        X = torch.cat(X)\n",
    "        \n",
    "        y1 = np.full((self.num_frames, 1), fill_value=period_length)\n",
    "        y2 = np.ones((self.num_frames, 1)) \n",
    "        for i in range(self.num_frames):\n",
    "            if i < start or i > end:\n",
    "                y1[i] = 0\n",
    "                y2[i] = 0\n",
    "                \n",
    "        y1 = torch.FloatTensor(y1) # period length / per frame [2, 3, ..., N/2 ]\n",
    "        y2 = torch.FloatTensor(y2) # periodicity [0, 1]\n",
    "        y3 = torch.FloatTensor([count])\n",
    "        return X, y1, y2, y3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-protein",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:10:49.013107Z",
     "start_time": "2021-05-23T10:10:48.896634Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CountixDataset(DATASET_PREFIX, 'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-norway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:10:51.411348Z",
     "start_time": "2021-05-23T10:10:49.456064Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y1, y2, y3 = next(iter(train_loader))\n",
    "X.shape, y1.shape, y2.shape, y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-gather",
   "metadata": {},
   "source": [
    "## RepNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-harvey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:04.847963Z",
     "start_time": "2021-05-20T12:23:04.790039Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_image('repnet_model.png', width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-restriction",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-cleanup",
   "metadata": {},
   "source": [
    "#### Convolutional Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-steering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:10:55.973717Z",
     "start_time": "2021-05-23T10:10:55.846156Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet50Base5D(nn.Module):\n",
    "    def __init__(self, pretrained=False, m=2):\n",
    "        super().__init__()\n",
    "        base_model = M.resnet50(pretrained=pretrained)\n",
    "        self.m = m\n",
    "        \n",
    "        if m == 1:\n",
    "            # method-1:\n",
    "            base_model.fc = nn.Identity()\n",
    "            base_model.avgpool = nn.Identity()\n",
    "            base_model.layer4 = nn.Identity()\n",
    "            base_model.layer3[3] = nn.Identity()\n",
    "            base_model.layer3[4] = nn.Identity()\n",
    "            base_model.layer3[5] = nn.Identity()\n",
    "            self.base_model = base_model\n",
    "        else:\n",
    "            # method-2:\n",
    "            self.base_model = nn.Sequential(\n",
    "                *list(base_model.children())[:-4],\n",
    "                *list(base_model.children())[-4][:3])\n",
    " \n",
    "    def forward(self, x):\n",
    "        N, S, C, H, W = x.shape \n",
    "        x = x.reshape(-1, C, H, W)  # 5D -> 4D\n",
    "        x = self.base_model(x)\n",
    "        if self.m == 1:\n",
    "            x = x.reshape(N, S, 1024, 7, 7)\n",
    "        else:\n",
    "            x = x.reshape(N, S, x.size(1), x.size(2), x.size(3))  # 4D -> 5D\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-regulation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:10:58.517258Z",
     "start_time": "2021-05-23T10:10:57.877335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet50 = ResNet50Base5D(pretrained=False)\n",
    "resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-weekend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:34.846218Z",
     "start_time": "2021-05-21T14:10:31.783Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet50_outputs = resnet50(X)\n",
    "resnet50_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-circular",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:10.303288Z",
     "start_time": "2021-05-20T12:23:10.154167Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "g = resnet50_outputs[0][:, :1]\n",
    "g = make_grid(g, padding=3)\n",
    "g = g.detach().numpy().transpose((1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.imshow(g, plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-journalist",
   "metadata": {},
   "source": [
    "#### Temporal Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-student",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T09:30:20.669399Z",
     "start_time": "2021-05-23T09:30:20.574290Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemporalContext(nn.Module):\n",
    "    def __init__(self, in_channels=1024, out_channels=512):\n",
    "        super().__init__()\n",
    "        self.conv3D = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=(3, 1, 1),\n",
    "                dilation=(3, 1, 1)),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (N, S, C:1024, H, W) -> (N, C/E, S, H, W): (4, 1024, 64, 7, 7)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv3D(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-reward",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T09:30:23.699485Z",
     "start_time": "2021-05-23T09:30:22.566461Z"
    }
   },
   "outputs": [],
   "source": [
    "TC = TemporalContext()\n",
    "tc_outputs = TC(resnet50_outputs)\n",
    "tc_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-stranger",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-validity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:14:27.468582Z",
     "start_time": "2021-05-21T14:14:27.364670Z"
    }
   },
   "outputs": [],
   "source": [
    "class GlobalMaxPool(nn.Module):\n",
    "    def __init__(self, m=1):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        \n",
    "        # method:2\n",
    "        if m == 2:\n",
    "            self.pool = nn.MaxPool3d(kernel_size = (1, 7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Inputs: (N, C, S, 7, 7)\n",
    "        # method:1\n",
    "        if self.m == 1:\n",
    "            x, _ = torch.max(x, dim=3)\n",
    "            x, _ = torch.max(x, dim=3)\n",
    "        else:\n",
    "            # method:2\n",
    "            x = self.pool(x).squeeze(3).squeeze(3)\n",
    "        return x # (N, S, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-roberts",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:14:28.262458Z",
     "start_time": "2021-05-21T14:14:28.152857Z"
    }
   },
   "outputs": [],
   "source": [
    "GMP = GlobalMaxPool()\n",
    "gmp_outputs = GMP(tc_outputs)\n",
    "gmp_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-difficulty",
   "metadata": {},
   "source": [
    "#### Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-medium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T09:55:38.825400Z",
     "start_time": "2021-05-21T09:55:31.757721Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet50Base5D(pretrained=True)\n",
    "        self.temporal_context = TemporalContext(in_channels=1024, out_channels=512)\n",
    "        self.max_pool = GlobalMaxPool()\n",
    "   \n",
    "    def forward(self,x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.temporal_context(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder_outputs = encoder(X)\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-victory",
   "metadata": {},
   "source": [
    "### Temporal Self-similarity Matrix(TSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-reminder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:38:49.571095Z",
     "start_time": "2021-05-23T10:38:49.450402Z"
    }
   },
   "outputs": [],
   "source": [
    "class TemproalSelfMatrix(nn.Module):\n",
    "    def __init__(self, num_frames=64, temperature=13.544, m=1):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.temperature = temperature\n",
    "        self.register_buffer('zero_value', torch.tensor(0.0))\n",
    "        self.register_buffer('one_value', torch.ones(num_frames))\n",
    "        \n",
    "    def calc_sims(self, x):\n",
    "        # (N, S, E)  --> (N, 1, S, S)\n",
    "        S = x.shape[1]\n",
    "        \n",
    "        I = self.one_value\n",
    "        xr = torch.einsum('nse,h->nhse', (x, I))\n",
    "        xc = torch.einsum('nse,h->nshe', (x, I))\n",
    "        diff = xr - xc\n",
    "        return torch.einsum('nsge,nsge->nsg', (diff, diff))\n",
    "        \n",
    "    def pairwise_l2_distance(self, x):\n",
    "        # (S, E)\n",
    "        a, b = x, x\n",
    "        norm_a = torch.sum(torch.square(a), dim=1)\n",
    "        norm_a = torch.reshape(norm_a, [-1, 1])\n",
    "        norm_b = torch.sum(torch.square(b), dim=1)\n",
    "        norm_b = torch.reshape(norm_b, [1, -1])\n",
    "        b = torch.transpose(b, 0, 1)  # a: 64x512  b: 512x64\n",
    "        dist = torch.maximum(\n",
    "            norm_a - 2.0 * torch.matmul(a, b) + norm_b,\n",
    "            self.zero_value)\n",
    "        return dist\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (N, S, E)\n",
    "        # method: 1\n",
    "        if self.m == 1:\n",
    "            # x = torch.transpose(x, 1, 2)\n",
    "            sims_list = []\n",
    "            for i in range(x.shape[0]):\n",
    "                sims_list.append(self.pairwise_l2_distance(x[i]))\n",
    "            sims = torch.stack(sims_list)\n",
    "        else:\n",
    "            # method: 2\n",
    "            sims = self.calc_sims(x)\n",
    "        \n",
    "        sims = F.softmax(-sims/self.temperature, dim=-1)\n",
    "        sims = sims.unsqueeze(1)\n",
    "        return F.relu(sims) # (N, 1, S, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-farmer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:38:50.848454Z",
     "start_time": "2021-05-23T10:38:50.715007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSM = TemproalSelfMatrix(m=2)\n",
    "tsm_outputs = TSM(gmp_outputs)\n",
    "tsm_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-england",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T12:23:15.572461Z",
     "start_time": "2021-05-20T12:23:11.972595Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def show_hotmap(data):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    xlabel = range(1, data.shape[0] + 1)\n",
    "    ylabel = range(1, data.shape[0] + 1)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.set_xticks(range(len(xlabel)))\n",
    "    ax.set_xticklabels(xlabel, rotation=90)\n",
    "    ax.xaxis.set_ticks_position('top') \n",
    "    \n",
    "    ax.set_yticks(range(len(ylabel)))\n",
    "    ax.set_yticklabels(ylabel)\n",
    "    \n",
    "    im = ax.imshow(data, cmap=plt.cm.PuBu)\n",
    "    plt.colorbar(im)\n",
    "tsm_hotmap_data = tsm_outputs[0][0].detach().numpy()\n",
    "show_hotmap(tsm_hotmap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-questionnaire",
   "metadata": {},
   "source": [
    "### Period Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-mortality",
   "metadata": {},
   "source": [
    "#### Features Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-boundary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:19:02.813418Z",
     "start_time": "2021-05-21T14:19:02.712553Z"
    }
   },
   "outputs": [],
   "source": [
    "TSM_Features = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1,\n",
    "              out_channels=32,\n",
    "              kernel_size=3,\n",
    "              padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25))\n",
    "\n",
    "tsm_f_outputs = TSM_Features(tsm_outputs)\n",
    "tsm_f_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-disposal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:34.868300Z",
     "start_time": "2021-05-21T14:10:31.850Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeaturesProjection(nn.Module):\n",
    "    def __init__(self, num_frames=64, out_features=512):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(num_frames*32, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, 32, S, S] -> [N, S, S, 32]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.reshape(x.size(0), x.size(1), -1) # N, S, 32*S\n",
    "        x = self.projection(x) # N, S, 512\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-taiwan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:21:10.871730Z",
     "start_time": "2021-05-21T14:21:10.725638Z"
    }
   },
   "outputs": [],
   "source": [
    "TSM_FP = FeaturesProjection()\n",
    "tsm_fp_outputs = TSM_FP(tsm_f_outputs)\n",
    "tsm_fp_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-coalition",
   "metadata": {},
   "source": [
    "#### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-applicant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:32:19.239651Z",
     "start_time": "2021-05-21T14:32:19.126564Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # S, 1, d_model\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (S, N, E:512)\n",
    "        x = x + self.pe[:x.size(0), :, :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_frames=64, d_model=512, \n",
    "                 n_head=4, dim_ff=512, dropout=0.2,\n",
    "                 num_layers=2, m=2):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            activation='relu')\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        if m == 1:\n",
    "            self.pos_encoder = PositionalEncoding(d_model, dropout, num_frames)\n",
    "        else:\n",
    "            self.pos_encoder = torch.empty(1, num_frames, 1).normal_(mean=0, std=0.02)\n",
    "            self.pos_encoder.requires_grad = True\n",
    "\n",
    "        self.trans_encoder = nn.TransformerEncoder(encoder_layer, num_layers, encoder_norm)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # [N, S, E]\n",
    "        if self.m == 1:\n",
    "            x = x.transpose(0, 1)\n",
    "            x = self.pos_encoder(x) # S, N, D_Model\n",
    "            x = self.trans_encoder(x)\n",
    "            x = x.transpose(0, 1)\n",
    "        else:\n",
    "            x += self.pos_encoder # N, S, D_Model\n",
    "            x = self.trans_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-laugh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:32:20.166000Z",
     "start_time": "2021-05-21T14:32:20.039108Z"
    }
   },
   "outputs": [],
   "source": [
    "TE = TransformerModel(NUM_FRAMES, d_model=NUM_DMODEL, n_head=4, dropout=0.2, dim_ff=512, m=1)\n",
    "te_outputs = TE(tsm_fp_outputs)\n",
    "te_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-stockholm",
   "metadata": {},
   "source": [
    "#### Period Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-comment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T11:35:58.149178Z",
     "start_time": "2021-05-23T11:35:58.034644Z"
    }
   },
   "outputs": [],
   "source": [
    "class PeriodClassifier(nn.Module):\n",
    "    def __init__(self, num_frames=64, in_features=512, out_features=1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(in_features=in_features, out_features=512),\n",
    "            # nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(in_features=512, out_features=num_frames//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=num_frames//2, out_features=out_features),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-malaysia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T11:35:59.540126Z",
     "start_time": "2021-05-23T11:35:59.426884Z"
    }
   },
   "outputs": [],
   "source": [
    "pc = PeriodClassifier(NUM_FRAMES)\n",
    "pc_outputs = pc(te_outputs)\n",
    "pc_outputs.shape\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-auction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T11:36:49.286419Z",
     "start_time": "2021-05-23T11:36:49.177986Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, _ in pc.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-hotel",
   "metadata": {},
   "source": [
    "### Make All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-thread",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:15:10.386110Z",
     "start_time": "2021-05-23T10:15:10.277671Z"
    }
   },
   "outputs": [],
   "source": [
    "class RepNet(nn.Module):\n",
    "    def __init__(self, num_frames=64, num_dmodel=512):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.resnet50 = ResNet50Base5D(pretrained=True)\n",
    "        self.tcxt = TemporalContext()\n",
    "        self.maxpool = GlobalMaxPool(m=1)\n",
    "        # TSM\n",
    "        self.tsm = TemproalSelfMatrix(num_frames=num_frames, temperature=13.544, m=1)  # noqa\n",
    "\n",
    "        # Period Predictor\n",
    "        self.tsm_features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=3,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25))\n",
    "\n",
    "        self.projection1 = FeaturesProjection(num_frames=num_frames, out_features=num_dmodel)\n",
    "        self.projection2 = FeaturesProjection(num_frames=num_frames, out_features=num_dmodel)\n",
    "\n",
    "        # period length prediction\n",
    "        self.trans1 = TransformerModel(\n",
    "                num_frames, d_model=num_dmodel, n_head=4,\n",
    "                dropout=0.25, dim_ff=num_dmodel, m=1)\n",
    "\n",
    "        self.pc1 = PeriodClassifier(num_frames, num_dmodel)\n",
    "        # periodicity prediction\n",
    "        self.trans2 = TransformerModel(\n",
    "                num_frames, d_model=num_dmodel, n_head=4,\n",
    "                dropout=0.25, dim_ff=num_dmodel, m=1)\n",
    "        self.pc2 = PeriodClassifier(num_frames, num_dmodel)\n",
    "\n",
    "    def forward(self, x, retsim=False):\n",
    "        x = self.resnet50(x)  # [N, 64, 1024, 7, 7]\n",
    "        x = self.tcxt(x)  # [N, 64, 512, 7, 7]\n",
    "        x = self.maxpool(x)  # [N, 64, 512]\n",
    "        x = self.tsm(x)  # [N, 1, 64, 64]\n",
    "        if retsim:\n",
    "            z = x\n",
    "\n",
    "        x = self.tsm_features(x)  # [N, 32, 64, 64]\n",
    "\n",
    "        x1 = self.projection1(x)\n",
    "        x2 = self.projection2(x)\n",
    "\n",
    "        y1 = self.pc1(self.trans1(x1))  # L\n",
    "        y2 = self.pc2(self.trans2(x2))  # P\n",
    "        if retsim:\n",
    "            return y1, y2, z\n",
    "        else:\n",
    "            return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-telescope",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:15:19.158945Z",
     "start_time": "2021-05-23T10:15:11.627568Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "repnet = RepNet()\n",
    "y1, y2 = repnet(X)\n",
    "y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-principal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T10:16:44.625986Z",
     "start_time": "2021-05-23T10:16:44.474449Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, weight in repnet.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-subsection",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-chick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:34.873703Z",
     "start_time": "2021-05-21T14:10:31.868Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CountixDataset(DATASET_PREFIX, 'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=4, shuffle=True, drop_last=True)\n",
    "\n",
    "valid_dataset = CountixDataset(DATASET_PREFIX, 'val')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, num_workers=4, shuffle=False)\n",
    "\n",
    "test_dataset = CountixDataset(DATASET_PREFIX, 'test')\n",
    "test_loader = DataLoader(valid_dataset, batch_size=8, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-medication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T07:00:31.785763Z",
     "start_time": "2021-05-23T07:00:30.468410Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_path = f'{DATASET_PREFIX}/repnet5.pt'\n",
    "device = torch.device(\"cuda\")\n",
    "model = RepNet(NUM_FRAMES, NUM_DMODEL).to(device)\n",
    "\n",
    "optimizer = O.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = O.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = O.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "# scheduler = O.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=0.9)\n",
    "# scheduler = O.lr_scheduler.MultiStepLR(optimizer, milestones=[\n",
    "#         3, 10, 50, 100, 200, 300, 400], gamma=0.6)\n",
    "scheduler = O.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, min_lr=1e-6)\n",
    "criterions = [nn.SmoothL1Loss(), nn.BCEWithLogitsLoss()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-heading",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-23T07:00:47.290532Z",
     "start_time": "2021-05-23T07:00:38.495893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(device, model, pbar, optimizer, criterions, metrics_callback=None):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for X, y1, y2, _ in pbar:\n",
    "        X, y1, y2 = X.to(device), y1.to(device), y2.to(device)\n",
    "        y1_pred, y2_pred = model(X)\n",
    "\n",
    "        loss1 = criterions[0](y1_pred, y1)\n",
    "        loss2 = criterions[1](y2_pred, y2)\n",
    "\n",
    "        # count error\n",
    "        y3_pred = torch.sum((y2_pred > 0) / (y1_pred + 1e-1), 1)\n",
    "        y3_calc = torch.sum((y2 > 0) / (y1 + 1e-1), 1)\n",
    "        loss3 = criterions[0](y3_pred, y3_calc)\n",
    "\n",
    "        loss = 3*loss1 + 5*loss2 + 2*loss3\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "        if metrics_callback is not None:\n",
    "            metrics_callback(\n",
    "                '%.3f' % np.mean(loss_list),\n",
    "                '%.3f' % loss1.item(),\n",
    "                '%.3f' % loss2.item(),\n",
    "                '%.3f' % loss3.item())\n",
    "            \n",
    "        del X, y1, y2, y1_pred, y2_pred\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "\n",
    "def valid(device, model, pbar, criterions, metrics_callback=None):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "        for X, y1, y2, _ in pbar:\n",
    "            X, y1, y2 = X.to(device), y1.to(device), y2.to(device)\n",
    "            y1_pred, y2_pred = model(X)\n",
    "\n",
    "            loss1 = criterions[0](y1_pred, y1)\n",
    "            loss2 = criterions[1](y2_pred, y2)\n",
    "\n",
    "            y3_pred = torch.sum((y2_pred > 0) / (y1_pred + 1e-1), 1)\n",
    "            y3_calc = torch.sum((y2 > 0) / (y1 + 1e-1), 1)\n",
    "            loss3 = criterions[0](y3_pred, y3_calc)\n",
    "\n",
    "            loss = 3*loss1 + 5*loss2 + 2*loss3\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            if metrics_callback is not None:\n",
    "                metrics_callback(\n",
    "                    '%.3f' % np.mean(loss_list),\n",
    "                    '%.3f' % loss1.item(),\n",
    "                    '%.3f' % loss2.item(),\n",
    "                    '%.3f' % loss3.item())\n",
    "\n",
    "            del X, y1, y2, y1_pred, y2_pred\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "\n",
    "def inference(device, model, pbar, metrics_callback=None):\n",
    "    # TODO only one test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y1, y2, y3_true in pbar:\n",
    "            X, y1, y2 = X.to(device), y1.to(device), y2.to(device)\n",
    "            y1_pred, y2_pred = model(X)\n",
    "\n",
    "            y3_pred = torch.round(torch.sum((y2_pred > 0) / (y1_pred + 1e-1), 1))\n",
    "            y3_calc = torch.round(torch.sum((y2 > 0) / (y1 + 1e-1), 1))\n",
    "\n",
    "            if metrics_callback is not None:\n",
    "                metrics_callback(\n",
    "                        y3_pred.cpu().numpy().flatten().astype(int).tolist()[:8],\n",
    "                        y3_calc.cpu().numpy().flatten().astype(int).tolist()[:8],\n",
    "                        y3_true.numpy().flatten().astype(int).tolist()[:8])\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "def train_loop(num_epochs, model, ckpt_path,\n",
    "               train_loader, valid_loader, test_loader,\n",
    "               optimizer, scheduler, criterions, device):\n",
    "\n",
    "    start_epoch = 0\n",
    "    fmode = 'w+'\n",
    "\n",
    "    # load model\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
    "        # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # for state in optimizer.state.values():\n",
    "        #     for k, v in state.items():\n",
    "        #         if isinstance(v, torch.Tensor):\n",
    "        #             state[k] = v.to(device)\n",
    "\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        fmode = 'a+'\n",
    "\n",
    "    # metrics log\n",
    "    metrics_writer = open(f'{DATASET_PREFIX}/metrics.txt', fmode)\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs + start_epoch), ascii=True):\n",
    "        # train\n",
    "        torch.cuda.empty_cache()\n",
    "        with tqdm(train_loader, total=len(train_loader), desc='train', ascii=True) as pbar:\n",
    "            train_loss = train(device, model, pbar, optimizer, criterions,\n",
    "                         lambda loss, loss1, loss2, loss3: pbar.set_postfix(\n",
    "                             epoch=epoch, lr=lr, loss=loss, loss1=loss1, loss2=loss2, loss3=loss3))\n",
    "\n",
    "            metrics_writer.write('{}\\n'.format(pbar))\n",
    "\n",
    "        # valid\n",
    "        torch.cuda.empty_cache()\n",
    "        with tqdm(valid_loader, desc='valid', ascii=True) as pbar:\n",
    "            valid_loss = valid(device, model, pbar, criterions,\n",
    "                         lambda loss, loss1, loss2, loss3: pbar.set_postfix(\n",
    "                             epoch=epoch, lr=lr, loss=loss, loss1=loss1, loss2=loss2, loss3=loss3))\n",
    "            metrics_writer.write('{}\\n'.format(pbar))\n",
    "\n",
    "        # inference\n",
    "        torch.cuda.empty_cache()\n",
    "        with tqdm(test_loader, desc='inference test', ascii=True) as pbar:\n",
    "            inference(device, model, pbar,\n",
    "                      lambda y_pred, y_calc, y_true: pbar.set_postfix(\n",
    "                          y_pred=y_pred, y_calc=y_calc, y_true=y_true))\n",
    "            metrics_writer.write('{}\\n'.format(pbar))\n",
    "        with tqdm(train_loader, desc='inference train', ascii=True) as pbar:\n",
    "            inference(device, model, pbar,\n",
    "                      lambda y_pred, y_calc, y_true: pbar.set_postfix(\n",
    "                          y_pred=y_pred, y_calc=y_calc, y_true=y_true))\n",
    "            metrics_writer.write('{}\\n'.format(pbar))\n",
    "\n",
    "        # update learning rate\n",
    "        if isinstance(scheduler, O.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(valid_loss)\n",
    "            lr = '%.7f' % scheduler._last_lr[0]\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            lr = '%.7f' % scheduler.get_last_lr()[0]\n",
    "\n",
    "        metrics_writer.flush()\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, ckpt_path)\n",
    "\n",
    "    metrics_writer.close()\n",
    "    \n",
    "train_loop(1000, model, '/data/last3_0.0001000.pt',\n",
    "           train_loader, valid_loader, test_loader,\n",
    "           optimizer, scheduler, criterions, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-major",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://arxiv.org/pdf/2006.15418.pdf\n",
    "\n",
    "2. https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb#scrollTo=76L5XFonl_Bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-printing",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-smell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T14:10:34.877334Z",
     "start_time": "2021-05-21T14:10:31.885Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "fm_int = '[-+]?\\d+'\n",
    "fm_flt = '[-+]?[0-9]+\\.[0-9]+'\n",
    "\n",
    "train_mdata_list = []\n",
    "valid_mdata_list = []\n",
    "\n",
    "def parse_log(mdata_list, phase, log):\n",
    "    p_epoch = 'epoch=(?P<epoch>%s)' % fm_int\n",
    "    p_loss = 'loss=(?P<loss>%s)' % fm_flt\n",
    "    p_loss1 = 'loss1=(?P<loss1>%s)' % fm_flt\n",
    "    p_loss2 = 'loss2=(?P<loss2>%s)' % fm_flt\n",
    "    p_loss3 = 'loss3=(?P<loss3>%s)' % fm_flt\n",
    "    p_lr = 'lr=(?P<lr>.*[^\\]])'\n",
    "    resdata = re.search(r'%s: .*, %s, %s, %s, %s, %s, %s' % (\n",
    "        phase, p_epoch, p_loss, p_loss1, p_loss2, p_loss3, p_lr), log)\n",
    "\n",
    "    if resdata:\n",
    "        grpdata = resdata.groupdict()\n",
    "        mdata_list.append({\n",
    "            'epoch': int(grpdata['epoch']),\n",
    "            'loss': float(grpdata['loss']),\n",
    "            'loss1': float(grpdata['loss1']),\n",
    "            'loss2': float(grpdata['loss2']),\n",
    "            'loss3': float(grpdata['loss3']),\n",
    "            'lr': float(grpdata['lr'])\n",
    "        })\n",
    "\n",
    "with open('/data/metrics3.txt') as fr:\n",
    "    for line in fr.read().split('\\n'):\n",
    "        parse_log(train_mdata_list, 'train', line)\n",
    "        parse_log(valid_mdata_list, 'valid', line)\n",
    "        \n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = [x['epoch'] for x in train_mdata_list],\n",
    "    y = [x['loss'] for x in train_mdata_list],\n",
    "    text = ['lr: %.6f' % x['lr'] for x in valid_mdata_list],\n",
    "    mode = 'lines',\n",
    "    name = 'train loss'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = [x['epoch'] for x in valid_mdata_list],\n",
    "    y = [x['loss'] for x in valid_mdata_list],\n",
    "    text = ['lr: %.6f' % x['lr'] for x in valid_mdata_list],\n",
    "    mode = 'lines',\n",
    "    name = 'valid loss'\n",
    "))\n",
    "\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "299.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
