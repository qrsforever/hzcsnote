{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hairy-juice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:42:22.864719Z",
     "start_time": "2021-06-01T06:42:14.932490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.9\n",
      "IPython 7.16.1\n",
      "\n",
      "numpy 1.18.5\n",
      "sklearn 0.24.0\n",
      "pandas 1.1.5\n",
      "CPython 3.6.9\n",
      "IPython 7.16.1\n",
      "\n",
      "cv2 4.5.1\n",
      "PIL 6.2.2\n",
      "matplotlib 3.3.3\n",
      "CPython 3.6.9\n",
      "IPython 7.16.1\n",
      "\n",
      "torch 1.8.0.dev20210103+cu101\n",
      "torchvision 0.9.0.dev20210103+cu101\n",
      "torchaudio not installed\n",
      "pytorch_lightning 1.2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%watermark -v -p numpy,sklearn,pandas\n",
    "%watermark -v -p cv2,PIL,matplotlib\n",
    "%watermark -v -p torch,torchvision,torchaudio,pytorch_lightning\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.use_jedi = False\n",
    "\n",
    "from IPython.display import display, HTML, Javascript\n",
    "display(HTML('<style>.container { width:%d%% !important; }</style>' % 90))\n",
    "\n",
    "def _IMPORT_(x):\n",
    "    try:\n",
    "        exec(x, globals())\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greenhouse-liechtenstein",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:42:43.085277Z",
     "start_time": "2021-06-01T06:42:41.465292Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Common ###\n",
    "###\n",
    "\n",
    "import sys, os, io, time, random, math\n",
    "import json, base64, requests\n",
    "\n",
    "_IMPORT_('import numpy as np')\n",
    "_IMPORT_('import pandas as pd')\n",
    "_IMPORT_('from tqdm.notebook import tqdm')\n",
    "\n",
    "def print_progress_bar(x):\n",
    "    print('\\r', end='')\n",
    "    print('Progress: {}%:'.format(x), '%s%s' % ('â–‹'*(x//2), '.'*((100-x)//2)), end='')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "###\n",
    "### Torch ###\n",
    "###\n",
    "\n",
    "_IMPORT_('import torch')\n",
    "_IMPORT_('import torch.nn as nn')\n",
    "_IMPORT_('import torch.nn.functional as F')\n",
    "_IMPORT_('import torch.optim as O')\n",
    "_IMPORT_('from torchvision import models as M')\n",
    "_IMPORT_('from torchvision import transforms as T')\n",
    "_IMPORT_('from torch.utils.data import Dataset, DataLoader')\n",
    "\n",
    "###\n",
    "### Display ###\n",
    "###\n",
    "\n",
    "_IMPORT_('import cv2')\n",
    "_IMPORT_('from PIL import Image')\n",
    "_IMPORT_('from torchvision.utils import make_grid')\n",
    "_IMPORT_('import matplotlib.pyplot as plt')\n",
    "_IMPORT_('import plotly')\n",
    "_IMPORT_('import plotly.graph_objects as go')\n",
    "\n",
    "# plotly.offline.init_notebook_mode(connected=False)\n",
    "\n",
    "def show_video(video_path, width=None, height=None):\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    if video_path.startswith('http'):\n",
    "        data_url = video_path\n",
    "    else:\n",
    "        mp4 = open(video_path, 'rb').read()\n",
    "        data_url = 'data:video/mp4;base64,' + base64.b64encode(mp4).decode()\n",
    "    return HTML('<video %s %s controls src=\"%s\" type=\"video/mp4\"/>' % (W, H, data_url))\n",
    "\n",
    "def show_image(image_path, width=None, height=None):\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    if image_path.startswith('http'):\n",
    "        data_url = image_path\n",
    "    else:\n",
    "        img = open(image_path, 'rb').read()\n",
    "        data_url = 'data:image/jpg;base64,' + base64.b64encode(img).decode()\n",
    "    return HTML('<img %s %s src=\"%s\"/>' % (W, H, data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cultural-amplifier",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:55:49.075336Z",
     "start_time": "2021-06-01T06:55:48.947376Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from random import randrange, randint\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "involved-accounting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T07:04:06.543227Z",
     "start_time": "2021-06-01T07:04:06.389191Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT = '/data/datasets/cv/repnet_datasets'\n",
    "\n",
    "frame_per_vid = 64\n",
    "\n",
    "synthvids = glob.glob(f'{DATASET_ROOT}/synthvids/train*.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "curious-confirmation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T07:10:14.525830Z",
     "start_time": "2021-06-01T07:10:14.319711Z"
    }
   },
   "outputs": [],
   "source": [
    "class miniDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, path_to_video):\n",
    "        \n",
    "        self.path = path_to_video\n",
    "        self.df = df.reset_index()\n",
    "        self.count = self.df.loc[0, 'count']\n",
    "        print(self.count)\n",
    "\n",
    "    def getFrames(self, path = None):\n",
    "        \"\"\"returns frames\"\"\"\n",
    "    \n",
    "        frames = []\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        \n",
    "        cap = cv2.VideoCapture(path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret is False:\n",
    "                break\n",
    "            \n",
    "            img = Image.fromarray(frame)\n",
    "            frames.append(img)\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        curFrames = self.getFrames()\n",
    "        \n",
    "        output_len = min(len(curFrames), randint(44, 64))\n",
    "                \n",
    "        newFrames = []\n",
    "        for i in range(1, output_len + 1):\n",
    "            newFrames.append(curFrames[i * len(curFrames)//output_len  - 1])\n",
    "\n",
    "        a = randint(0, 64 - output_len)\n",
    "        b = 64 - output_len - a\n",
    "        \n",
    "        randpath = random.choice(synthvids)\n",
    "        randFrames = self.getFrames(randpath)\n",
    "        newRandFrames = []\n",
    "        for i in range(1, a + b + 1):\n",
    "            newRandFrames.append(randFrames[i * len(randFrames)//(a+b)  - 1])\n",
    "        \n",
    "        same = np.random.choice([0, 1], p = [0.5, 0.5])\n",
    "        if same:\n",
    "            finalFrames = [newFrames[0] for i in range(a)]\n",
    "            finalFrames.extend( newFrames )        \n",
    "            finalFrames.extend([newFrames[-1] for i in range(b)] )\n",
    "        else:\n",
    "            finalFrames = newRandFrames[:a]\n",
    "            finalFrames.extend( newFrames )        \n",
    "            finalFrames.extend( newRandFrames[a:] )\n",
    "\n",
    "        Xlist = []\n",
    "        for img in finalFrames:\n",
    "        \n",
    "            preprocess = T.Compose([\n",
    "                T.Resize((112, 112)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "            frameTensor = preprocess(img).unsqueeze(0)\n",
    "            Xlist.append(frameTensor)\n",
    "        \n",
    "        Xlist = [Xlist[i] if a<i<(64-b) else torch.nn.functional.dropout(Xlist[i], 0.2) for i in range(64)]  \n",
    "        X = torch.cat(Xlist)\n",
    "        y = [0 for i in range(0,a)]\n",
    "        y.extend([output_len/self.count if 1<output_len/self.count<32 else 0 for i in range(0, output_len)])\n",
    "        \n",
    "        y.extend( [ 0 for i in range(0, b)] )\n",
    "        y = torch.FloatTensor(y).unsqueeze(-1)\n",
    "        \n",
    "        return X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "class dataset_with_indices(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Modifies the given Dataset class to return a tuple data, target, index\n",
    "    instead of just data, target.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, y = self.ds[index]\n",
    "        return X, y, index\n",
    "    \n",
    "    def getPeriodDist(self):\n",
    "        arr = np.zeros(32,)\n",
    "        \n",
    "        for i in tqdm(range(self.__len__())):\n",
    "            _, p,_ = self.__getitem__(i)\n",
    "            per = max(p)\n",
    "            arr[per] += 1\n",
    "        return arr\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "TEST_COUNT = 3\n",
    "\n",
    "def getCombinedDataset(dfPath, videoDir, videoPrefix):\n",
    "    df = pd.read_csv(dfPath)\n",
    "    path_prefix = videoDir + '/' + videoPrefix\n",
    "    \n",
    "    files_present = []\n",
    "    for i in range(0, len(df)):\n",
    "        path_to_video = path_prefix + str(i) + '.mp4'\n",
    "        if os.path.exists(path_to_video):\n",
    "            files_present.append(i)\n",
    "        if i > TEST_COUNT:\n",
    "            break\n",
    "\n",
    "    df = df.iloc[files_present]\n",
    "    \n",
    "    print(files_present)\n",
    "    \n",
    "    miniDatasetList = []\n",
    "    for i in range(0, len(df)):\n",
    "        dfi = df.iloc[[i]]\n",
    "        path_to_video = path_prefix + str(dfi.index.item()) +'.mp4'\n",
    "        miniDatasetList.append(miniDataset(dfi, path_to_video))\n",
    "        \n",
    "    megaDataset = dataset_with_indices(ConcatDataset(miniDatasetList))\n",
    "    return megaDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "accepting-underground",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T07:10:15.435983Z",
     "start_time": "2021-06-01T07:10:15.159422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "/data/datasets/cv/repnet_datasets/trainvids/train0.mp4\n",
      "20\n",
      "/data/datasets/cv/repnet_datasets/trainvids/train1.mp4\n",
      "12\n",
      "/data/datasets/cv/repnet_datasets/trainvids/train2.mp4\n",
      "8\n",
      "/data/datasets/cv/repnet_datasets/trainvids/train3.mp4\n",
      "3\n",
      "/data/datasets/cv/repnet_datasets/trainvids/train4.mp4\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "trainDatasetC = getCombinedDataset(f'{DATASET_ROOT}/countix/countix_train.csv',\n",
    "                                   f'{DATASET_ROOT}/trainvids',\n",
    "                                   'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tender-terrorism",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T07:10:40.500492Z",
     "start_time": "2021-06-01T07:10:16.213197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "/data/datasets/cv/repnet_datasets/synthvids/train922.mp4\n",
      "aaa\n",
      "58\n",
      "/data/datasets/cv/repnet_datasets/synthvids/train704.mp4\n",
      "aaa\n",
      "61\n",
      "/data/datasets/cv/repnet_datasets/synthvids/train122.mp4\n",
      "aaa\n",
      "44\n",
      "/data/datasets/cv/repnet_datasets/synthvids/train242.mp4\n",
      "aaa\n",
      "51\n",
      "/data/datasets/cv/repnet_datasets/synthvids/train652.mp4\n",
      "aaa\n"
     ]
    }
   ],
   "source": [
    "for a in trainDatasetC:\n",
    "    print('aaa')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
