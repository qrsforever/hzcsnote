{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cac9a6b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1><span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-&amp;-Tools\" data-toc-modified-id=\"Import-&amp;-Tools-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import &amp; Tools</a></span></li><li><span><a href=\"#Repnet-Model\" data-toc-modified-id=\"Repnet-Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Repnet Model</a></span></li><li><span><a href=\"#Load-Model\" data-toc-modified-id=\"Load-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modify-Add-OP-Shape-Align(Dim-Tile)\" data-toc-modified-id=\"Modify-Add-OP-Shape-Align(Dim-Tile)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Modify Add OP Shape Align(Dim Tile)</a></span></li><li><span><a href=\"#Simple-Conv3D-Model\" data-toc-modified-id=\"Simple-Conv3D-Model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Simple Conv3D Model</a></span></li></ul></li><li><span><a href=\"#Frozen-Graph\" data-toc-modified-id=\"Frozen-Graph-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Frozen Graph</a></span></li><li><span><a href=\"#Saved-Model\" data-toc-modified-id=\"Saved-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Saved Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modify-Transpose-Perm-Attributes(FusedBatchNormV3)\" data-toc-modified-id=\"Modify-Transpose-Perm-Attributes(FusedBatchNormV3)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Modify Transpose Perm Attributes(FusedBatchNormV3)</a></span></li><li><span><a href=\"#Original-Model-Outputs-VS-Modified-Model-Outputs\" data-toc-modified-id=\"Original-Model-Outputs-VS-Modified-Model-Outputs-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Original Model Outputs VS Modified Model Outputs</a></span></li><li><span><a href=\"#Onnx-Optimizer\" data-toc-modified-id=\"Onnx-Optimizer-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Onnx Optimizer</a></span></li><li><span><a href=\"#Fix-Unsqueeze-Op-(axis-attr)-Error-(TF--->-Onnx--->-OpenVINO)\" data-toc-modified-id=\"Fix-Unsqueeze-Op-(axis-attr)-Error-(TF--->-Onnx--->-OpenVINO)-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Fix Unsqueeze Op (axis attr) Error (TF --&gt; Onnx --&gt; OpenVINO)</a></span></li><li><span><a href=\"#Repnet-Inference\" data-toc-modified-id=\"Repnet-Inference-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Repnet Inference</a></span></li><li><span><a href=\"#Calculate-Count\" data-toc-modified-id=\"Calculate-Count-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Calculate Count</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inv-Period-Confidence-Score\" data-toc-modified-id=\"Inv-Period-Confidence-Score-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Inv-Period Confidence Score</a></span></li><li><span><a href=\"#Within-Period-Confience-Scores\" data-toc-modified-id=\"Within-Period-Confience-Scores-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Within Period Confience Scores</a></span></li><li><span><a href=\"#Period-Count\" data-toc-modified-id=\"Period-Count-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>Period Count</a></span></li></ul></li></ul></li><li><span><a href=\"#TFlite\" data-toc-modified-id=\"TFlite-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TFlite</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#FAQs\" data-toc-modified-id=\"FAQs-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>FAQs</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b7e37",
   "metadata": {},
   "source": [
    "## Import & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "piano-stereo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:00.362689Z",
     "start_time": "2021-08-31T10:44:55.869241Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.19.5\n",
      "sklearn 0.24.2\n",
      "pandas 1.1.5\n",
      "cv2 4.5.3\n",
      "PIL 8.2.0\n",
      "matplotlib 3.3.4\n",
      "torch 1.9.0\n",
      "torchvision 0.10.0\n",
      "torchaudio not installed\n",
      "tensorflow 2.6.0\n",
      "tensorboard 2.6.0\n",
      "onnx 1.10.1\n",
      "onnxruntime 1.8.1\n",
      "tf2onnx 1.9.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%watermark -p numpy,sklearn,pandas\n",
    "%watermark -p cv2,PIL,matplotlib\n",
    "%watermark -p torch,torchvision,torchaudio\n",
    "%watermark -p tensorflow,tensorboard\n",
    "%watermark -p onnx,onnxruntime,tf2onnx\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "%config IPCompleter.use_jedi = False\n",
    "\n",
    "from IPython.display import display, Markdown, HTML, Javascript\n",
    "display(HTML('<style>.container { width:%d%% !important; }</style>' % 80))\n",
    "\n",
    "import sys, os, io, time, random, math\n",
    "import json, base64, requests\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def _IMPORT_(x):\n",
    "    try:\n",
    "        segs = x.split(' ')\n",
    "        g = globals()\n",
    "        if 'github.com' in segs[1]:\n",
    "            uri = segs[1].replace('github.com', 'raw.githubusercontent.com')\n",
    "            mod = uri.split('/')\n",
    "            for s in ['main', 'master']:\n",
    "                uri = 'https://' + '/'.join(mod[:-1]) + '/main/' + mod[-1] + '.py'\n",
    "                x = requests.get(uri)\n",
    "                if x.status_code == 200:\n",
    "                    x = x.text\n",
    "                    break\n",
    "        elif 'gitee.com' in segs[1]:\n",
    "            mod = segs[1].split('/')\n",
    "            for s in ['/raw/main/', '/raw/master/']:\n",
    "                uri = 'https://' + '/'.join(mod[:3]) + s + '/'.join(mod[3:]) + '.py'\n",
    "                x = requests.get(uri)\n",
    "                if x.status_code == 200:\n",
    "                    x = x.text\n",
    "                    break\n",
    "        elif segs[1][0] == '/':\n",
    "            with open(segs[1] + '.py') as fr:\n",
    "                x = fr.read()\n",
    "        exec(x, g)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1506b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(imgsrc, width=None, height=None):\n",
    "    if isinstance(imgsrc, np.ndarray):\n",
    "        img = imgsrc\n",
    "        if width or height:\n",
    "            if width and height:\n",
    "                size = (width, height)\n",
    "            else:\n",
    "                rate = img.shape[1] / img.shape[0]\n",
    "                if width:\n",
    "                    size = (width, int(width/rate))\n",
    "                else:\n",
    "                    size = (int(height*rate), height)\n",
    "            img = cv2.resize(img, size)\n",
    "            plt.figure(figsize=(3*int(size[0]/80+1), 3*int(size[1]/80+1)), dpi=80)\n",
    "        plt.axis('off')\n",
    "        if len(img.shape) > 2:\n",
    "            plt.imshow(img);\n",
    "        else:\n",
    "            plt.imshow(img, cmap='gray');\n",
    "        return\n",
    "\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    if imgsrc.startswith('http'):\n",
    "        data_url = imgsrc\n",
    "    else:\n",
    "        if len(imgsrc) > 2048:\n",
    "            data_url = 'data:image/jpg;base64,' + imgsrc\n",
    "        else:\n",
    "            img = open(imgsrc, 'rb').read()\n",
    "            data_url = 'data:image/jpg;base64,' + base64.b64encode(img).decode()\n",
    "    return HTML('<center><img %s %s src=\"%s\"/></center>' % (W, H, data_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3e1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(vidsrc, width=None, height=None):\n",
    "    W, H = '', ''\n",
    "    if width:\n",
    "        W = 'width=%d' % width\n",
    "    if height:\n",
    "        H = 'height=%d' % height\n",
    "    if vidsrc.startswith('http'):\n",
    "        data_url = vidsrc\n",
    "    else:\n",
    "        mp4 = open(vidsrc, 'rb').read()\n",
    "        data_url = 'data:video/mp4;base64,' + base64.b64encode(mp4).decode()\n",
    "    return HTML('<center><video %s %s controls src=\"%s\" type=\"video/mp4\"/></center>' % (W, H, data_url))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unable-velvet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:00.426874Z",
     "start_time": "2021-08-31T10:45:00.366077Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic, register_line_magic, register_cell_magic\n",
    "def display_html(port, height=600):\n",
    "    from IPython import display\n",
    "    from html import escape as html_escape\n",
    "    frame_id = 'tensorboard-frame-{:08x}'.format(random.getrandbits(64))\n",
    "    shell = '''\n",
    "      <iframe id='%HTML_ID%' width='100%' height='%HEIGHT%' frameborder='0'>\n",
    "      </iframe>\n",
    "      <script>\n",
    "        (function() {\n",
    "          const frame = document.getElementById(%JSON_ID%);\n",
    "          const url = new URL(%URL%, window.location);\n",
    "          const port = %PORT%;\n",
    "          if (port) {\n",
    "            url.port = port;\n",
    "          }\n",
    "          frame.src = url;\n",
    "        })();\n",
    "      </script>\n",
    "    '''\n",
    "    replacements = [\n",
    "        ('%HTML_ID%', html_escape(frame_id, quote=True)),\n",
    "        ('%JSON_ID%', json.dumps(frame_id)),\n",
    "        ('%HEIGHT%', '%d' % height),\n",
    "        ('%PORT%', '%d' % port),\n",
    "        ('%URL%', json.dumps('/')),\n",
    "    ]\n",
    "    for (k, v) in replacements:\n",
    "        shell = shell.replace(k, v)\n",
    "    display.display(display.HTML(shell))\n",
    "\n",
    "@register_line_cell_magic\n",
    "def template_writefile(line, cell):\n",
    "    with open(line, 'w') as fw:\n",
    "        fw.write(cell.format(**globals()))\n",
    "\n",
    "@register_line_magic\n",
    "def netron(line):\n",
    "    args = line.split()\n",
    "    file, port, height = args[0], int(args[1]), 600\n",
    "    if len(args) == 3:\n",
    "        height = int(args[2])\n",
    "    # res = !lsof -i:$port | grep $port\n",
    "    # if len(res) == 1:\n",
    "    #     pid = int(res[0].split(' ')[1])\n",
    "    #     !kill -9 $pid\n",
    "    import netron\n",
    "    try:\n",
    "        netron.start(file, address=('0.0.0.0', port), browse=False)\n",
    "        display_html(port, height)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7cfec991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:00.485773Z",
     "start_time": "2021-08-31T10:45:00.429188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.lite.python.util import run_graph_optimizations, get_grappler_config\n",
    "import onnx\n",
    "import onnx.helper as OH\n",
    "import onnxruntime as rt\n",
    "from onnxsim import simplify\n",
    "from onnx.numpy_helper import to_array, from_array\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38538d9c",
   "metadata": {},
   "source": [
    "## Repnet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e6914",
   "metadata": {},
   "source": [
    "Repnet模型转换为onnx进行推理时, 存在两个问题:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <li> 加法算子操作输入shape不一致问题(pos_encoding)</li>\n",
    "    <li> 3D卷积(5-D)后进行BatchNormlization时, 不支持5D</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-execution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:19:00.584468Z",
     "start_time": "2021-08-31T14:19:00.449511Z"
    }
   },
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "regularizers = tf.keras.regularizers\n",
    "\n",
    "\n",
    "def get_sims(embs, temperature):\n",
    "    \"\"\"Calculates self-similarity between batch of sequence of embeddings.\"\"\"\n",
    "    batch_size = tf.shape(embs)[0]\n",
    "    seq_len = tf.shape(embs)[1]\n",
    "    embs = tf.reshape(embs, [batch_size, seq_len, -1])\n",
    "\n",
    "    def _get_sims(embs):\n",
    "        \"\"\"Calculates self-similarity between sequence of embeddings.\"\"\"\n",
    "        dist = pairwise_l2_distance(embs, embs)\n",
    "        sims = -1.0 * dist\n",
    "        return sims\n",
    "\n",
    "    sims = tf.map_fn(_get_sims, embs)\n",
    "    sims /= temperature\n",
    "    sims = tf.nn.softmax(sims, axis=-1)\n",
    "    sims = tf.expand_dims(sims, -1)\n",
    "    return sims\n",
    "\n",
    "\n",
    "def flatten_sequential_feats(x, batch_size, seq_len):\n",
    "    \"\"\"Flattens sequential features with known batch size and seq_len.\"\"\"\n",
    "    x = tf.reshape(x, [batch_size, seq_len, -1])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Transformer from https://www.tensorflow.org/tutorials/text/transformer .\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead)\n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "      Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable\n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "      Returns:\n",
    "        outputs: shape == (..., seq_len_q, depth_v)\n",
    "        attention_weights: shape == (..., seq_len_q, seq_len_k)\n",
    "      \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk.\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    # (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    outputs = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return outputs, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-headed attention layer.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(\n",
    "            q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(\n",
    "            k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(\n",
    "            v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention,\n",
    "            perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention,\n",
    "            (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        outputs = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return outputs, attention_weights\n",
    "\n",
    "\n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Implements a single transformer layer (https://arxiv.org/abs/1706.03762).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff,\n",
    "                 dropout_rate=0.1,\n",
    "                 reorder_ln=False):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.reorder_ln = reorder_ln\n",
    "\n",
    "    def call(self, x):\n",
    "        inp_x = x\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            x = self.layernorm1(x)\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        attn_output, _ = self.mha(x, x, x, mask=None)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            out1 = inp_x + attn_output\n",
    "            x = out1\n",
    "        else:\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out1 = self.layernorm1(x + attn_output)\n",
    "            x = out1\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            x = self.layernorm2(x)\n",
    "\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        if self.reorder_ln:\n",
    "            out2 = out1 + ffn_output\n",
    "        else:\n",
    "            # (batch_size, input_seq_len, d_model)\n",
    "            out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "def pairwise_l2_distance(a, b):\n",
    "    \"\"\"Computes pairwise distances between all rows of a and all rows of b.\"\"\"\n",
    "    norm_a = tf.reduce_sum(tf.square(a), 1)\n",
    "    norm_a = tf.reshape(norm_a, [-1, 1])\n",
    "    norm_b = tf.reduce_sum(tf.square(b), 1)\n",
    "    norm_b = tf.reshape(norm_b, [1, -1])\n",
    "    dist = tf.maximum(norm_a - 2.0 * tf.matmul(a, b, False, True) + norm_b, 0.0)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class ResnetPeriodEstimator(tf.keras.models.Model):\n",
    "    \"\"\"RepNet model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_frames=64,\n",
    "        image_size=112,\n",
    "        base_model_layer_name='conv4_block3_out',\n",
    "        temperature=13.544,\n",
    "        dropout_rate=0.25,\n",
    "        l2_reg_weight=1e-6,\n",
    "        temporal_conv_channels=512,\n",
    "        temporal_conv_kernel_size=3,\n",
    "        temporal_conv_dilation_rate=3,\n",
    "        conv_channels=32,\n",
    "        conv_kernel_size=3,\n",
    "        transformer_layers_config=((512, 4, 512),),\n",
    "        transformer_dropout_rate=0.0,\n",
    "        transformer_reorder_ln=True,\n",
    "        period_fc_channels=(512, 512),\n",
    "        within_period_fc_channels=(512, 512)):\n",
    "        super(ResnetPeriodEstimator, self).__init__()\n",
    "\n",
    "        # Model params.\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.base_model_layer_name = base_model_layer_name\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_reg_weight = l2_reg_weight\n",
    "\n",
    "        self.temporal_conv_channels = temporal_conv_channels\n",
    "        self.temporal_conv_kernel_size = temporal_conv_kernel_size\n",
    "        self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\n",
    "\n",
    "        self.conv_channels = conv_channels\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        # Transformer config in form of (channels, heads, bottleneck channels).\n",
    "        self.transformer_layers_config = transformer_layers_config\n",
    "        self.transformer_dropout_rate = transformer_dropout_rate\n",
    "        self.transformer_reorder_ln = transformer_reorder_ln\n",
    "\n",
    "        self.period_fc_channels = period_fc_channels\n",
    "        self.within_period_fc_channels = within_period_fc_channels\n",
    "\n",
    "        # Base ResNet50 Model.\n",
    "        base_model = tf.keras.applications.ResNet50V2(\n",
    "            include_top=False, weights=None, pooling='max')\n",
    "        self.base_model = tf.keras.models.Model(\n",
    "            inputs=base_model.input,\n",
    "            outputs=base_model.get_layer(self.base_model_layer_name).output)\n",
    "\n",
    "        # 3D Conv on k Frames\n",
    "        self.temporal_conv_layers = [\n",
    "            layers.Conv3D(self.temporal_conv_channels,\n",
    "                          self.temporal_conv_kernel_size,\n",
    "                          padding='same',\n",
    "                          dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\n",
    "                          kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                          kernel_initializer='he_normal')]\n",
    "        self.temporal_bn_layers = [layers.BatchNormalization(axis=4)\n",
    "                                   for _ in self.temporal_conv_layers]\n",
    "\n",
    "        # Counting Module (Self-sim > Conv > Transformer > Classifier)\n",
    "        self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\n",
    "                                            self.conv_kernel_size,\n",
    "                                            padding='same',\n",
    "                                            activation=tf.nn.relu)\n",
    "\n",
    "        channels = self.transformer_layers_config[0][0]\n",
    "        self.input_projection = layers.Dense(\n",
    "            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "            activation=None)\n",
    "\n",
    "        self.input_projection2 = layers.Dense(\n",
    "            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "            activation=None)\n",
    "\n",
    "        length = self.num_frames\n",
    "        self.pos_encoding = tf.compat.v1.get_variable(\n",
    "            name='resnet_period_estimator/pos_encoding',\n",
    "            shape=[1, length, 1],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "        self.pos_encoding2 = tf.compat.v1.get_variable(\n",
    "            name='resnet_period_estimator/pos_encoding2',\n",
    "            shape=[1, length, 1],\n",
    "            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "        self.transformer_layers = []\n",
    "        for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "            self.transformer_layers.append(\n",
    "                TransformerLayer(d_model, num_heads, dff,\n",
    "                                 self.transformer_dropout_rate,\n",
    "                                 self.transformer_reorder_ln))\n",
    "\n",
    "        self.transformer_layers2 = []\n",
    "        for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "            self.transformer_layers2.append(\n",
    "                TransformerLayer(d_model, num_heads, dff,\n",
    "                                 self.transformer_dropout_rate,\n",
    "                                 self.transformer_reorder_ln))\n",
    "\n",
    "        # Period Prediction Module.\n",
    "        self.dropout_layer = layers.Dropout(self.dropout_rate)\n",
    "        num_preds = self.num_frames//2\n",
    "        self.fc_layers = []\n",
    "        for channels in self.period_fc_channels:\n",
    "            self.fc_layers.append(layers.Dense(\n",
    "                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                activation=tf.nn.relu))\n",
    "        self.fc_layers.append(layers.Dense(\n",
    "            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "        # Within Period Module\n",
    "        num_preds = 1\n",
    "        self.within_period_fc_layers = []\n",
    "        for channels in self.within_period_fc_channels:\n",
    "            self.within_period_fc_layers.append(layers.Dense(\n",
    "                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                activation=tf.nn.relu))\n",
    "        self.within_period_fc_layers.append(layers.Dense(\n",
    "            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "    def call(self, x):\n",
    "        # Ensures we are always using the right batch_size during train/eval.\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Conv Feature Extractor.\n",
    "        x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\n",
    "        x = self.base_model(x)\n",
    "        h = tf.shape(x)[1]\n",
    "        w = tf.shape(x)[2]\n",
    "        c = tf.shape(x)[3]\n",
    "        x = tf.reshape(x, [batch_size, -1, h, w, c])\n",
    "\n",
    "        # 3D Conv to give temporal context to per-frame embeddings. \n",
    "        for bn_layer, conv_layer in zip(self.temporal_bn_layers,\n",
    "                                        self.temporal_conv_layers):\n",
    "            x = conv_layer(x)\n",
    "            x = bn_layer(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "        x = tf.reshape(x, (batch_size, self.num_frames, 7, 7, -1))\n",
    "    \n",
    "        x = tf.reduce_max(x, [2, 3])\n",
    "\n",
    "        # Reshape and prepare embs for output.\n",
    "        final_embs = x\n",
    "\n",
    "        # Get self-similarity matrix.\n",
    "        x = get_sims(x, self.temperature)\n",
    "\n",
    "        # 3x3 conv layer on self-similarity matrix.\n",
    "        x = self.conv_3x3_layer(x)\n",
    "        x = tf.reshape(x, [batch_size, self.num_frames, -1])\n",
    "        within_period_x = x\n",
    "\n",
    "        # Period prediction.\n",
    "        x = self.input_projection(x)\n",
    "        x += self.pos_encoding\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)\n",
    "        x = flatten_sequential_feats(x, batch_size, self.num_frames)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = self.dropout_layer(x)\n",
    "            x = fc_layer(x)\n",
    "\n",
    "        # Within period prediction.\n",
    "        within_period_x = self.input_projection2(within_period_x)\n",
    "        within_period_x += self.pos_encoding2\n",
    "        for transformer_layer in self.transformer_layers2:\n",
    "            within_period_x = transformer_layer(within_period_x)\n",
    "        within_period_x = flatten_sequential_feats(within_period_x,\n",
    "                                                   batch_size,\n",
    "                                                   self.num_frames)\n",
    "        for fc_layer in self.within_period_fc_layers:\n",
    "            within_period_x = self.dropout_layer(within_period_x)\n",
    "            within_period_x = fc_layer(within_period_x)\n",
    "\n",
    "        return x, within_period_x, final_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f2ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T07:04:02.633001Z",
     "start_time": "2021-08-31T07:04:02.349943Z"
    }
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147969f",
   "metadata": {},
   "source": [
    "### Modify Add OP Shape Align(Dim Tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rocky-investigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:08:23.538673Z",
     "start_time": "2021-09-01T02:08:19.611372Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64, 1)\n",
      "tf.Tensor([-0.01535174], shape=(1,), dtype=float32)\n",
      "--------------------\n",
      "(1, 64, 512)\n",
      "tf.Tensor(\n",
      "[0.02054866 0.02054866 0.02054866 0.02054866 0.02054866 0.02054866\n",
      " 0.02054866 0.02054866 0.02054866 0.02054866], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = ResnetPeriodEstimator()\n",
    "print(model.pos_encoding.shape)\n",
    "print(model.pos_encoding[0][0][:10])\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore('/data/pretrained/cv/repnet/ckpt-88').expect_partial()\n",
    "print('-'*20)\n",
    "model.pos_encoding = tf.tile(model.pos_encoding, multiples=[1, 1, 512])\n",
    "model.pos_encoding2 = tf.tile(model.pos_encoding2, multiples=[1, 1, 512])\n",
    "print(model.pos_encoding.shape)\n",
    "print(model.pos_encoding[0][0][:10])\n",
    "test_inputs = np.random.randn(2, 64, 112, 112, 3).astype(np.float32)\n",
    "test_inputs_tensor = tf.convert_to_tensor(test_inputs)\n",
    "test_outputs = model(test_inputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d3480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:08:26.022357Z",
     "start_time": "2021-09-01T02:08:25.960425Z"
    }
   },
   "outputs": [],
   "source": [
    "test_outputs[0].shape, test_outputs[1].shape, test_outputs[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411d10f",
   "metadata": {},
   "source": [
    "### Simple Conv3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196a301",
   "metadata": {},
   "source": [
    "tool | version\n",
    "---: | :---:\n",
    "tensorflow | 2.6.0\n",
    "tensorboard | 2.6.0\n",
    "onnx | 1.10.1\n",
    "onnxruntime | 1.8.1\n",
    "tf2onnx | 1.9.2\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "针对上面版本, 转换onnx格式时, BatchNormalization会和Transpose操作融合为FusedBatchNormV3, Transpose的dim参数让人疑惑\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950928d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:00:41.204434Z",
     "start_time": "2021-08-31T14:00:40.230917Z"
    }
   },
   "outputs": [],
   "source": [
    "model3d = tf.keras.models.Sequential([\n",
    "    layers.Conv3D(\n",
    "        512, 3, padding='same',\n",
    "        dilation_rate=(3, 1, 1),\n",
    "        kernel_regularizer=regularizers.l2(1e-6),\n",
    "        kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(axis=1),\n",
    "    layers.ReLU()\n",
    "])\n",
    "\n",
    "inputs3d = np.random.randn(2, 64, 7, 7, 1024).astype(np.float32)\n",
    "outputs3d = model3d(inputs3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0245077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:00:43.416288Z",
     "start_time": "2021-08-31T14:00:42.301130Z"
    }
   },
   "outputs": [],
   "source": [
    "saved_model3d_path = '/data/nb_data/saved_models3d'\n",
    "model3d.save(saved_model3d_path, save_format='tf', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65e8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:01:03.945889Z",
     "start_time": "2021-08-31T14:00:44.382647Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python3 -m tf2onnx.convert --opset 14 \\\n",
    "    --tag serve --signature_def serving_default \\\n",
    "    --saved-model {saved_model3d_path} --output {saved_model3d_path}/saved_model.onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81907ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:01:09.203110Z",
     "start_time": "2021-08-31T14:01:08.879980Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%netron {saved_model3d_path}/saved_model.onnx 8131 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28bc36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T14:01:14.001751Z",
     "start_time": "2021-08-31T14:01:12.805542Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession(f'{saved_model3d_path}/saved_model.onnx')  \n",
    "inputs = {sess.get_inputs()[0].name: inputs3d}\n",
    "outputs = [o.name for o in sess.get_outputs()]\n",
    "predictions = sess.run(outputs, inputs)\n",
    "predictions[-1].shape, outputs3d[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739203af",
   "metadata": {},
   "source": [
    "## Frozen Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963c7ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:30:50.980124Z",
     "start_time": "2021-08-31T10:30:46.937974Z"
    }
   },
   "outputs": [],
   "source": [
    "full_model = tf.function(lambda x: model(x))\n",
    "full_model = full_model.get_concrete_function(tf.TensorSpec(test_inputs.shape, test_inputs.dtype))\n",
    "frozen_func = convert_variables_to_constants_v2(full_model, lower_control_flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3e5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T09:55:35.177872Z",
     "start_time": "2021-08-31T09:55:35.114636Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ x for x in dir(frozen_func) if x[0] != '_'], '-' * 60, \\\n",
    "frozen_func.inputs, frozen_func.outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2dda0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T09:55:35.244732Z",
     "start_time": "2021-08-31T09:55:35.180147Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[ x for x in dir(frozen_func.graph) if x[0] != '_'], '-' * 60, \\\n",
    "frozen_func.graph.inputs, frozen_func.graph.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ecc089",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:29:07.271986Z",
     "start_time": "2021-08-31T10:29:06.867092Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frozen_graph_def = frozen_func.graph.as_graph_def()\n",
    "[ x for x in dir(frozen_graph_def) if x[0] != '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9e30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T09:55:35.699933Z",
     "start_time": "2021-08-31T09:55:35.635282Z"
    }
   },
   "outputs": [],
   "source": [
    "operations = frozen_func.graph.get_operations()\n",
    "[ x for x in dir(operations[590]) if x[0] != '_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8f28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T09:55:36.088460Z",
     "start_time": "2021-08-31T09:55:36.016567Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, op in enumerate(operations[580: 595], 580):\n",
    "    print('%5d' % i, op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7223b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T09:55:36.159392Z",
     "start_time": "2021-08-31T09:55:36.091096Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(operations[590])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7a8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:23:58.909599Z",
     "start_time": "2021-08-31T10:23:55.630784Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_tensors = [x for x in frozen_func.inputs if x.dtype != tf.resource]\n",
    "# output_tensors = frozen_func.outputs\n",
    "# \n",
    "# config = [\n",
    "#     'pruning', 'function', 'constfold', 'shape', 'remap', 'memory',\n",
    "#     'common_subgraph_elimination', 'arithmetic', 'loop', 'dependency', 'debug_stripper'\n",
    "# ]\n",
    "# \n",
    "# frozen_graph_def = run_graph_optimizations(\n",
    "#     frozen_graph_def,\n",
    "#     input_tensors,\n",
    "#     output_tensors,\n",
    "#     config=get_grappler_config(config),\n",
    "#     graph=frozen_func.graph\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b668b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_proto_path = '/data/nb_data/frozen_models/frozen_graph.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf6f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:29:53.730937Z",
     "start_time": "2021-08-31T10:29:19.717963Z"
    }
   },
   "outputs": [],
   "source": [
    "as_text = False\n",
    "frozen_graph_proto_path = tf.io.write_graph(\n",
    "    graph_or_graph_def=frozen_graph_def,\n",
    "    logdir='/data/nb_data/frozen_models',\n",
    "    name='frozen_graph.pb%s' % ('txt' if as_text else ''),\n",
    "    as_text=as_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cf7bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:29:53.730937Z",
     "start_time": "2021-08-31T10:29:19.717963Z"
    }
   },
   "outputs": [],
   "source": [
    "# %netron {frozen_graph_proto_path} 8120 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b199d8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:29:53.730937Z",
     "start_time": "2021-08-31T10:29:19.717963Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = ','.join([x.name for x in frozen_func.inputs])\n",
    "outputs = ','.join([x.name for x in frozen_func.outputs])\n",
    "\n",
    "!python3 -m tf2onnx.convert --opset 14 --input {frozen_graph_proto_path} \\\n",
    "    --inputs {inputs} --outputs {outputs} --output {frozen_graph_proto_path}.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed96c133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:26:50.736798Z",
     "start_time": "2021-08-31T10:26:50.621633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/data/nb_data/frozen_models/frozen_graph.pb.onnx' at http://0.0.0.0:8132\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id='tensorboard-frame-3a02809003636b4b' width='100%' height='500' frameborder='0'>\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3a02809003636b4b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8132;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%netron {frozen_graph_proto_path}.onnx 8132 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b81c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:30:00.867656Z",
     "start_time": "2021-08-31T10:30:00.332084Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(f'{frozen_graph_proto_path}.onnx')\n",
    "try:\n",
    "    onnx.checker.check_model(onnx_model, full_check=True) \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "graph_def = onnx_model.graph\n",
    "[x for x in dir(graph_def) if x[0] != '_'], '-'*60, graph_def.input, '-'*60, graph_def.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e2cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:29:53.730937Z",
     "start_time": "2021-08-31T10:29:19.717963Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, node in enumerate(graph_def.node[95:99], 95):\n",
    "    print('%5d' % i, node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dd90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:17.739861Z",
     "start_time": "2021-09-01T07:34:16.979480Z"
    }
   },
   "source": [
    "## Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c576b5f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:17.739861Z",
     "start_time": "2021-09-01T07:34:16.979480Z"
    }
   },
   "outputs": [],
   "source": [
    "saved_model_path = '/data/nb_data/saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a8766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:17.739861Z",
     "start_time": "2021-09-01T07:34:16.979480Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(saved_model_path, save_format='tf', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691576b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:17.739861Z",
     "start_time": "2021-09-01T07:34:16.979480Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python3 -m tf2onnx.convert --opset 14 \\\n",
    "    --tag serve --signature_def serving_default \\\n",
    "    --saved-model {saved_model_path} --output {saved_model_path}/saved_model.onnx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb3fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T02:10:06.032829Z",
     "start_time": "2021-09-01T02:10:05.818239Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%netron {saved_model_path}/saved_model.onnx 8133 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c49169a",
   "metadata": {},
   "source": [
    "### Modify Transpose Perm Attributes(FusedBatchNormV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0800e378",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:17.739861Z",
     "start_time": "2021-09-01T07:34:16.979480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  180 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_transpose__549\n",
      "  181 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_reshape__550\n",
      "  182 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_concat__551\n",
      "  183 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_gather__553\n",
      "  184 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_reshape__554\n",
      "  185 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_transpose__559\n",
      "  186 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND_reshape__561\n",
      "  187 StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/BatchToSpaceND\n",
      "  188 StatefulPartitionedCall/resnet_period_estimator/conv3d/BiasAdd\n",
      "  189 StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576\n",
      "------------------------------------------------------------\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/BiasAdd:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576\"\n",
      "op_type: \"Transpose\"\n",
      "attribute {\n",
      "  name: \"perm\"\n",
      "  ints: 0\n",
      "  ints: 4\n",
      "  ints: 1\n",
      "  ints: 2\n",
      "  ints: 3\n",
      "  type: INTS\n",
      "}\n",
      "\n",
      "------------------------------------------------------------\n",
      "  190 StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3\n",
      "  191 StatefulPartitionedCall/resnet_period_estimator/Relu\n",
      "  192 StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__581\n",
      "------------------------------------------------------------\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/Relu:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__581:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__581\"\n",
      "op_type: \"Transpose\"\n",
      "attribute {\n",
      "  name: \"perm\"\n",
      "  ints: 0\n",
      "  ints: 2\n",
      "  ints: 3\n",
      "  ints: 4\n",
      "  ints: 1\n",
      "  type: INTS\n",
      "}\n",
      "\n",
      "------------------------------------------------------------\n",
      "  193 StatefulPartitionedCall/resnet_period_estimator/Reshape_2\n",
      "  194 StatefulPartitionedCall/resnet_period_estimator/Max\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(f'{saved_model_path}/saved_model.onnx')\n",
    "onnx_model = onnx.shape_inference.infer_shapes(onnx_model, check_type=True) # for value_info\n",
    "onnx_graph_def = onnx_model.graph\n",
    "for i, node in enumerate(onnx_graph_def.node[180:195], 180):\n",
    "    print('%5d' % i, node.name)\n",
    "    if 'FusedBatchNormV3' in node.name and 'Transpose' == node.op_type:\n",
    "        attr = node.attribute.pop()\n",
    "        if attr.ints[1] == 3:\n",
    "            node.attribute.append(OH.make_attribute('perm', [0, 4, 1, 2, 3]))\n",
    "        elif attr.ints[1] == 2:\n",
    "            node.attribute.append(OH.make_attribute('perm', [0, 2, 3, 4, 1]))\n",
    "        print('-'*60)\n",
    "        print(node)\n",
    "        print('-'*60)\n",
    "        \n",
    "onnx.save(onnx_model, f'{saved_model_path}/saved_model_mod.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f11b2ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:34:27.274057Z",
     "start_time": "2021-09-01T07:34:27.206510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 479, 206)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onnx_graph_def.node), len(onnx_graph_def.value_info), len(onnx_graph_def.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e17166bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:35:23.669329Z",
     "start_time": "2021-09-01T07:35:23.609303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(name: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576:0\"\n",
       " type {\n",
       "   tensor_type {\n",
       "     elem_type: 1\n",
       "   }\n",
       " },\n",
       " '--------------------------------------------------------------------------------',\n",
       " input: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/BiasAdd:0\"\n",
       " output: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576:0\"\n",
       " name: \"StatefulPartitionedCall/resnet_period_estimator/batch_normalization/FusedBatchNormV3__576\"\n",
       " op_type: \"Transpose\"\n",
       " attribute {\n",
       "   name: \"perm\"\n",
       "   ints: 0\n",
       "   ints: 4\n",
       "   ints: 1\n",
       "   ints: 2\n",
       "   ints: 3\n",
       "   type: INTS\n",
       " })"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_idx = 189\n",
    "onnx_graph_def.value_info[target_idx], '-'*80, onnx_graph_def.node[target_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c8558fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:35:30.064157Z",
     "start_time": "2021-09-01T07:35:29.973956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512] StatefulPartitionedCall/resnet_period_estimator/conv3d/BiasAdd/ReadVariableOp:0\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(onnx_graph_def.initializer):\n",
    "    if onnx_graph_def.node[target_idx].input[0][:-2] in w.name:\n",
    "        print(w.dims, w.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dd15ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  213 name: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims:0\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"unk__20\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"unk__1163\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"unk__1164\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, proto in enumerate(onnx_model.graph.value_info):\n",
    "    if 'ExpandDims' in proto.name:\n",
    "        print('%5d' % i, proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde492b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  212 input: \"StatefulPartitionedCall/resnet_period_estimator/Softmax:0\"\n",
      "input: \"const_starts__835\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims\"\n",
      "op_type: \"Unsqueeze\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(onnx_graph_def.node):\n",
    "    if 'ExpandDims' in node.name:\n",
    "        print('%5d'%i, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8d43761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:35:42.850850Z",
     "start_time": "2021-09-01T07:35:42.733903Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/data/nb_data/saved_models/saved_model_mod.onnx' at http://0.0.0.0:8134\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id='tensorboard-frame-8ff5fffcddf0bddc' width='100%' height='400' frameborder='0'>\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8ff5fffcddf0bddc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8134;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%netron {saved_model_path}/saved_model_mod.onnx 8134 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968aa687",
   "metadata": {},
   "source": [
    "### Original Model Outputs VS Modified Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "417d9a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:35:59.862806Z",
     "start_time": "2021-09-01T07:35:57.531288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 64, 512)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = rt.InferenceSession(f'{saved_model_path}/saved_model_mod.onnx')  \n",
    "inputs = {sess.get_inputs()[0].name: test_inputs}\n",
    "outputs = [o.name for o in sess.get_outputs()]\n",
    "outputs_onnx = sess.run(outputs, inputs)\n",
    "outputs_onnx[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "88277511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 64, 112, 112, 3), 'input_1')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " test_inputs.shape, sess.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9d4c09e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-14.010881  ,  -0.43419096,  -2.5228245 ], dtype=float32),\n",
       " '------------------------------------------------------------',\n",
       " array([-14.010895 ,  -0.4341318,  -2.5229156], dtype=float32))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs[0][0][0][:3].numpy(), '-'*60, outputs_onnx[0][0][0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "169472b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T07:36:48.948098Z",
     "start_time": "2021-09-01T07:36:48.880296Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.408767 , 1.654074 , 2.0360768]], dtype=float32),\n",
       " '------------------------------------------------------------',\n",
       " array([[1.4087402, 1.6540375, 2.03607  ]], dtype=float32))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs[1][0][:3].numpy().T, '-'*60, outputs_onnx[1][0][:3].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77eb5a",
   "metadata": {},
   "source": [
    "### Onnx Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eb01e75f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([name: \"input_1\"\n",
       " type {\n",
       "   tensor_type {\n",
       "     elem_type: 1\n",
       "     shape {\n",
       "       dim {\n",
       "         dim_value: 1\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 64\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 112\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 112\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 3\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " }\n",
       " ],\n",
       " [name: \"output_1\"\n",
       " type {\n",
       "   tensor_type {\n",
       "     elem_type: 1\n",
       "     shape {\n",
       "       dim {\n",
       "         dim_value: 1\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 64\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 32\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " }\n",
       " , name: \"output_2\"\n",
       " type {\n",
       "   tensor_type {\n",
       "     elem_type: 1\n",
       "     shape {\n",
       "       dim {\n",
       "         dim_value: 1\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 64\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 1\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " }\n",
       " , name: \"output_3\"\n",
       " type {\n",
       "   tensor_type {\n",
       "     elem_type: 1\n",
       "     shape {\n",
       "       dim {\n",
       "         dim_value: 1\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 64\n",
       "       }\n",
       "       dim {\n",
       "         dim_value: 512\n",
       "       }\n",
       "     }\n",
       "   }\n",
       " }\n",
       " ])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = onnx.load(f'{saved_model_path}/saved_model_mod.onnx')  \n",
    "model_simp, check = simplify(\n",
    "    onnx_model, dynamic_input_shape=False,\n",
    "    input_shapes={sess.get_inputs()[0].name: [1, 64, 112, 112, 3]})\n",
    "if not check:\n",
    "    raise\n",
    "model_simp.graph.input, model_simp.graph.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb6848",
   "metadata": {},
   "source": [
    "### Fix Unsqueeze Op (axis attr) Error (TF --> Onnx --> OpenVINO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f0ba9",
   "metadata": {},
   "source": [
    "Run the script will crash, error info as below.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "CUR_DIR=$(cd $(dirname ${BASH_SOURCE[0]}); pwd)\n",
    "\n",
    "EXEBIN=/opt/intel/openvino_2021/deployment_tools/model_optimizer/mo_onnx.py\n",
    "\n",
    "$EXEBIN --input_model $CUR_DIR/saved_model_simp.onnx  --output_dir $CUR_DIR \\\n",
    "    --model_name repnet --input_shape [1,64,112,112,3] \\\n",
    "    --log_level DEBUG\n",
    "```\n",
    "\n",
    "<font color=\"red\"><B>Error Info:</B></font>\n",
    "\n",
    "```\n",
    "[ 2021-09-03 18:45:59,828 ] [ DEBUG ] [ infer:117 ]  Partial infer for StatefulPartitionedCall/resnet_period_estimator/ExpandDims\n",
    "[ 2021-09-03 18:45:59,828 ] [ DEBUG ] [ infer:118 ]  Op: ExpandDims\n",
    "[ 2021-09-03 18:45:59,828 ] [ DEBUG ] [ infer:119 ]  Inputs:\n",
    "[ 2021-09-03 18:45:59,828 ] [ DEBUG ] [ infer:19 ]  input[0]: shape = [ 1 64 64], value = <UNKNOWN>\n",
    "[ 2021-09-03 18:45:59,829 ] [ DEBUG ] [ infer:19 ]  input[1]: shape = [1], value = [-1]\n",
    "[ ERROR ]  Cannot infer shapes or values for node \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims\".\n",
    "[ ERROR ]  Wrong number of inputs to the layer StatefulPartitionedCall/resnet_period_estimator/ExpandDims\n",
    "[ ERROR ]  \n",
    "[ ERROR ]  It can happen due to bug in custom shape infer function <function ExpandDims.infer at 0x7f16e38afdc0>.\n",
    "[ ERROR ]  Or because the node inputs have incorrect values/shapes.\n",
    "[ ERROR ]  Or because input shapes are incorrect (embedded to the model or passed via --input_shape).\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "IF dynamic_input_shape is true (input batch size is dynamic), the mo_onnx will crash. \n",
    "</div>\n",
    "\n",
    "```    \n",
    "[ ERROR ]  -------------------------------------------------\n",
    "[ ERROR ]  ----------------- INTERNAL ERROR ----------------\n",
    "[ ERROR ]  Unexpected exception happened.\n",
    "[ ERROR ]  Please contact Model Optimizer developers and forward the following information:\n",
    "[ ERROR ]  Exception occurred during running replacer \"REPLACEMENT_ID (<class 'extensions.middle.ApplyPermutations.ApplyPermutation'>)\": Cannot infer `StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__511` due to both order and reverse_order was set\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "361d75a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_pad__486:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__487:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__487\"\n",
      "op_type: \"Transpose\"\n",
      "domain: \"\"\n",
      "\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_reshape__510:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__511:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__511\"\n",
      "op_type: \"Transpose\"\n",
      "domain: \"\"\n",
      "\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_reshape__514:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__520:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/conv3d/Conv3D/SpaceToBatchND_transpose__520\"\n",
      "op_type: \"Transpose\"\n",
      "attribute {\n",
      "  name: \"perm\"\n",
      "  ints: 2\n",
      "  ints: 4\n",
      "  ints: 6\n",
      "  ints: 0\n",
      "  ints: 1\n",
      "  ints: 3\n",
      "  ints: 5\n",
      "  ints: 7\n",
      "  type: INTS\n",
      "}\n",
      "domain: \"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in onnx_model.graph.node:\n",
    "    if 'SpaceToBatchND_transpose__' in node.name:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b856a189",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114,0)\n",
      "------------------------------------------------------------\n",
      "\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/Softmax:0\"\n",
      "output: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims:0\"\n",
      "name: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims\"\n",
      "op_type: \"Unsqueeze\"\n",
      "attribute {\n",
      "  name: \"axes\"\n",
      "  ints: -1\n",
      "  type: INTS\n",
      "}\n",
      " dims: 1\n",
      "data_type: 7\n",
      "name: \"const_starts__835\"\n",
      "raw_data: \"\\377\\377\\377\\377\\377\\377\\377\\377\"\n",
      " [-1] \n",
      "\n",
      "(115,3)\n",
      "------------------------------------------------------------\n",
      "\n",
      "input: \"StatefulPartitionedCall/resnet_period_estimator/ExpandDims:0\"\n",
      "output: \"Unsqueeze__1052:0\"\n",
      "name: \"Unsqueeze__1052\"\n",
      "op_type: \"Unsqueeze\"\n",
      "attribute {\n",
      "  name: \"axes\"\n",
      "  ints: 0\n",
      "  type: INTS\n",
      "}\n",
      "domain: \"\"\n",
      " dims: 1\n",
      "data_type: 7\n",
      "name: \"const_axes__1090\"\n",
      "raw_data: \"\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      " [0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_simp_ = copy.deepcopy(model_simp)\n",
    "for i, node in enumerate(model_simp_.graph.node):\n",
    "    if 'ExpandDims' in node.name or node.op_type == 'Unsqueeze':\n",
    "        for j, weight in enumerate(model_simp_.graph.initializer):\n",
    "            for x in node.input:\n",
    "                if x.startswith('const_') and x in weight.name and weight.data_type == onnx.TensorProto.INT64:\n",
    "                    tensor_value = to_array(weight)\n",
    "                    for attr in node.attribute:\n",
    "                        if attr.name == 'axes':\n",
    "                            node.attribute.remove(attr)\n",
    "                            break\n",
    "                    node.input.remove(x)\n",
    "                    node.attribute.append(OH.make_attribute('axes', tensor_value))\n",
    "                    print('(%d,%d)\\n%s\\n'%(i, j, '-'*60))\n",
    "                    print(node, weight, tensor_value, '\\n')\n",
    "                    break\n",
    "model_simp.CopyFrom(model_simp_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "35b09406",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "onnx.save(model_simp, f'{saved_model_path}/saved_model_simp.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "630c5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8135\n",
      "Serving '/data/nb_data/saved_models/saved_model_simp.onnx' at http://0.0.0.0:8135\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id='tensorboard-frame-c77158b9c8e18b52' width='100%' height='500' frameborder='0'>\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c77158b9c8e18b52\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8135;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%netron {saved_model_path}/saved_model_simp.onnx 8135 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a677a40",
   "metadata": {},
   "source": [
    "### Repnet Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from scipy.signal import medfilt\n",
    "from scipy.special import softmax\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "num_frames = 64\n",
    "# video_path = '/data/testvids/test1.mp4'\n",
    "video_path = 'https://frepai.s3.didiyunapi.com/datasets/test/repnet_bird.gif'\n",
    "\n",
    "def read_video(video_filename, width=112, height=112):\n",
    "    cap = cv2.VideoCapture(video_filename)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames = []\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            success, frame_bgr = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "            frame_rgb = cv2.resize(frame_rgb, (width, height))\n",
    "            frames.append(frame_rgb)\n",
    "        frames = np.asarray(frames)\n",
    "    return frames, fps\n",
    "\n",
    "video_frames, fps = read_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_image(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ec8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(video_frames)\n",
    "diff = math.ceil(seq_len / num_frames) * num_frames - seq_len\n",
    "fps, seq_len, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1434f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align num_frames\n",
    "frames = np.take(video_frames, indices=list(range(seq_len)) + [seq_len-1] * diff, axis=0)\n",
    "batch_size = int(len(frames) / num_frames)\n",
    "len(frames), batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image\n",
    "inputs_numpy = np.reshape(frames, (batch_size, 64, 112, 112, 3)).astype(np.float32)\n",
    "inputs_numpy -= 127.5\n",
    "inputs_numpy /= 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76567eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tensor = tf.convert_to_tensor(inputs_numpy)\n",
    "outputs_tensor = model(inputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_numpy = np.random.randn(1, 64, 112, 112, 3).astype(np.float32)\n",
    "sess = rt.InferenceSession(f'{saved_model_path}/saved_model_simp.onnx')  \n",
    "inputs = {sess.get_inputs()[0].name: inputs_numpy}\n",
    "outputs = [o.name for o in sess.get_outputs()]\n",
    "outputs_onnx = sess.run(outputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_tensor[1][0][:3].numpy().T, '-'*60, outputs_onnx[1][0][:3].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77adec",
   "metadata": {},
   "source": [
    "### Calculate Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores1, within_period1, sim1 = outputs_tensor[0].numpy(), outputs_tensor[1].numpy(), outputs_tensor[2].numpy()\n",
    "raw_scores2, within_period2, sim2 = outputs_onnx\n",
    "(raw_scores1.shape, within_period1.shape, sim1.shape), '-'*50, (raw_scores2.shape, within_period2.shape, sim2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_used = True\n",
    "if not onnx_used:\n",
    "    period_matrix = raw_scores1.reshape((-1, raw_scores1.shape[-1]))[:seq_len]\n",
    "    within_period = within_period1.reshape((-1, within_period1.shape[-1]))[:seq_len]\n",
    "else:\n",
    "    period_matrix = raw_scores2.reshape((-1, raw_scores2.shape[-1]))[:seq_len]\n",
    "    within_period = within_period2.reshape((-1, within_period2.shape[-1]))[:seq_len]\n",
    "(period_matrix.shape, within_period.shape), '-'*60, \\\n",
    "period_matrix[0][:3], '-'*60, within_period[:3].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd9a77",
   "metadata": {},
   "source": [
    "####  Inv-Period Confidence Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best value from [3, 32] as the inv-period\n",
    "per_frame_inv_period = np.argmax(period_matrix, axis=-1) + 1\n",
    "per_frame_inv_period[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee357eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_period_conf = np.max(softmax(period_matrix, axis=-1), axis=-1)\n",
    "inv_period_conf = np.where(np.less(per_frame_inv_period, 3), 0.0, inv_period_conf)\n",
    "inv_period_conf[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d637f",
   "metadata": {},
   "source": [
    "#### Within Period Confience Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_period_conf = sigmoid(within_period)[:, 0]\n",
    "within_period_conf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_period_conf  = np.sqrt(np.multiply(within_period_conf, inv_period_conf))\n",
    "within_period_conf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d56c34",
   "metadata": {},
   "source": [
    "#### Period Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d08f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_filter\n",
    "within_period_binary = np.asarray(within_period_conf > 0.5)\n",
    "within_period_binary = medfilt(within_period_binary, 5)\n",
    "within_period_binary[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_frame_counts = np.where(np.less(per_frame_inv_period, 3), 0.0, np.divide(1.0, per_frame_inv_period))\n",
    "per_frame_counts = medfilt(per_frame_counts, 5)\n",
    "per_frame_counts *= np.asarray(within_period_binary)\n",
    "per_frame_counts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87611738",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(per_frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da38f2",
   "metadata": {},
   "source": [
    "## TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdfc9c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:06.302285Z",
     "start_time": "2021-08-31T10:44:56.002Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(saved_model_path)\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f2afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:06.303367Z",
     "start_time": "2021-08-31T10:44:56.007Z"
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS, # tflite: some op not support\n",
    "]\n",
    "litemodel = converter.convert()\n",
    "with open(f'{saved_model_path}/saved_model.tflite', 'wb') as f:\n",
    "    f.write(litemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c56797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:06.304625Z",
     "start_time": "2021-08-31T10:44:56.012Z"
    }
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=f'{saved_model_path}/saved_model.tflite')\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "interpreter.resize_tensor_input(input_details[0]['index'], test_inputs.shape) # modify input shape\n",
    "interpreter.allocate_tensors()\n",
    "input_details, output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081b980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-31T10:45:06.306552Z",
     "start_time": "2021-08-31T10:44:56.016Z"
    }
   },
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], test_inputs)\n",
    "interpreter.invoke()\n",
    "output_data1 = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_data2 = interpreter.get_tensor(output_details[1]['index'])\n",
    "output_data3 = interpreter.get_tensor(output_details[2]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5f7c7",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da404e",
   "metadata": {},
   "source": [
    "- https://github.com/onnx/onnx/blob/master/docs/Operators.md\n",
    "- https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
    "- https://github.com/onnx/tensorflow-onnx/blob/master/support_status.md\n",
    "- https://segmentfault.com/a/1190000039936376\n",
    "- https://github.com/onnx/tensorflow-onnx/issues/1606\n",
    "- https://github.com/onnx/tensorflow-onnx/issues/1634"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108552f5",
   "metadata": {},
   "source": [
    "## FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d92bd",
   "metadata": {},
   "source": [
    "[TensorFlow Lite: TensorListFromTensor, TensorListReserve, TensorListStack, While][1]\n",
    "\n",
    "[1]: https://github.com/tensorflow/tensorflow/issues/33416"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "283.026px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "224.361px",
    "left": "1111.42px",
    "top": "66.2926px",
    "width": "407.102px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
