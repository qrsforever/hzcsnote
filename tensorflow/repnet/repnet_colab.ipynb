{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:04.836183Z",
     "start_time": "2021-08-26T03:05:04.822399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display, Markdown, HTML, Javascript\n",
    "display(HTML('<style>.container { width:%d%% !important; }</style>' % 80))\n",
    "\n",
    "# tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:08.258107Z",
     "start_time": "2021-08-26T03:05:06.186485Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:08.587300Z",
     "start_time": "2021-08-26T03:05:08.260739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f3fac7f53c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:08.768417Z",
     "start_time": "2021-08-26T03:05:08.763099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:14.310675Z",
     "start_time": "2021-08-26T03:05:13.912835Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:22.195210Z",
     "start_time": "2021-08-26T03:05:21.674865Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76L5XFonl_Bw",
    "outputId": "e6e2032e-3b45-4c00-a85a-46ec53d53bbc"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Javascript\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# import tensorflow.compat.v2 as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "# ! pip install youtube_dl\n",
    "# import youtube_dl\n",
    "\n",
    "# from google.colab import drive\n",
    "# from google.colab import output\n",
    "# from google.colab.output import eval_js\n",
    "\n",
    "# Model definition\n",
    "layers = tf.keras.layers\n",
    "regularizers = tf.keras.regularizers\n",
    "\n",
    "\n",
    "class ResnetPeriodEstimator(tf.keras.models.Model):\n",
    "  \"\"\"RepNet model.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_frames=64,\n",
    "      image_size=112,\n",
    "      base_model_layer_name='conv4_block3_out',\n",
    "      temperature=13.544,\n",
    "      dropout_rate=0.25,\n",
    "      l2_reg_weight=1e-6,\n",
    "      temporal_conv_channels=512,\n",
    "      temporal_conv_kernel_size=3,\n",
    "      temporal_conv_dilation_rate=3,\n",
    "      conv_channels=32,\n",
    "      conv_kernel_size=3,\n",
    "      transformer_layers_config=((512, 4, 512),),\n",
    "      transformer_dropout_rate=0.0,\n",
    "      transformer_reorder_ln=True,\n",
    "      period_fc_channels=(512, 512),\n",
    "      within_period_fc_channels=(512, 512)):\n",
    "    super(ResnetPeriodEstimator, self).__init__()\n",
    "\n",
    "    # Model params.\n",
    "    self.num_frames = num_frames\n",
    "    self.image_size = image_size\n",
    "\n",
    "    self.base_model_layer_name = base_model_layer_name\n",
    "\n",
    "    self.temperature = temperature\n",
    "\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.l2_reg_weight = l2_reg_weight\n",
    "\n",
    "    self.temporal_conv_channels = temporal_conv_channels\n",
    "    self.temporal_conv_kernel_size = temporal_conv_kernel_size\n",
    "    self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\n",
    "\n",
    "    self.conv_channels = conv_channels\n",
    "    self.conv_kernel_size = conv_kernel_size\n",
    "    # Transformer config in form of (channels, heads, bottleneck channels).\n",
    "    self.transformer_layers_config = transformer_layers_config\n",
    "    self.transformer_dropout_rate = transformer_dropout_rate\n",
    "    self.transformer_reorder_ln = transformer_reorder_ln\n",
    "\n",
    "    self.period_fc_channels = period_fc_channels\n",
    "    self.within_period_fc_channels = within_period_fc_channels\n",
    "\n",
    "    # Base ResNet50 Model.\n",
    "    base_model = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, pooling='max')\n",
    "    self.base_model = tf.keras.models.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=base_model.get_layer(self.base_model_layer_name).output)\n",
    "\n",
    "    # 3D Conv on k Frames\n",
    "    self.temporal_conv_layers = [\n",
    "        layers.Conv3D(self.temporal_conv_channels,\n",
    "                      self.temporal_conv_kernel_size,\n",
    "                      padding='same',\n",
    "                      dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\n",
    "                      kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                      kernel_initializer='he_normal', name='RE_conv3D')]\n",
    "    self.temporal_bn_layers = [layers.BatchNormalization()\n",
    "                               for _ in self.temporal_conv_layers]\n",
    "\n",
    "    # Counting Module (Self-sim > Conv > Transformer > Classifier)\n",
    "    self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\n",
    "                                        self.conv_kernel_size,\n",
    "                                        padding='same',\n",
    "                                        activation=tf.nn.relu)\n",
    "\n",
    "    channels = self.transformer_layers_config[0][0]\n",
    "    self.input_projection = layers.Dense(\n",
    "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "        activation=None)\n",
    "    self.input_projection2 = layers.Dense(\n",
    "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "        activation=None)\n",
    "\n",
    "    length = self.num_frames\n",
    "    self.pos_encoding = tf.compat.v1.get_variable(\n",
    "        name='resnet_period_estimator/pos_encoding',\n",
    "        shape=[1, length, 1],\n",
    "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "    self.pos_encoding2 = tf.compat.v1.get_variable(\n",
    "        name='resnet_period_estimator/pos_encoding2',\n",
    "        shape=[1, length, 1],\n",
    "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    self.transformer_layers = []\n",
    "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "      self.transformer_layers.append(\n",
    "          TransformerLayer(d_model, num_heads, dff,\n",
    "                           self.transformer_dropout_rate,\n",
    "                           self.transformer_reorder_ln))\n",
    "\n",
    "    self.transformer_layers2 = []\n",
    "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "      self.transformer_layers2.append(\n",
    "          TransformerLayer(d_model, num_heads, dff,\n",
    "                           self.transformer_dropout_rate,\n",
    "                           self.transformer_reorder_ln))\n",
    "\n",
    "    # Period Prediction Module.\n",
    "    self.dropout_layer = layers.Dropout(self.dropout_rate)\n",
    "    num_preds = self.num_frames//2\n",
    "    self.fc_layers = []\n",
    "    for channels in self.period_fc_channels:\n",
    "      self.fc_layers.append(layers.Dense(\n",
    "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "          activation=tf.nn.relu))\n",
    "    self.fc_layers.append(layers.Dense(\n",
    "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "    # Within Period Module\n",
    "    num_preds = 1\n",
    "    self.within_period_fc_layers = []\n",
    "    for channels in self.within_period_fc_channels:\n",
    "      self.within_period_fc_layers.append(layers.Dense(\n",
    "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "          activation=tf.nn.relu))\n",
    "    self.within_period_fc_layers.append(layers.Dense(\n",
    "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
    "\n",
    "  def call(self, x):\n",
    "    # Ensures we are always using the right batch_size during train/eval.\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    # Conv Feature Extractor.\n",
    "    x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\n",
    "    x = self.base_model(x)\n",
    "    h = tf.shape(x)[1]\n",
    "    w = tf.shape(x)[2]\n",
    "    c = tf.shape(x)[3]\n",
    "    x = tf.reshape(x, [batch_size, -1, h, w, c])\n",
    "\n",
    "    # 3D Conv to give temporal context to per-frame embeddings. \n",
    "    for bn_layer, conv_layer in zip(self.temporal_bn_layers,\n",
    "                                    self.temporal_conv_layers):\n",
    "      x = conv_layer(x)\n",
    "      x = bn_layer(x)\n",
    "      x = tf.nn.relu(x)\n",
    "    \n",
    "    print('temporal:', x.shape)\n",
    "\n",
    "    x = tf.reduce_max(x, [2, 3])\n",
    "\n",
    "    # Reshape and prepare embs for output.\n",
    "    final_embs = x\n",
    "\n",
    "    print('before get_sims', x.shape)\n",
    "    # Get self-similarity matrix.\n",
    "    x = get_sims(x, self.temperature)\n",
    "\n",
    "    # 3x3 conv layer on self-similarity matrix.\n",
    "    x = self.conv_3x3_layer(x)\n",
    "    print('before reshape', x.shape)\n",
    "    x = tf.reshape(x, [batch_size, self.num_frames, -1])\n",
    "    within_period_x = x\n",
    "    print('x-------', x.shape)\n",
    "\n",
    "    # Period prediction.\n",
    "    x = self.input_projection(x)\n",
    "    x += self.pos_encoding\n",
    "    for transformer_layer in self.transformer_layers:\n",
    "      x = transformer_layer(x)\n",
    "    x = flatten_sequential_feats(x, batch_size, self.num_frames)\n",
    "    for fc_layer in self.fc_layers:\n",
    "      x = self.dropout_layer(x)\n",
    "      x = fc_layer(x)\n",
    "\n",
    "    print('x-----2-', x.shape)\n",
    "    # Within period prediction.\n",
    "    within_period_x = self.input_projection2(within_period_x)\n",
    "    within_period_x += self.pos_encoding2\n",
    "    for transformer_layer in self.transformer_layers2:\n",
    "      within_period_x = transformer_layer(within_period_x)\n",
    "    within_period_x = flatten_sequential_feats(within_period_x,\n",
    "                                               batch_size,\n",
    "                                               self.num_frames)\n",
    "    for fc_layer in self.within_period_fc_layers:\n",
    "      within_period_x = self.dropout_layer(within_period_x)\n",
    "      within_period_x = fc_layer(within_period_x)\n",
    "\n",
    "    print('x-----3-', x.shape)\n",
    "    print('x-----4-', within_period_x.shape)\n",
    "    return x, within_period_x, final_embs\n",
    "\n",
    "  @tf.function\n",
    "  def preprocess(self, imgs):\n",
    "    imgs = tf.cast(imgs, tf.float32)\n",
    "    imgs -= 127.5\n",
    "    imgs /= 127.5\n",
    "    imgs = tf.image.resize(imgs, (self.image_size, self.image_size))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def get_sims(embs, temperature):\n",
    "  \"\"\"Calculates self-similarity between batch of sequence of embeddings.\"\"\"\n",
    "  batch_size = tf.shape(embs)[0]\n",
    "  seq_len = tf.shape(embs)[1]\n",
    "  embs = tf.reshape(embs, [batch_size, seq_len, -1])\n",
    "\n",
    "  def _get_sims(embs):\n",
    "    \"\"\"Calculates self-similarity between sequence of embeddings.\"\"\"\n",
    "    dist = pairwise_l2_distance(embs, embs)\n",
    "    sims = -1.0 * dist\n",
    "    return sims\n",
    "\n",
    "  sims = tf.map_fn(_get_sims, embs)\n",
    "  sims /= temperature\n",
    "  sims = tf.nn.softmax(sims, axis=-1)\n",
    "  sims = tf.expand_dims(sims, -1)\n",
    "  return sims\n",
    "\n",
    "\n",
    "def flatten_sequential_feats(x, batch_size, seq_len):\n",
    "  \"\"\"Flattens sequential features with known batch size and seq_len.\"\"\"\n",
    "  x = tf.reshape(x, [batch_size, seq_len, -1])\n",
    "  return x\n",
    "\n",
    "\n",
    "# Transformer from https://www.tensorflow.org/tutorials/text/transformer .\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead)\n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    outputs: shape == (..., seq_len_q, depth_v)\n",
    "    attention_weights: shape == (..., seq_len_q, seq_len_k)\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk.\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  # (..., seq_len_q, seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "  outputs = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return outputs, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model)\n",
    "  ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  \"\"\"Multi-headed attention layer.\"\"\"\n",
    "\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(\n",
    "        q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(\n",
    "        k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(\n",
    "        v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(\n",
    "        scaled_attention,\n",
    "        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(\n",
    "        scaled_attention,\n",
    "        (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    outputs = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return outputs, attention_weights\n",
    "\n",
    "\n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Implements a single transformer layer (https://arxiv.org/abs/1706.03762).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, d_model, num_heads, dff,\n",
    "               dropout_rate=0.1,\n",
    "               reorder_ln=False):\n",
    "    super(TransformerLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    self.reorder_ln = reorder_ln\n",
    "\n",
    "  def call(self, x):\n",
    "    inp_x = x\n",
    "\n",
    "    if self.reorder_ln:\n",
    "      x = self.layernorm1(x)\n",
    "\n",
    "    # (batch_size, input_seq_len, d_model)\n",
    "    attn_output, _ = self.mha(x, x, x, mask=None)\n",
    "    attn_output = self.dropout1(attn_output)\n",
    "\n",
    "    if self.reorder_ln:\n",
    "      out1 = inp_x + attn_output\n",
    "      x = out1\n",
    "    else:\n",
    "      # (batch_size, input_seq_len, d_model)\n",
    "      out1 = self.layernorm1(x + attn_output)\n",
    "      x = out1\n",
    "\n",
    "    if self.reorder_ln:\n",
    "      x = self.layernorm2(x)\n",
    "\n",
    "    # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.ffn(x)\n",
    "    ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "    if self.reorder_ln:\n",
    "      out2 = out1 + ffn_output\n",
    "    else:\n",
    "      # (batch_size, input_seq_len, d_model)\n",
    "      out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    return out2\n",
    "\n",
    "\n",
    "def pairwise_l2_distance(a, b):\n",
    "  \"\"\"Computes pairwise distances between all rows of a and all rows of b.\"\"\"\n",
    "  norm_a = tf.reduce_sum(tf.square(a), 1)\n",
    "  norm_a = tf.reshape(norm_a, [-1, 1])\n",
    "  norm_b = tf.reduce_sum(tf.square(b), 1)\n",
    "  norm_b = tf.reshape(norm_b, [1, -1])\n",
    "  dist = tf.maximum(norm_a - 2.0 * tf.matmul(a, b, False, True) + norm_b, 0.0)\n",
    "  return dist\n",
    "\n",
    "\n",
    "def get_repnet_model(logdir):\n",
    "  \"\"\"Returns a trained RepNet model.\n",
    "\n",
    "  Args:\n",
    "    logdir (string): Path to directory where checkpoint will be downloaded.\n",
    "\n",
    "  Returns:\n",
    "    model (Keras model): Trained RepNet model.\n",
    "  \"\"\"\n",
    "  # Check if we are in eager mode.\n",
    "  assert tf.executing_eagerly()\n",
    "\n",
    "  # Models will be called in eval mode.\n",
    "  tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "  # Define RepNet model.\n",
    "  model = ResnetPeriodEstimator()\n",
    "  # tf.function for speed.\n",
    "  model.call = tf.function(model.call)\n",
    "\n",
    "  # Define checkpoint and checkpoint manager.\n",
    "  ckpt = tf.train.Checkpoint(model=model)\n",
    "  ckpt_manager = tf.train.CheckpointManager(\n",
    "      ckpt, directory=logdir, max_to_keep=10)\n",
    "  latest_ckpt = ckpt_manager.latest_checkpoint\n",
    "  print('Loading from: ', latest_ckpt)\n",
    "  if not latest_ckpt:\n",
    "    raise ValueError('Path does not have a checkpoint to load.')\n",
    "  # Restore weights.\n",
    "  ckpt.restore(latest_ckpt).expect_partial()\n",
    "\n",
    "  # Pass dummy frames to build graph.\n",
    "  model(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "  return model\n",
    "\n",
    "\n",
    "def unnorm(query_frame):\n",
    "  min_v = query_frame.min()\n",
    "  max_v = query_frame.max()\n",
    "  query_frame = (query_frame - min_v) / max(1e-7, (max_v - min_v))\n",
    "  return query_frame\n",
    "\n",
    "\n",
    "def create_count_video(frames,\n",
    "                       per_frame_counts,\n",
    "                       within_period,\n",
    "                       score,\n",
    "                       fps,\n",
    "                       output_file,\n",
    "                       delay,\n",
    "                       plot_count=True,\n",
    "                       plot_within_period=False,\n",
    "                       plot_score=False):\n",
    "  \"\"\"Creates video with running count and within period predictions.\n",
    "\n",
    "  Args:\n",
    "    frames (List): List of images in form of NumPy arrays.\n",
    "    per_frame_counts (List): List of floats indicating repetition count for\n",
    "      each frame. This is the rate of repetition for that particular frame.\n",
    "      Summing this list up gives count over entire video.\n",
    "    within_period (List): List of floats indicating score between 0 and 1 if the\n",
    "      frame is inside the periodic/repeating portion of a video or not.\n",
    "    score (float): Score between 0 and 1 indicating the confidence of the\n",
    "      RepNet model's count predictions.\n",
    "    fps (int): Frames per second of the input video. Used to scale the\n",
    "      repetition rate predictions to Hz.\n",
    "    output_file (string): Path of the output video.\n",
    "    delay (integer): Delay between each frame in the output video.\n",
    "    plot_count (boolean): if True plots the count in the output video.\n",
    "    plot_within_period (boolean): if True plots the per-frame within period\n",
    "      scores.\n",
    "    plot_score (boolean): if True plots the confidence of the model along with\n",
    "      count ot within_period scores.\n",
    "  \"\"\"\n",
    "  if output_file[-4:] not in ['.mp4', '.gif']:\n",
    "    raise ValueError('Output format can only be mp4 or gif')\n",
    "  num_frames = len(frames)\n",
    "\n",
    "  running_counts = np.cumsum(per_frame_counts)\n",
    "  final_count = running_counts[-1]\n",
    "\n",
    "  def count(idx):\n",
    "    return int(np.round(running_counts[idx]))\n",
    "\n",
    "  def rate(idx):\n",
    "    return per_frame_counts[idx] * fps\n",
    "\n",
    "  if plot_count and not plot_within_period:\n",
    "    fig = plt.figure(figsize=(10, 12), tight_layout=True)\n",
    "    im = plt.imshow(unnorm(frames[0]))\n",
    "    if plot_score:\n",
    "      plt.suptitle('Pred Count: %d, '\n",
    "                   'Prob: %0.1f' % (int(np.around(final_count)), score),\n",
    "                   fontsize=24)\n",
    "\n",
    "    plt.title('Count 0,create_count_video Rate: 0', fontsize=24)\n",
    "    plt.axis('off')\n",
    "    plt.grid(b=None)\n",
    "    def update_count_plot(i):\n",
    "      \"\"\"Updates the count plot.\"\"\"\n",
    "      im.set_data(unnorm(frames[i]))\n",
    "      plt.title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)), fontsize=24)\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update_count_plot,\n",
    "        frames=np.arange(1, num_frames),\n",
    "        interval=delay,\n",
    "        blit=False)\n",
    "    if output_file[-3:] == 'mp4':\n",
    "      anim.save(output_file, dpi=100, fps=24)\n",
    "    elif output_file[-3:] == 'gif':\n",
    "      anim.save(output_file, writer='imagemagick', fps=24, dpi=100)\n",
    "\n",
    "  elif plot_within_period:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    im = axs[0].imshow(unnorm(frames[0]))\n",
    "    axs[1].plot(0, within_period[0])\n",
    "    axs[1].set_xlim((0, len(frames)))\n",
    "    axs[1].set_ylim((0, 1))\n",
    "\n",
    "    if plot_score:\n",
    "      plt.suptitle('Pred Count: %d, '\n",
    "                   'Prob: %0.1f' % (int(np.around(final_count)), score),\n",
    "                   fontsize=24)\n",
    "\n",
    "    if plot_count:\n",
    "      axs[0].set_title('Count 0, Rate: 0', fontsize=20)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.grid(b=None)\n",
    "\n",
    "    def update_within_period_plot(i):\n",
    "      \"\"\"Updates the within period plot along with count.\"\"\"\n",
    "      im.set_data(unnorm(frames[i]))\n",
    "      axs[0].set_xticks([])\n",
    "      axs[0].set_yticks([])\n",
    "      xs = []\n",
    "      ys = []\n",
    "      if plot_count:\n",
    "        axs[0].set_title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)),\n",
    "                         fontsize=20)\n",
    "      for idx in range(i):\n",
    "        xs.append(idx)\n",
    "        ys.append(within_period[int(idx * len(within_period) / num_frames)])\n",
    "      axs[1].clear()\n",
    "      axs[1].set_title('Within Period or Not', fontsize=20)\n",
    "      axs[1].set_xlim((0, num_frames))\n",
    "      axs[1].set_ylim((-0.05, 1.05))\n",
    "      axs[1].plot(xs, ys)\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update_within_period_plot,\n",
    "        frames=np.arange(1, num_frames),\n",
    "        interval=delay,\n",
    "        blit=False,\n",
    "    )\n",
    "    if output_file[-3:] == 'mp4':\n",
    "      anim.save(output_file, dpi=100, fps=24)\n",
    "    elif output_file[-3:] == 'gif':\n",
    "      anim.save(output_file, writer='imagemagick', fps=24, dpi=100)\n",
    "\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def show_video(video_path):\n",
    "  mp4 = open(video_path, 'rb').read()\n",
    "  data_url = 'data:video/mp4;base64,' + base64.b64encode(mp4).decode()\n",
    "  return HTML(\"\"\"<video width=600 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\"></video>\n",
    "  \"\"\" % data_url)\n",
    "\n",
    "\n",
    "def viz_reps(frames,\n",
    "             count,\n",
    "             score,\n",
    "             alpha=1.0,\n",
    "             pichart=True,\n",
    "             colormap=plt.cm.PuBu,\n",
    "             num_frames=None,\n",
    "             interval=30,\n",
    "             plot_score=True):\n",
    "  \"\"\"Visualize repetitions.\"\"\"\n",
    "  if isinstance(count, list):\n",
    "    counts = len(frames) * [count/len(frames)]\n",
    "  else:\n",
    "    counts = count\n",
    "    \n",
    "  sum_counts = np.cumsum(counts)\n",
    "  tmp_path = '/tmp/output.mp4'\n",
    "  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 5),\n",
    "                         tight_layout=True,)\n",
    "\n",
    "  h, w, _ = np.shape(frames[0])\n",
    "  wedge_x = 95 / 112 * w\n",
    "  wedge_y = 17 / 112 * h\n",
    "  wedge_r = 15 / 112 * h\n",
    "  txt_x = 95 / 112 * w\n",
    "  txt_y = 19 / 112 * h\n",
    "  otxt_size = 62 / 112 * h\n",
    "\n",
    "  if plot_score:\n",
    "    plt.title('Score:%.2f' % score, fontsize=20)\n",
    "  im0 = ax.imshow(unnorm(frames[0]))\n",
    "\n",
    "  if not num_frames:\n",
    "    num_frames = len(frames)\n",
    "\n",
    "  if pichart:\n",
    "    wedge1 = matplotlib.patches.Wedge(\n",
    "        center=(wedge_x, wedge_y),\n",
    "        r=wedge_r,\n",
    "        theta1=0,\n",
    "        theta2=0,\n",
    "        color=colormap(1.),\n",
    "        alpha=alpha)\n",
    "    wedge2 = matplotlib.patches.Wedge(\n",
    "        center=(wedge_x, wedge_y),\n",
    "        r=wedge_r,\n",
    "        theta1=0,\n",
    "        theta2=0,\n",
    "        color=colormap(0.5),\n",
    "        alpha=alpha)\n",
    "\n",
    "    ax.add_patch(wedge1)\n",
    "    ax.add_patch(wedge2)\n",
    "    txt = ax.text(\n",
    "        txt_x,\n",
    "        txt_y,\n",
    "        '0',\n",
    "        size=35,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        alpha=0.9,\n",
    "        color='white',\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    txt = ax.text(\n",
    "        txt_x,\n",
    "        txt_y,\n",
    "        '0',\n",
    "        size=otxt_size,\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        alpha=0.8,\n",
    "        color=colormap(0.4),\n",
    "    )\n",
    "\n",
    "  def update(i):\n",
    "    \"\"\"Update plot with next frame.\"\"\"\n",
    "    im0.set_data(unnorm(frames[i]))\n",
    "    ctr = int(sum_counts[i])\n",
    "    if pichart:\n",
    "      if ctr%2 == 0:\n",
    "        wedge1.set_color(colormap(1.0))\n",
    "        wedge2.set_color(colormap(0.5))\n",
    "      else:\n",
    "        wedge1.set_color(colormap(0.5))\n",
    "        wedge2.set_color(colormap(1.0))\n",
    "\n",
    "      wedge1.set_theta1(-90)\n",
    "      wedge1.set_theta2(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
    "      wedge2.set_theta1(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
    "      wedge2.set_theta2(-90)\n",
    "\n",
    "    txt.set_text(int(sum_counts[i]))\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "\n",
    "  anim = FuncAnimation(\n",
    "      fig,\n",
    "      update,\n",
    "      frames=num_frames,\n",
    "      interval=interval,\n",
    "      blit=False)\n",
    "  anim.save(tmp_path, dpi=80)\n",
    "  plt.close()\n",
    "  return show_video(tmp_path)\n",
    "\n",
    "\n",
    "def record_video(interval_in_ms, num_frames, quality=0.8):\n",
    "  \"\"\"Capture video from webcam.\"\"\"\n",
    "  # https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb.\n",
    "\n",
    "  # Give warning before recording.\n",
    "  for i in range(0, 3):\n",
    "    print('Opening webcam in %d seconds'%(3-i))\n",
    "    time.sleep(1)\n",
    "    output.clear('status_text')\n",
    "\n",
    "  js = Javascript('''\n",
    "    async function recordVideo(interval_in_ms, num_frames, quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      // show the video in the HTML element\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight,\n",
    "        true);\n",
    "\n",
    "      for (let i = 0; i < num_frames; i++) {\n",
    "        const canvas = document.createElement('canvas');\n",
    "        canvas.width = video.videoWidth;\n",
    "        canvas.height = video.videoHeight;\n",
    "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "        img = canvas.toDataURL('image/jpeg', quality);\n",
    "        google.colab.kernel.invokeFunction(\n",
    "        'notebook.get_webcam_video', [img], {});\n",
    "        await new Promise(resolve => setTimeout(resolve, interval_in_ms));\n",
    "      }\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  eval_js('recordVideo({},{},{})'.format(interval_in_ms, num_frames, quality))\n",
    "\n",
    "\n",
    "def data_uri_to_img(uri):\n",
    "  \"\"\"Convert base64image to Numpy array.\"\"\"\n",
    "  image = base64.b64decode(uri.split(',')[1], validate=True)\n",
    "  # Binary string to PIL image.\n",
    "  image = Image.open(io.BytesIO(image))\n",
    "  image = image.resize((224, 224))\n",
    "  # PIL to Numpy array.\n",
    "  image = np.array(np.array(image, dtype=np.uint8), np.float32)\n",
    "  return image\n",
    "\n",
    "\n",
    "def read_video(video_filename, width=224, height=224):\n",
    "  \"\"\"Read video from file.\"\"\"\n",
    "  cap = cv2.VideoCapture(video_filename)\n",
    "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "  frames = []\n",
    "  if cap.isOpened():\n",
    "    while True:\n",
    "      success, frame_bgr = cap.read()\n",
    "      if not success:\n",
    "        break\n",
    "      frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "      frame_rgb = cv2.resize(frame_rgb, (width, height))\n",
    "      frames.append(frame_rgb)\n",
    "  frames = np.asarray(frames)\n",
    "  return frames, fps\n",
    "\n",
    "\n",
    "def get_webcam_video(img_b64):\n",
    "  \"\"\"Populates global variable imgs by converting image URI to Numpy array.\"\"\"\n",
    "  image = data_uri_to_img(img_b64)\n",
    "  imgs.append(image)\n",
    "\n",
    "\n",
    "def download_video_from_url(url_to_video,\n",
    "                            path_to_video='/tmp/video.mp4'):\n",
    "  if os.path.exists(path_to_video):\n",
    "    os.remove(path_to_video)\n",
    "  ydl_opts = {\n",
    "      'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4',\n",
    "      'outtmpl': str(path_to_video),\n",
    "  }\n",
    "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url_to_video])\n",
    "\n",
    "\n",
    "def get_score(period_score, within_period_score):\n",
    "  \"\"\"Combine the period and periodicity scores.\"\"\"\n",
    "  within_period_score = tf.nn.sigmoid(within_period_score)[:, 0]\n",
    "  per_frame_periods = tf.argmax(period_score, axis=-1) + 1\n",
    "  pred_period_conf = tf.reduce_max(\n",
    "      tf.nn.softmax(period_score, axis=-1), axis=-1)\n",
    "  pred_period_conf = tf.where(\n",
    "      tf.math.less(per_frame_periods, 3), 0.0, pred_period_conf)\n",
    "  within_period_score *= pred_period_conf\n",
    "  within_period_score = np.sqrt(within_period_score)\n",
    "  pred_score = tf.reduce_mean(within_period_score)\n",
    "  return pred_score, within_period_score\n",
    "\n",
    "\n",
    "def get_counts(model, frames, strides, batch_size,\n",
    "               threshold,\n",
    "               within_period_threshold,\n",
    "               constant_speed=False,\n",
    "               median_filter=False,\n",
    "               fully_periodic=False):\n",
    "  \"\"\"Pass frames through model and conver period predictions to count.\"\"\"\n",
    "  seq_len = len(frames)\n",
    "  raw_scores_list = []\n",
    "  scores = []\n",
    "  within_period_scores_list = []\n",
    "\n",
    "  if fully_periodic:\n",
    "    within_period_threshold = 0.0\n",
    "\n",
    "  frames = model.preprocess(frames)\n",
    "\n",
    "  for stride in strides:\n",
    "    num_batches = int(np.ceil(seq_len/model.num_frames/stride/batch_size))\n",
    "    raw_scores_per_stride = []\n",
    "    within_period_score_stride = []\n",
    "    for batch_idx in range(num_batches):\n",
    "      idxes = tf.range(batch_idx*batch_size*model.num_frames*stride,\n",
    "                       (batch_idx+1)*batch_size*model.num_frames*stride,\n",
    "                       stride)\n",
    "      idxes = tf.clip_by_value(idxes, 0, seq_len-1)\n",
    "      curr_frames = tf.gather(frames, idxes)\n",
    "      curr_frames = tf.reshape(\n",
    "          curr_frames,\n",
    "          [batch_size, model.num_frames, model.image_size, model.image_size, 3])\n",
    "\n",
    "      raw_scores, within_period_scores, _ = model(curr_frames)\n",
    "      raw_scores_per_stride.append(np.reshape(raw_scores.numpy(),\n",
    "                                              [-1, model.num_frames//2]))\n",
    "      within_period_score_stride.append(np.reshape(within_period_scores.numpy(),\n",
    "                                                   [-1, 1]))\n",
    "    raw_scores_per_stride = np.concatenate(raw_scores_per_stride, axis=0)\n",
    "    raw_scores_list.append(raw_scores_per_stride)\n",
    "    within_period_score_stride = np.concatenate(\n",
    "        within_period_score_stride, axis=0)\n",
    "    pred_score, within_period_score_stride = get_score(\n",
    "        raw_scores_per_stride, within_period_score_stride)\n",
    "    scores.append(pred_score)\n",
    "    within_period_scores_list.append(within_period_score_stride)\n",
    "\n",
    "  # Stride chooser\n",
    "  argmax_strides = np.argmax(scores)\n",
    "  chosen_stride = strides[argmax_strides]\n",
    "  raw_scores = np.repeat(\n",
    "      raw_scores_list[argmax_strides], chosen_stride, axis=0)[:seq_len]\n",
    "  within_period = np.repeat(\n",
    "      within_period_scores_list[argmax_strides], chosen_stride,\n",
    "      axis=0)[:seq_len]\n",
    "  within_period_binary = np.asarray(within_period > within_period_threshold)\n",
    "  if median_filter:\n",
    "    within_period_binary = medfilt(within_period_binary, 5)\n",
    "\n",
    "  # Select Periodic frames\n",
    "  periodic_idxes = np.where(within_period_binary)[0]\n",
    "\n",
    "  if constant_speed:\n",
    "    # Count by averaging predictions. Smoother but\n",
    "    # assumes constant speed.\n",
    "    scores = tf.reduce_mean(\n",
    "        tf.nn.softmax(raw_scores[periodic_idxes], axis=-1), axis=0)\n",
    "    max_period = np.argmax(scores)\n",
    "    pred_score = scores[max_period]\n",
    "    pred_period = chosen_stride * (max_period + 1)\n",
    "    per_frame_counts = (\n",
    "        np.asarray(seq_len * [1. / pred_period]) *\n",
    "        np.asarray(within_period_binary))\n",
    "  else:\n",
    "    # Count each frame. More noisy but adapts to changes in speed.\n",
    "    pred_score = tf.reduce_mean(within_period)\n",
    "    per_frame_periods = tf.argmax(raw_scores, axis=-1) + 1\n",
    "    per_frame_counts = tf.where(\n",
    "        tf.math.less(per_frame_periods, 3),\n",
    "        0.0,\n",
    "        tf.math.divide(1.0,\n",
    "                       tf.cast(chosen_stride * per_frame_periods, tf.float32)),\n",
    "    )\n",
    "    if median_filter:\n",
    "      per_frame_counts = medfilt(per_frame_counts, 5)\n",
    "\n",
    "    per_frame_counts *= np.asarray(within_period_binary)\n",
    "\n",
    "    pred_period = seq_len/np.sum(per_frame_counts)\n",
    "\n",
    "  if pred_score < threshold:\n",
    "    print('No repetitions detected in video as score '\n",
    "          '%0.2f is less than threshold %0.2f.'%(pred_score, threshold))\n",
    "    per_frame_counts = np.asarray(len(per_frame_counts) * [0.])\n",
    "\n",
    "  return (pred_period, pred_score, within_period,\n",
    "          per_frame_counts, chosen_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPpgGXG_aalo"
   },
   "source": [
    "## Load trained RepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:05:49.513671Z",
     "start_time": "2021-08-26T03:05:49.466116Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResnetPeriodEstimator(tf.keras.models.Model):\n",
    "  \"\"\"RepNet model.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_frames=64,\n",
    "      image_size=112,\n",
    "      base_model_layer_name='conv4_block3_out',\n",
    "      temperature=13.544,\n",
    "      dropout_rate=0.25,\n",
    "      l2_reg_weight=1e-6,\n",
    "      temporal_conv_channels=512,\n",
    "      temporal_conv_kernel_size=3,\n",
    "      temporal_conv_dilation_rate=3,\n",
    "      conv_channels=32,\n",
    "      conv_kernel_size=3,\n",
    "      transformer_layers_config=((512, 4, 512),),\n",
    "      transformer_dropout_rate=0.0,\n",
    "      transformer_reorder_ln=True,\n",
    "      period_fc_channels=(512, 512),\n",
    "      within_period_fc_channels=(512, 512)):\n",
    "    super(ResnetPeriodEstimator, self).__init__()\n",
    "\n",
    "    # Model params.\n",
    "    self.num_frames = num_frames\n",
    "    self.image_size = image_size\n",
    "\n",
    "    self.base_model_layer_name = base_model_layer_name\n",
    "\n",
    "    self.temperature = temperature\n",
    "\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.l2_reg_weight = l2_reg_weight\n",
    "\n",
    "    self.temporal_conv_channels = temporal_conv_channels\n",
    "    self.temporal_conv_kernel_size = temporal_conv_kernel_size\n",
    "    self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\n",
    "\n",
    "    self.conv_channels = conv_channels\n",
    "    self.conv_kernel_size = conv_kernel_size\n",
    "    # Transformer config in form of (channels, heads, bottleneck channels).\n",
    "    self.transformer_layers_config = transformer_layers_config\n",
    "    self.transformer_dropout_rate = transformer_dropout_rate\n",
    "    self.transformer_reorder_ln = transformer_reorder_ln\n",
    "\n",
    "    self.period_fc_channels = period_fc_channels\n",
    "    self.within_period_fc_channels = within_period_fc_channels\n",
    "\n",
    "    # Base ResNet50 Model.\n",
    "    base_model = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, pooling='max')\n",
    "    self.base_model = tf.keras.models.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=base_model.get_layer(self.base_model_layer_name).output)\n",
    "\n",
    "    # 3D Conv on k Frames\n",
    "    self.temporal_conv_layers = [\n",
    "        layers.Conv3D(self.temporal_conv_channels,\n",
    "                      self.temporal_conv_kernel_size,\n",
    "                      padding='same',\n",
    "                      dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\n",
    "                      kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "                      kernel_initializer='he_normal', name='RE_temporal_conv_layers')]\n",
    "    self.temporal_bn_layers = [layers.BatchNormalization(name='RE_temporal_bn_layers')\n",
    "                               for _ in self.temporal_conv_layers]\n",
    "\n",
    "    # Counting Module (Self-sim > Conv > Transformer > Classifier)\n",
    "    self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\n",
    "                                        self.conv_kernel_size,\n",
    "                                        padding='same',\n",
    "                                        activation=tf.nn.relu, name='RE_conv_3x3_layer')\n",
    "\n",
    "    channels = self.transformer_layers_config[0][0]\n",
    "    self.input_projection = layers.Dense(\n",
    "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "        activation=None, name='RE_input_projection')\n",
    "\n",
    "    self.input_projection2 = layers.Dense(\n",
    "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "        activation=None, name='RE_input_projection2')\n",
    "\n",
    "    length = self.num_frames\n",
    "    self.pos_encoding = tf.compat.v1.get_variable(\n",
    "        name='resnet_period_estimator/pos_encoding',\n",
    "        shape=[1, length, 1],\n",
    "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "    self.pos_encoding2 = tf.compat.v1.get_variable(\n",
    "        name='resnet_period_estimator/pos_encoding2',\n",
    "        shape=[1, length, 1],\n",
    "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    self.transformer_layers = []\n",
    "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "      self.transformer_layers.append(\n",
    "          TransformerLayer(d_model, num_heads, dff,\n",
    "                           self.transformer_dropout_rate,\n",
    "                           self.transformer_reorder_ln))\n",
    "\n",
    "    self.transformer_layers2 = []\n",
    "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
    "      self.transformer_layers2.append(\n",
    "          TransformerLayer(d_model, num_heads, dff,\n",
    "                           self.transformer_dropout_rate,\n",
    "                           self.transformer_reorder_ln))\n",
    "\n",
    "    # Period Prediction Module.\n",
    "    self.dropout_layer = layers.Dropout(self.dropout_rate, name='RE_dropout_layer')\n",
    "    num_preds = self.num_frames//2\n",
    "    self.fc_layers = []\n",
    "    for i, channels in enumerate(self.period_fc_channels):\n",
    "      self.fc_layers.append(layers.Dense(\n",
    "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "          activation=tf.nn.relu, name=f'RE_fc_layers_{i}'))\n",
    "    self.fc_layers.append(layers.Dense(\n",
    "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight), name='RE_fc_layers_x'))\n",
    "\n",
    "    # Within Period Module\n",
    "    num_preds = 1\n",
    "    self.within_period_fc_layers = []\n",
    "    for i, channels in enumerate(self.within_period_fc_channels):\n",
    "      self.within_period_fc_layers.append(layers.Dense(\n",
    "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
    "          activation=tf.nn.relu, name=f'RE_within_period_fc_layers_{i}'))\n",
    "    self.within_period_fc_layers.append(layers.Dense(\n",
    "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight), name='RE_within_period_fc_layers_x'))\n",
    "\n",
    "  def call(self, x):\n",
    "    # Ensures we are always using the right batch_size during train/eval.\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    # Conv Feature Extractor.\n",
    "    print('input x:', x.shape)\n",
    "    x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\n",
    "    print('before base_model', x.shape)\n",
    "    x = self.base_model(x)\n",
    "    print('after base_model', x.shape)\n",
    "    h = tf.shape(x)[1]\n",
    "    w = tf.shape(x)[2]\n",
    "    c = tf.shape(x)[3]\n",
    "    x = tf.reshape(x, [batch_size, -1, h, w, c])\n",
    "    print('reshape:', x.shape)\n",
    "\n",
    "    # 3D Conv to give temporal context to per-frame embeddings. \n",
    "    for bn_layer, conv_layer in zip(self.temporal_bn_layers,\n",
    "                                    self.temporal_conv_layers):\n",
    "      x = conv_layer(x)\n",
    "      x = bn_layer(x)\n",
    "      x = tf.nn.relu(x)\n",
    "    \n",
    "    print('temporal shape:', x.shape)\n",
    "\n",
    "    x = tf.reduce_max(x, [2, 3])\n",
    "    \n",
    "    print('reduce max shape:', x.shape)\n",
    "\n",
    "    # Reshape and prepare embs for output.\n",
    "    final_embs = x\n",
    "\n",
    "    # Get self-similarity matrix.\n",
    "    x = get_sims(x, self.temperature)\n",
    "    \n",
    "    print('sims x:', x.shape)\n",
    "\n",
    "    # 3x3 conv layer on self-similarity matrix.\n",
    "    x = self.conv_3x3_layer(x)\n",
    "    print('before reshape: ', x.shape)\n",
    "    x = tf.reshape(x, [batch_size, self.num_frames, -1])\n",
    "    print('after reshape', x.shape)\n",
    "    within_period_x = x\n",
    "\n",
    "    # Period prediction.\n",
    "    x = self.input_projection(x)\n",
    "    print('input_projection:', x.shape)\n",
    "    x += self.pos_encoding\n",
    "    for transformer_layer in self.transformer_layers:\n",
    "      x = transformer_layer(x)\n",
    "    x = flatten_sequential_feats(x, batch_size, self.num_frames)\n",
    "    for fc_layer in self.fc_layers:\n",
    "      x = self.dropout_layer(x)\n",
    "      x = fc_layer(x)\n",
    "\n",
    "    # Within period prediction.\n",
    "    within_period_x = self.input_projection2(within_period_x)\n",
    "    within_period_x += self.pos_encoding2\n",
    "    for transformer_layer in self.transformer_layers2:\n",
    "      within_period_x = transformer_layer(within_period_x)\n",
    "    within_period_x = flatten_sequential_feats(within_period_x,\n",
    "                                               batch_size,\n",
    "                                               self.num_frames)\n",
    "    for fc_layer in self.within_period_fc_layers:\n",
    "      within_period_x = self.dropout_layer(within_period_x)\n",
    "      within_period_x = fc_layer(within_period_x)\n",
    "\n",
    "    return x, within_period_x, final_embs\n",
    "\n",
    "  @tf.function\n",
    "  def preprocess(self, imgs):\n",
    "    imgs = tf.cast(imgs, tf.float32)\n",
    "    imgs -= 127.5\n",
    "    imgs /= 127.5\n",
    "    imgs = tf.image.resize(imgs, (self.image_size, self.image_size))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T14:14:32.343899Z",
     "start_time": "2021-08-25T14:14:28.437770Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_repnet_model(logdir):\n",
    "    # Models will be called in eval mode.\n",
    "    # tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "    # Define RepNet model.\n",
    "    model = ResnetPeriodEstimator()\n",
    "    \n",
    "    # tf.function for speed.\n",
    "    # model.call = tf.function(model.call)\n",
    "\n",
    "    # Define checkpoint and checkpoint manager.\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(\n",
    "        ckpt, directory=logdir, max_to_keep=10)\n",
    "    latest_ckpt = ckpt_manager.latest_checkpoint\n",
    "    print('Loading from: ', latest_ckpt)\n",
    "    if not latest_ckpt:\n",
    "        raise ValueError('Path does not have a checkpoint to load.')\n",
    "        # Restore weights.\n",
    "    ckpt.restore(latest_ckpt).expect_partial()\n",
    "    # Pass dummy frames to build graph.\n",
    "    model(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "    return model\n",
    "\n",
    "PATH_TO_CKPT = '/data/pretrained/cv/repnet'\n",
    "model = get_repnet_model(PATH_TO_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:06:12.681280Z",
     "start_time": "2021-08-26T03:06:10.708122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3e9d431b70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = ResnetPeriodEstimator()\n",
    "ckpt = tf.train.Checkpoint(model=model1)\n",
    "ckpt.restore('/data/pretrained/cv/repnet/ckpt-88').expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:08:46.112728Z",
     "start_time": "2021-08-26T03:08:09.561807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input x: (1, 64, 112, 112, 3)\n",
      "before base_model (64, 112, 112, 3)\n",
      "after base_model (64, 7, 7, 1024)\n",
      "reshape: (1, 64, 7, 7, 1024)\n",
      "temporal shape: (1, 64, 7, 7, 512)\n",
      "reduce max shape: (1, 64, 512)\n",
      "sims x: (1, 64, 64, 1)\n",
      "before reshape:  (1, 64, 64, 32)\n",
      "after reshape (1, 64, 2048)\n",
      "input_projection: (1, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n",
      "input x: (None, 64, 112, 112, 3)\n",
      "before base_model (None, 112, 112, 3)\n",
      "after base_model (None, 7, 7, 1024)\n",
      "reshape: (None, None, None, None, None)\n",
      "temporal shape: (None, None, None, None, 512)\n",
      "reduce max shape: (None, None, 512)\n",
      "sims x: (None, None, None, 1)\n",
      "before reshape:  (None, None, None, 32)\n",
      "after reshape (None, 64, None)\n",
      "input_projection: (None, 64, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as preprocess, multi_head_attention_2_layer_call_fn, multi_head_attention_2_layer_call_and_return_conditional_losses, layer_normalization_4_layer_call_fn, layer_normalization_4_layer_call_and_return_conditional_losses while saving (showing 5 of 91). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/cc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /data/cc/assets\n",
      "/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "model1 = ResnetPeriodEstimator()\n",
    "model1(tf.random.uniform((1, 64, 112, 112, 3)));\n",
    "os.system('rm -rf /data/cc')\n",
    "model1.save(\"/data/cc\", save_format='tf', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-26T03:09:39.906810Z",
     "start_time": "2021-08-26T03:08:46.131018Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-26 11:08:47.194060: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-08-26 11:08:47.194112: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2021-08-26 11:08:49.033172: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-08-26 11:08:49.033236: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: k12nb\n",
      "2021-08-26 11:08:49.033315: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: k12nb\n",
      "2021-08-26 11:08:49.033447: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.67.0\n",
      "2021-08-26 11:08:49.033496: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.67.0\n",
      "2021-08-26 11:08:49.033556: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.67.0\n",
      "2021-08-26 11:08:49.033924: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-26 11:09:00,078 - INFO - tf2onnx.tf_loader: Signatures found in model: [serving_default].\n",
      "2021-08-26 11:09:00,079 - INFO - tf2onnx.tf_loader: Output names: ['output_1', 'output_2', 'output_3']\n",
      "2021-08-26 11:09:00.121281: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-08-26 11:09:00.121501: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-08-26 11:09:00.251859: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 1615 nodes (1383), 2238 edges (2004), time = 52.339ms.\n",
      "  function_optimizer: Graph size after: 1615 nodes (0), 2238 edges (0), time = 22.895ms.\n",
      "Optimization results for grappler item: __inference_resnet_period_estimator_1_map_while_body_49858_9668\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.015ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "Optimization results for grappler item: __inference_resnet_period_estimator_1_map_while_cond_49857_10197\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.007ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:662: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2021-08-26 11:09:05,409 - WARNING - tensorflow: From /usr/local/lib/python3.6/dist-packages/tf2onnx/tf_loader.py:662: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2021-08-26 11:09:05.489114: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-08-26 11:09:05.489334: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-08-26 11:09:07.487404: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 1141 nodes (-469), 1762 edges (-474), time = 973.587ms.\n",
      "  function_optimizer: Graph size after: 1141 nodes (0), 1762 edges (0), time = 214.531ms.\n",
      "  constant_folding: Graph size after: 1141 nodes (0), 1762 edges (0), time = 241.791ms.\n",
      "  function_optimizer: Graph size after: 1141 nodes (0), 1762 edges (0), time = 218.29ms.\n",
      "Optimization results for grappler item: __inference_resnet_period_estimator_1_map_while_body_49858_9668\n",
      "  constant_folding: Graph size after: 42 nodes (0), 42 edges (0), time = 1.36ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.077ms.\n",
      "  constant_folding: Graph size after: 42 nodes (0), 42 edges (0), time = 1.257ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.049ms.\n",
      "Optimization results for grappler item: __inference_resnet_period_estimator_1_map_while_cond_49857_10197\n",
      "  constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.54ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.026ms.\n",
      "  constant_folding: Graph size after: 11 nodes (0), 8 edges (0), time = 0.284ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.017ms.\n",
      "\n",
      "2021-08-26 11:09:08,039 - INFO - tf2onnx: inputs: ['input_1:0']\n",
      "2021-08-26 11:09:08,040 - INFO - tf2onnx: outputs: ['Identity:0', 'Identity_1:0', 'Identity_2:0']\n",
      "2021-08-26 11:09:08,644 - INFO - tf2onnx.tfonnx: Using tensorflow=2.6.0, onnx=1.8.1, tf2onnx=1.9.1/8e8c23\n",
      "2021-08-26 11:09:08,644 - INFO - tf2onnx.tfonnx: Using opset <onnx, 13>\n",
      "2021-08-26 11:09:14,147 - INFO - tf2onnx.tf_utils: Computed 3 values for constant folding\n",
      "2021-08-26 11:09:28,130 - INFO - tf2onnx.tf_utils: Computed 0 values for constant folding\n",
      "2021-08-26 11:09:28,154 - VERBOSE - tf2onnx.tfonnx: Mapping TF node to ONNX node(s)\n",
      "2021-08-26 11:09:28,160 - VERBOSE - tf2onnx.tfonnx: Summay Stats:\n",
      "\ttensorflow ops: Counter({'Const': 10, 'Placeholder': 6, 'Identity': 4, 'AddV2': 3, 'Square': 2, 'Sum': 2, 'Mul': 2, 'Reshape': 2, 'TensorListGetItem': 1, 'MatMul': 1, 'Sub': 1, 'Maximum': 1, 'TensorListSetItem': 1})\n",
      "\ttensorflow attr: Counter({'T': 36, 'dtype': 32, 'value': 20, 'shape': 12, 'element_dtype': 4, 'Tidx': 4, 'keep_dims': 4, 'Tshape': 4, 'transpose_a': 2, 'transpose_b': 2})\n",
      "\tonnx mapped: Counter({'Const': 10, 'Placeholder': 6, 'Identity': 4, 'AddV2': 3, 'Square': 2, 'Sum': 2, 'Reshape': 2, 'Mul': 2, 'TensorListGetItem': 1, 'MatMul': 1, 'Sub': 1, 'Maximum': 1, 'TensorListSetItem': 1})\n",
      "\tonnx unmapped: Counter()\n",
      "2021-08-26 11:09:28,161 - INFO - tf2onnx.tf_utils: Computed 0 values for constant folding\n",
      "2021-08-26 11:09:28,168 - VERBOSE - tf2onnx.tfonnx: Mapping TF node to ONNX node(s)\n",
      "2021-08-26 11:09:28,170 - VERBOSE - tf2onnx.tfonnx: Summay Stats:\n",
      "\ttensorflow ops: Counter({'Placeholder': 6, 'Less': 2, 'LogicalAnd': 1, 'Identity': 1})\n",
      "\ttensorflow attr: Counter({'dtype': 12, 'shape': 12, 'T': 6})\n",
      "\tonnx mapped: Counter({'Placeholder': 6, 'Less': 2, 'LogicalAnd': 1, 'Identity': 1})\n",
      "\tonnx unmapped: Counter()\n",
      "2021-08-26 11:09:28,219 - INFO - tf2onnx.tfonnx: folding node using tf type=StridedSlice, name=StatefulPartitionedCall/resnet_period_estimator_1/strided_slice_1\n",
      "2021-08-26 11:09:28,220 - INFO - tf2onnx.tfonnx: folding node using tf type=StridedSlice, name=StatefulPartitionedCall/resnet_period_estimator_1/strided_slice_2\n",
      "2021-08-26 11:09:28,220 - INFO - tf2onnx.tfonnx: folding node using tf type=StridedSlice, name=StatefulPartitionedCall/resnet_period_estimator_1/strided_slice_3\n",
      "2021-08-26 11:09:28,752 - VERBOSE - tf2onnx.tfonnx: Mapping TF node to ONNX node(s)\n",
      "2021-08-26 11:09:29,953 - VERBOSE - tf2onnx.tfonnx: Summay Stats:\n",
      "\ttensorflow ops: Counter({'Const': 574, 'Identity': 80, 'Reshape': 54, 'Pack': 42, 'GatherV2': 40, 'Prod': 40, 'Relu': 38, 'Conv2D': 35, 'Shape': 32, 'FusedBatchNormV3': 31, 'AddV2': 26, 'StridedSlice': 20, 'ConcatV2': 20, 'MatMul': 20, 'Mul': 13, 'Pad': 12, 'Mean': 8, 'Transpose': 8, 'StopGradient': 4, 'SquaredDifference': 4, 'Rsqrt': 4, 'Neg': 4, 'BatchMatMulV2': 4, 'NoOp': 3, 'MaxPool': 3, 'Softmax': 3, 'FloorMod': 2, 'Cast': 2, 'Sqrt': 2, 'RealDiv': 2, 'Placeholder': 1, 'Sub': 1, 'SpaceToBatchND': 1, 'Conv3D': 1, 'BatchToSpaceND': 1, 'Max': 1, 'TensorListFromTensor': 1, 'TensorListReserve': 1, 'StatelessWhile': 1, 'TensorListStack': 1, 'ExpandDims': 1})\n",
      "\ttensorflow attr: Counter({'dtype': 1150, 'value': 1148, 'T': 1036, 'data_format': 140, 'Tidx': 138, 'N': 124, 'Tshape': 108, 'keep_dims': 98, 'axis': 84, 'Taxis': 80, 'Tparams': 80, 'Tindices': 80, 'batch_dims': 80, 'padding': 78, 'strides': 78, 'explicit_paddings': 76, 'dilations': 72, 'use_cudnn_on_gpu': 70, 'out_type': 64, 'exponential_avg_factor': 62, 'epsilon': 62, 'is_training': 62, 'U': 62, 'end_mask': 40, 'begin_mask': 40, 'ellipsis_mask': 40, 'new_axis_mask': 40, 'Index': 40, 'shrink_axis_mask': 40, 'transpose_b': 40, 'transpose_a': 40, 'Tpaddings': 26, 'Tperm': 16, 'adj_x': 8, 'adj_y': 8, 'ksize': 6, 'element_dtype': 6, '_acd_function_control_output': 4, 'Tblock_shape': 4, 'shape_type': 4, 'DstT': 4, 'SrcT': 4, 'Truncate': 4, 'shape': 2, 'Tcrops': 2, '_num_original_outputs': 2, 'parallel_iterations': 2, '_read_only_resource_inputs': 2, 'cond': 2, '_lower_using_switch_merge': 2, 'body': 2, 'output_shapes': 2, '_stateful_parallelism': 2, 'num_elements': 2, 'Tdim': 2})\n",
      "\tonnx mapped: Counter({'Const': 471, 'Identity': 80, 'Reshape': 54, 'Pack': 42, 'GatherV2': 40, 'Prod': 40, 'Relu': 38, 'FusedBatchNormV3': 31, 'Shape': 29, 'AddV2': 26, 'Conv2D': 24, 'ConcatV2': 20, 'MatMul': 20, 'StridedSlice': 17, 'Mul': 13, 'Mean': 8, 'Transpose': 8, 'StopGradient': 4, 'SquaredDifference': 4, 'Rsqrt': 4, 'Neg': 4, 'BatchMatMulV2': 4, 'MaxPool': 3, 'Softmax': 3, 'FloorMod': 2, 'Cast': 2, 'Sqrt': 2, 'RealDiv': 2, 'Placeholder': 1, 'Pad': 1, 'Sub': 1, 'SpaceToBatchND': 1, 'Conv3D': 1, 'BatchToSpaceND': 1, 'Max': 1, 'TensorListFromTensor': 1, 'TensorListReserve': 1, 'StatelessWhile': 1, 'TensorListStack': 1, 'ExpandDims': 1})\n",
      "\tonnx unmapped: Counter()\n",
      "2021-08-26 11:09:29,955 - INFO - tf2onnx.optimizer: Optimizing ONNX model\n",
      "2021-08-26 11:09:29,963 - VERBOSE - tf2onnx.optimizer: Apply optimize_transpose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-26 11:09:30,726 - VERBOSE - tf2onnx.optimizer.TransposeOptimizer: Concat +1 (68->69), Const -166 (682->516), Gather +6 (42->48), Identity -19 (102->83), Placeholder -1 (6->5), Reshape +1 (69->70), Shape +1 (33->34), Split +1 (0->1), Transpose -134 (158->24)\n",
      "2021-08-26 11:09:30,726 - VERBOSE - tf2onnx.optimizer: Apply remove_redundant_upsample\n",
      "2021-08-26 11:09:30,923 - VERBOSE - tf2onnx.optimizer.UpsampleOptimizer: no change\n",
      "2021-08-26 11:09:30,924 - VERBOSE - tf2onnx.optimizer: Apply fold_constants\n",
      "2021-08-26 11:09:31,178 - VERBOSE - tf2onnx.optimizer.ConstFoldOptimizer: Cast -7 (102->95), Concat -1 (69->68), Const -43 (516->473), Reshape -4 (70->66), Split -1 (1->0), Transpose -1 (24->23), Unsqueeze -39 (109->70)\n",
      "2021-08-26 11:09:31,178 - VERBOSE - tf2onnx.optimizer: Apply const_dequantize_optimizer\n",
      "2021-08-26 11:09:32,259 - VERBOSE - tf2onnx.optimizer.ConstDequantizeOptimizer: no change\n",
      "2021-08-26 11:09:32,260 - VERBOSE - tf2onnx.optimizer: Apply loop_optimizer\n",
      "2021-08-26 11:09:32,454 - VERBOSE - tf2onnx.optimizer.LoopOptimizer: no change\n",
      "2021-08-26 11:09:32,454 - VERBOSE - tf2onnx.optimizer: Apply merge_duplication\n",
      "2021-08-26 11:09:33,479 - VERBOSE - tf2onnx.optimizer.MergeDuplicatedNodesOptimizer: Cast -24 (95->71), Concat -16 (68->52), Const -354 (473->119), Gather -11 (48->37), ReduceProd -10 (40->30), Shape -8 (34->26), Unsqueeze -19 (70->51)\n",
      "2021-08-26 11:09:33,479 - VERBOSE - tf2onnx.optimizer: Apply reshape_optimizer\n",
      "2021-08-26 11:09:33,625 - VERBOSE - tf2onnx.optimizer.ReshapeOptimizer: Cast -2 (71->69), Concat -1 (52->51), Const +1 (119->120), Gather -2 (37->35), Shape -2 (26->24), Slice -2 (25->23), Squeeze -2 (19->17), Unsqueeze -1 (51->50)\n",
      "2021-08-26 11:09:33,625 - VERBOSE - tf2onnx.optimizer: Apply global_pool_optimizer\n",
      "2021-08-26 11:09:33,761 - VERBOSE - tf2onnx.optimizer.GlobalPoolOptimizer: GlobalAveragePool +8 (0->8), ReduceMean -8 (8->0)\n",
      "2021-08-26 11:09:33,761 - VERBOSE - tf2onnx.optimizer: Apply q_dq_optimizer\n",
      "2021-08-26 11:09:33,894 - VERBOSE - tf2onnx.optimizer.QDQOptimizer: no change\n",
      "2021-08-26 11:09:33,894 - VERBOSE - tf2onnx.optimizer: Apply remove_identity\n",
      "2021-08-26 11:09:34,154 - VERBOSE - tf2onnx.optimizer.IdentityOptimizer: Const -1 (120->119), Identity -83 (83->0)\n",
      "2021-08-26 11:09:34,154 - VERBOSE - tf2onnx.optimizer: Apply remove_back_to_back\n",
      "2021-08-26 11:09:34,301 - VERBOSE - tf2onnx.optimizer.BackToBackOptimizer: BatchNormalization -20 (31->11), Cast -2 (69->67), Concat -2 (51->49), Const +18 (119->137), Reshape -6 (66->60), Squeeze -12 (17->5), Transpose -2 (23->21), Unsqueeze -12 (50->38)\n",
      "2021-08-26 11:09:34,301 - VERBOSE - tf2onnx.optimizer: Apply einsum_optimizer\n",
      "2021-08-26 11:09:34,409 - VERBOSE - tf2onnx.optimizer.EinsumOptimizer: no change\n",
      "2021-08-26 11:09:34,409 - VERBOSE - tf2onnx.optimizer: Apply optimize_transpose\n",
      "2021-08-26 11:09:34,708 - VERBOSE - tf2onnx.optimizer.TransposeOptimizer: no change\n",
      "2021-08-26 11:09:34,708 - VERBOSE - tf2onnx.optimizer: Apply remove_redundant_upsample\n",
      "2021-08-26 11:09:34,820 - VERBOSE - tf2onnx.optimizer.UpsampleOptimizer: no change\n",
      "2021-08-26 11:09:34,820 - VERBOSE - tf2onnx.optimizer: Apply fold_constants\n",
      "2021-08-26 11:09:34,939 - VERBOSE - tf2onnx.optimizer.ConstFoldOptimizer: Cast -2 (67->65), Const +2 (137->139)\n",
      "2021-08-26 11:09:34,939 - VERBOSE - tf2onnx.optimizer: Apply const_dequantize_optimizer\n",
      "2021-08-26 11:09:35,043 - VERBOSE - tf2onnx.optimizer.ConstDequantizeOptimizer: no change\n",
      "2021-08-26 11:09:35,043 - VERBOSE - tf2onnx.optimizer: Apply loop_optimizer\n",
      "2021-08-26 11:09:35,149 - VERBOSE - tf2onnx.optimizer.LoopOptimizer: no change\n",
      "2021-08-26 11:09:35,150 - VERBOSE - tf2onnx.optimizer: Apply merge_duplication\n",
      "2021-08-26 11:09:35,551 - VERBOSE - tf2onnx.optimizer.MergeDuplicatedNodesOptimizer: Cast -1 (65->64), Const -21 (139->118), Less -1 (4->3), Reshape -5 (60->55)\n",
      "2021-08-26 11:09:35,551 - VERBOSE - tf2onnx.optimizer: Apply reshape_optimizer\n",
      "2021-08-26 11:09:35,672 - VERBOSE - tf2onnx.optimizer.ReshapeOptimizer: Cast -15 (64->49), Concat -15 (49->34), Const +17 (118->135), Gather -17 (35->18), ReduceProd -30 (30->0), Squeeze +3 (5->8), Unsqueeze -30 (38->8)\n",
      "2021-08-26 11:09:35,672 - VERBOSE - tf2onnx.optimizer: Apply global_pool_optimizer\n",
      "2021-08-26 11:09:35,761 - VERBOSE - tf2onnx.optimizer.GlobalPoolOptimizer: no change\n",
      "2021-08-26 11:09:35,761 - VERBOSE - tf2onnx.optimizer: Apply q_dq_optimizer\n",
      "2021-08-26 11:09:35,855 - VERBOSE - tf2onnx.optimizer.QDQOptimizer: no change\n",
      "2021-08-26 11:09:35,855 - VERBOSE - tf2onnx.optimizer: Apply remove_identity\n",
      "2021-08-26 11:09:35,948 - VERBOSE - tf2onnx.optimizer.IdentityOptimizer: no change\n",
      "2021-08-26 11:09:35,948 - VERBOSE - tf2onnx.optimizer: Apply remove_back_to_back\n",
      "2021-08-26 11:09:36,038 - VERBOSE - tf2onnx.optimizer.BackToBackOptimizer: no change\n",
      "2021-08-26 11:09:36,038 - VERBOSE - tf2onnx.optimizer: Apply einsum_optimizer\n",
      "2021-08-26 11:09:36,122 - VERBOSE - tf2onnx.optimizer.EinsumOptimizer: no change\n",
      "2021-08-26 11:09:36,122 - VERBOSE - tf2onnx.optimizer: Apply optimize_transpose\n",
      "2021-08-26 11:09:36,382 - VERBOSE - tf2onnx.optimizer.TransposeOptimizer: no change\n",
      "2021-08-26 11:09:36,383 - VERBOSE - tf2onnx.optimizer: Apply remove_redundant_upsample\n",
      "2021-08-26 11:09:36,478 - VERBOSE - tf2onnx.optimizer.UpsampleOptimizer: no change\n",
      "2021-08-26 11:09:36,478 - VERBOSE - tf2onnx.optimizer: Apply fold_constants\n",
      "2021-08-26 11:09:36,576 - VERBOSE - tf2onnx.optimizer.ConstFoldOptimizer: no change\n",
      "2021-08-26 11:09:36,576 - VERBOSE - tf2onnx.optimizer: Apply const_dequantize_optimizer\n",
      "2021-08-26 11:09:36,666 - VERBOSE - tf2onnx.optimizer.ConstDequantizeOptimizer: no change\n",
      "2021-08-26 11:09:36,666 - VERBOSE - tf2onnx.optimizer: Apply loop_optimizer\n",
      "2021-08-26 11:09:36,756 - VERBOSE - tf2onnx.optimizer.LoopOptimizer: no change\n",
      "2021-08-26 11:09:36,756 - VERBOSE - tf2onnx.optimizer: Apply merge_duplication\n",
      "2021-08-26 11:09:37,037 - VERBOSE - tf2onnx.optimizer.MergeDuplicatedNodesOptimizer: Const -16 (135->119)\n",
      "2021-08-26 11:09:37,037 - VERBOSE - tf2onnx.optimizer: Apply reshape_optimizer\n",
      "2021-08-26 11:09:37,134 - VERBOSE - tf2onnx.optimizer.ReshapeOptimizer: no change\n",
      "2021-08-26 11:09:37,134 - VERBOSE - tf2onnx.optimizer: Apply global_pool_optimizer\n",
      "2021-08-26 11:09:37,230 - VERBOSE - tf2onnx.optimizer.GlobalPoolOptimizer: no change\n",
      "2021-08-26 11:09:37,230 - VERBOSE - tf2onnx.optimizer: Apply q_dq_optimizer\n",
      "2021-08-26 11:09:37,321 - VERBOSE - tf2onnx.optimizer.QDQOptimizer: no change\n",
      "2021-08-26 11:09:37,321 - VERBOSE - tf2onnx.optimizer: Apply remove_identity\n",
      "2021-08-26 11:09:37,416 - VERBOSE - tf2onnx.optimizer.IdentityOptimizer: no change\n",
      "2021-08-26 11:09:37,416 - VERBOSE - tf2onnx.optimizer: Apply remove_back_to_back\n",
      "2021-08-26 11:09:37,511 - VERBOSE - tf2onnx.optimizer.BackToBackOptimizer: no change\n",
      "2021-08-26 11:09:37,511 - VERBOSE - tf2onnx.optimizer: Apply einsum_optimizer\n",
      "2021-08-26 11:09:37,597 - VERBOSE - tf2onnx.optimizer.EinsumOptimizer: no change\n",
      "2021-08-26 11:09:37,598 - VERBOSE - tf2onnx.optimizer: Apply optimize_transpose\n",
      "2021-08-26 11:09:37,706 - VERBOSE - tf2onnx.optimizer.TransposeOptimizer: no change\n",
      "2021-08-26 11:09:37,706 - VERBOSE - tf2onnx.optimizer: Apply remove_redundant_upsample\n",
      "2021-08-26 11:09:37,800 - VERBOSE - tf2onnx.optimizer.UpsampleOptimizer: no change\n",
      "2021-08-26 11:09:37,800 - VERBOSE - tf2onnx.optimizer: Apply fold_constants\n",
      "2021-08-26 11:09:37,895 - VERBOSE - tf2onnx.optimizer.ConstFoldOptimizer: no change\n",
      "2021-08-26 11:09:37,895 - VERBOSE - tf2onnx.optimizer: Apply const_dequantize_optimizer\n",
      "2021-08-26 11:09:38,157 - VERBOSE - tf2onnx.optimizer.ConstDequantizeOptimizer: no change\n",
      "2021-08-26 11:09:38,157 - VERBOSE - tf2onnx.optimizer: Apply loop_optimizer\n",
      "2021-08-26 11:09:38,255 - VERBOSE - tf2onnx.optimizer.LoopOptimizer: no change\n",
      "2021-08-26 11:09:38,256 - VERBOSE - tf2onnx.optimizer: Apply merge_duplication\n",
      "2021-08-26 11:09:38,444 - VERBOSE - tf2onnx.optimizer.MergeDuplicatedNodesOptimizer: no change\n",
      "2021-08-26 11:09:38,444 - VERBOSE - tf2onnx.optimizer: Apply reshape_optimizer\n",
      "2021-08-26 11:09:38,537 - VERBOSE - tf2onnx.optimizer.ReshapeOptimizer: no change\n",
      "2021-08-26 11:09:38,537 - VERBOSE - tf2onnx.optimizer: Apply global_pool_optimizer\n",
      "2021-08-26 11:09:38,633 - VERBOSE - tf2onnx.optimizer.GlobalPoolOptimizer: no change\n",
      "2021-08-26 11:09:38,633 - VERBOSE - tf2onnx.optimizer: Apply q_dq_optimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-26 11:09:38,729 - VERBOSE - tf2onnx.optimizer.QDQOptimizer: no change\n",
      "2021-08-26 11:09:38,729 - VERBOSE - tf2onnx.optimizer: Apply remove_identity\n",
      "2021-08-26 11:09:38,822 - VERBOSE - tf2onnx.optimizer.IdentityOptimizer: no change\n",
      "2021-08-26 11:09:38,822 - VERBOSE - tf2onnx.optimizer: Apply remove_back_to_back\n",
      "2021-08-26 11:09:38,916 - VERBOSE - tf2onnx.optimizer.BackToBackOptimizer: no change\n",
      "2021-08-26 11:09:38,916 - VERBOSE - tf2onnx.optimizer: Apply einsum_optimizer\n",
      "2021-08-26 11:09:39,011 - VERBOSE - tf2onnx.optimizer.EinsumOptimizer: no change\n",
      "2021-08-26 11:09:39,027 - INFO - tf2onnx.optimizer: After optimization: BatchNormalization -20 (31->11), Cast -53 (102->49), Concat -34 (68->34), Const -563 (682->119), Gather -24 (42->18), GlobalAveragePool +8 (0->8), Identity -102 (102->0), Less -1 (4->3), Placeholder -1 (6->5), ReduceMean -8 (8->0), ReduceProd -40 (40->0), Reshape -14 (69->55), Shape -9 (33->24), Slice -2 (25->23), Squeeze -11 (19->8), Transpose -137 (158->21), Unsqueeze -101 (109->8)\n",
      "2021-08-26 11:09:39,243 - INFO - tf2onnx: \n",
      "2021-08-26 11:09:39,243 - INFO - tf2onnx: Successfully converted TensorFlow model /data/cc to ONNX\n",
      "2021-08-26 11:09:39,244 - INFO - tf2onnx: Model inputs: ['input_1']\n",
      "2021-08-26 11:09:39,244 - INFO - tf2onnx: Model outputs: ['output_1', 'output_2', 'output_3']\n",
      "2021-08-26 11:09:39,244 - INFO - tf2onnx: ONNX model is saved at /data/cc/test.onnx\n"
     ]
    }
   ],
   "source": [
    "!python3 -m tf2onnx.convert --tag serve --signature_def serving_default --opset 13 \\\n",
    "    --saved-model /data/cc --output /data/cc/test.onnx --verbose --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:05:30.245945Z",
     "start_time": "2021-08-25T13:05:29.572127Z"
    }
   },
   "outputs": [],
   "source": [
    "model.build(input_shape=(1, 64, 112, 112, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:20:35.522093Z",
     "start_time": "2021-08-25T13:20:33.120525Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = ResnetPeriodEstimator()\n",
    "model.predict(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "# model2.build(input_shape=(1, 64, 112, 112, 3))\n",
    "model2.save('/data/gg', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:08:02.302321Z",
     "start_time": "2021-08-25T13:07:47.443138Z"
    }
   },
   "outputs": [],
   "source": [
    "model.base_model.save(\"/data/aa_base\", overwrite=True, include_optimizer=False, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:06:41.621857Z",
     "start_time": "2021-08-25T13:06:10.138708Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:08:41.951474Z",
     "start_time": "2021-08-25T13:08:41.921960Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:09:41.839174Z",
     "start_time": "2021-08-25T13:09:38.762274Z"
    }
   },
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir /data/cc --tag_set serve  --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T13:49:01.100204Z",
     "start_time": "2021-08-25T13:48:38.010570Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python3 -m tf2onnx.convert --tag serve --signature_def serving_default --opset 13 \\\n",
    "    --saved-model /data/cc --output /data/aa/test.onnx --verbose --debug\n",
    "\n",
    "#     --inputs serving_default_input_1:0 \\\n",
    "#     --outputs StatefulPartitionedCall:0,StatefulPartitionedCall:0,StatefulPartitionedCall:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.654499Z",
     "start_time": "2021-08-25T07:05:14.174Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m tf2onnx.convert --tag serve --signature_def serving_default --opset 10 --verbose \\\n",
    "    --input /data/aa/saved_model.pb --output /data/aa/test.onnx \\\n",
    "    --inputs serving_default_input_1:0 \\\n",
    "    --outputs StatefulPartitionedCall:0,StatefulPartitionedCall:0,StatefulPartitionedCall:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.655744Z",
     "start_time": "2021-08-25T07:05:14.177Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls $PATH_TO_CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:43:02.619654Z",
     "start_time": "2021-08-25T07:43:02.361485Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.658052Z",
     "start_time": "2021-08-25T07:05:14.183Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(tf.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:10:15.282498Z",
     "start_time": "2021-08-25T07:10:15.024034Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def traceme(x):\n",
    "    return model1(x)\n",
    "\n",
    "\n",
    "logdir = \"log2\"\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "# Forward pass\n",
    "traceme(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.660581Z",
     "start_time": "2021-08-25T07:05:14.191Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResnetPeriodEstimator()\n",
    "model(tf.random.uniform((1, 64, 112, 112, 3)))\n",
    "model.save('/data/aa/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.661749Z",
     "start_time": "2021-08-25T07:05:14.194Z"
    }
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.662854Z",
     "start_time": "2021-08-25T07:05:14.197Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, '/data/bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.664018Z",
     "start_time": "2021-08-25T07:05:14.200Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load('/data/bb')\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.665378Z",
     "start_time": "2021-08-25T07:05:14.203Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(loaded.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.666464Z",
     "start_time": "2021-08-25T07:05:14.207Z"
    }
   },
   "outputs": [],
   "source": [
    "infer = loaded.signatures[\"serving_default\"]\n",
    "print(infer.structured_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.667682Z",
     "start_time": "2021-08-25T07:05:14.212Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b, c, = loaded(tf.random.uniform((1, 64, 112, 112, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.668830Z",
     "start_time": "2021-08-25T07:05:14.215Z"
    }
   },
   "outputs": [],
   "source": [
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.669956Z",
     "start_time": "2021-08-25T07:05:14.218Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(loaded, '/data/cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.671162Z",
     "start_time": "2021-08-25T07:05:14.222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\n",
    "    \"/data/cc\", signature_keys=['serving_default'])\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.experimental_new_converter = True\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "# with open('/data/model.tflite', 'wb') as f:\n",
    "#   f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNR6sVATy8yX"
   },
   "source": [
    "## Set Params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.672582Z",
     "start_time": "2021-08-25T07:05:14.226Z"
    },
    "id": "PaCB432zUrZu"
   },
   "outputs": [],
   "source": [
    "##@title \n",
    "\n",
    "# FPS while recording video from webcam.\n",
    "WEBCAM_FPS = 16#@param {type:\"integer\"}\n",
    "\n",
    "# Time in seconds to record video on webcam. \n",
    "RECORDING_TIME_IN_SECONDS = 8. #@param {type:\"number\"}\n",
    "\n",
    "# Threshold to consider periodicity in entire video.\n",
    "THRESHOLD = 0.2#@param {type:\"number\"}\n",
    "\n",
    "# Threshold to consider periodicity for individual frames in video.\n",
    "WITHIN_PERIOD_THRESHOLD = 0.5#@param {type:\"number\"}\n",
    "\n",
    "# Use this setting for better results when it is \n",
    "# known action is repeating at constant speed.\n",
    "CONSTANT_SPEED = False#@param {type:\"boolean\"}\n",
    "\n",
    "# Use median filtering in time to ignore noisy frames.\n",
    "MEDIAN_FILTER = True#@param {type:\"boolean\"}\n",
    "\n",
    "# Use this setting for better results when it is \n",
    "# known the entire video is periodic/reapeating and\n",
    "# has no aperiodic frames.\n",
    "FULLY_PERIODIC = False#@param {type:\"boolean\"}\n",
    "\n",
    "# Plot score in visualization video.\n",
    "PLOT_SCORE = False#@param {type:\"boolean\"}\n",
    "\n",
    "# Visualization video's FPS.\n",
    "VIZ_FPS = 30#@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.673691Z",
     "start_time": "2021-08-25T07:05:14.230Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "IScNPW9C2Urp",
    "outputId": "887bc8cc-be55-435c-e687-4cb8c295fe41"
   },
   "outputs": [],
   "source": [
    "vfile = \"/data/test-9.mp4\"\n",
    "imgs, vid_fps = read_video(vfile)\n",
    "show_video(vfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.674816Z",
     "start_time": "2021-08-25T07:05:14.234Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(imgs), vid_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKWkGlEsa3Tg"
   },
   "source": [
    "# Run RepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.675959Z",
     "start_time": "2021-08-25T07:05:14.237Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "FUg2vSYhmsT0",
    "outputId": "14171077-b308-411a-a901-bdab6fb3e7f5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Running RepNet...') \n",
    "(pred_period, pred_score, within_period,\n",
    " per_frame_counts, chosen_stride) = get_counts(\n",
    "     model,\n",
    "     imgs,\n",
    "     strides=[1,2,3,4],\n",
    "     batch_size=20,\n",
    "     threshold=THRESHOLD,\n",
    "     within_period_threshold=WITHIN_PERIOD_THRESHOLD,\n",
    "     constant_speed=CONSTANT_SPEED,\n",
    "     median_filter=MEDIAN_FILTER,\n",
    "     fully_periodic=True)\n",
    "print('Visualizing results...') \n",
    "viz_reps(imgs, per_frame_counts, pred_score, interval=1000/VIZ_FPS,\n",
    "         plot_score=PLOT_SCORE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.677042Z",
     "start_time": "2021-08-25T07:05:14.241Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "IYcthjnIJC3P",
    "outputId": "85baf822-7dbf-40b0-eb9f-78b847d3bd24"
   },
   "outputs": [],
   "source": [
    "# Debugging video showing scores, per-frame frequency prediction and \n",
    "# within_period scores.\n",
    "# create_count_video(imgs,\n",
    "#                    per_frame_counts,\n",
    "#                    within_period,\n",
    "#                    score=pred_score,\n",
    "#                    fps=vid_fps,\n",
    "#                    output_file='/tmp/debug_video.mp4',\n",
    "#                    delay=1000/VIZ_FPS,\n",
    "#                    plot_count=True,\n",
    "#                    plot_within_period=True,\n",
    "#                    plot_score=True)\n",
    "# show_video('/tmp/debug_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:05:19.679544Z",
     "start_time": "2021-08-25T07:05:14.708Z"
    }
   },
   "outputs": [],
   "source": [
    "1. https://github.com/onnx/onnx/blob/master/docs/Operators.md\n",
    "2. https://gitee.com/eyecloud/openncc/tree/master"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "repnet_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
