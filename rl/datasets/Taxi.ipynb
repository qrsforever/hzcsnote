{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:45.867108Z",
     "start_time": "2020-04-15T09:00:45.667554Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:31:50.245278Z",
     "start_time": "2020-04-13T11:31:50.233627Z"
    }
   },
   "source": [
    "| 符号 | | 描述 |\n",
    "|------|---| :------|\n",
    "|<img width=200 /> | | <img width=500 /> |\n",
    "| $S$ | | 环境的状态空间 |\n",
    "| $A$| | agent可选择的动作空间 |\n",
    "| $R(s，a)$ | | 奖励函数，返回的值表示在s状态下执行a动作的奖励 |\n",
    "| $T(s'|s,a)$ | |  状态转移概率函数，表示从s状态执行a动作后环境转移至s′状态的概率|\n",
    "\n",
    "<br>\n",
    "\n",
    "目标：找到一个策略$\\pi$能够最大化我们的对未来奖励的期望$E(\\sum_{t=0}^n \\gamma^tR_t)$，$R_t$为t时刻的奖励，$\\gamma$为折扣因子，代表距离现在越遥远的奖励不如现在的奖励大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:32:00.377254Z",
     "start_time": "2020-04-13T11:32:00.368940Z"
    }
   },
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:45.874858Z",
     "start_time": "2020-04-15T09:00:45.870115Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 20\n",
    "game = 'Taxi-v3'\n",
    "path = f'/tmp/gym-{game}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:46.267048Z",
     "start_time": "2020-04-15T09:00:45.879327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game)\n",
    "env = env.unwrapped\n",
    "print(env.action_space)\n",
    "print(env.observation_space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:46.274659Z",
     "start_time": "2020-04-15T09:00:46.269917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:46.287966Z",
     "start_time": "2020-04-15T09:00:46.276839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:46.297115Z",
     "start_time": "2020-04-15T09:00:46.290538Z"
    }
   },
   "outputs": [],
   "source": [
    "total_episodes = 5000         # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.618                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.01             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:00:49.776985Z",
     "start_time": "2020-04-15T09:00:46.299432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: -2.7242\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.50483568  -2.47628218  -2.50344543  -2.43558422  -2.32039715\n",
      "  -11.42409504]\n",
      " [ -2.34529997  -1.54053054  -2.19015506  -2.2323482   -0.57891593\n",
      "  -10.84216566]\n",
      " ...\n",
      " [ -2.11157136   0.34405221  -2.1595745   -2.07001531  -9.92\n",
      "   -8.        ]\n",
      " [ -2.41400084  -2.43996321  -2.41400084  -2.18964195  -8.\n",
      "  -10.27016909]\n",
      " [ -0.8         -0.8         -0.8         11.35999956   0.\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 均匀分布, 从当前状态选择动作\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        if exp_exp_tradeoff > epsilon: # Greedy\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        else: # Random\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Bellman Equations: Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *\n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:10:27.273476Z",
     "start_time": "2020-04-13T11:10:27.269134Z"
    }
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:01:38.901885Z",
     "start_time": "2020-04-15T09:01:30.872169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a544140f33934d6ba3f8e649c7f588ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "Number of steps 13\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Number of steps 11\n"
     ]
    }
   ],
   "source": [
    "out = Output()\n",
    "display(out)\n",
    "for episode in range(2):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 获取每个状态下未来奖励的期望(ER)最大的动作\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "       \n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            time.sleep(0.3)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "    print(\"Number of steps\", step)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T12:24:51.168477Z",
     "start_time": "2020-04-13T12:24:50.607774Z"
    }
   },
   "source": [
    "![Taxi](../assets/taxi.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
