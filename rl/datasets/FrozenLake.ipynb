{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:02.678025Z",
     "start_time": "2020-04-14T09:31:02.491483Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import Output\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:31:50.245278Z",
     "start_time": "2020-04-13T11:31:50.233627Z"
    }
   },
   "source": [
    "| 符号 | | 描述 |\n",
    "|------|---| :------|\n",
    "|<img width=200 /> | | <img width=500 /> |\n",
    "| $S$ | | 环境的状态空间 |\n",
    "| $A$| | agent可选择的动作空间 |\n",
    "| $R(s，a)$ | | 奖励函数，返回的值表示在s状态下执行a动作的奖励 |\n",
    "| $T(s'|s,a)$ | |  状态转移概率函数，表示从s状态执行a动作后环境转移至s′状态的概率|\n",
    "\n",
    "<br>\n",
    "\n",
    "目标：找到一个策略$\\pi$能够最大化我们的对未来奖励的期望$E(\\sum_{t=0}^n \\gamma^tR_t)$ (未来收益总和），$R_t$为t时刻的奖励，$\\gamma$为折扣因子，代表距离现在越遥远的奖励不如现在的奖励大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:32:00.377254Z",
     "start_time": "2020-04-13T11:32:00.368940Z"
    }
   },
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/frozenlake.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:02.685063Z",
     "start_time": "2020-04-14T09:31:02.681054Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 20\n",
    "game = 'FrozenLake-v0'\n",
    "path = f'/tmp/gym-{game}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:03.070304Z",
     "start_time": "2020-04-14T09:31:02.687969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game)\n",
    "env = env.unwrapped\n",
    "print(env.action_space)\n",
    "print(env.observation_space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:03.077638Z",
     "start_time": "2020-04-14T09:31:03.073189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:03.090912Z",
     "start_time": "2020-04-14T09:31:03.080202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:03.101136Z",
     "start_time": "2020-04-14T09:31:03.093241Z"
    }
   },
   "outputs": [],
   "source": [
    "total_episodes = 15000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algo & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:24.806968Z",
     "start_time": "2020-04-14T09:31:03.103571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.4835333333333333\n",
      "          left          down          right          up\n",
      "[[1.98653290e-01 1.85374851e-01 9.67267071e-02 1.12160278e-01]\n",
      " [6.00929756e-03 9.10543996e-04 5.44080872e-03 1.76304760e-01]\n",
      " [1.39336251e-02 2.89212246e-02 7.47053358e-03 7.73760269e-02]\n",
      " [4.47458509e-02 5.10942068e-04 2.06908451e-04 5.65650505e-02]\n",
      " [2.19188406e-01 5.32120391e-02 5.66851958e-02 5.03485451e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.46151233e-02 2.46667460e-09 1.89912329e-05 1.74778525e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.44276017e-02 2.20195540e-02 6.73519038e-03 2.84193266e-01]\n",
      " [2.02794709e-02 4.73943087e-01 1.22406483e-01 8.71226470e-03]\n",
      " [8.86996913e-01 9.55843218e-03 1.59631417e-03 1.91359673e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.67731694e-02 1.51803611e-02 6.28038612e-01 5.53557700e-02]\n",
      " [4.61783499e-01 9.32463707e-01 6.60666844e-01 4.89598865e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 均匀分布, 从当前状态选择动作\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        if exp_exp_tradeoff > epsilon: # Greedy\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        else: # Random\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Bellman Equations: Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                            np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print (\"          left          down          right          up\")\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:10:27.273476Z",
     "start_time": "2020-04-13T11:10:27.269134Z"
    }
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:04:24.508322Z",
     "start_time": "2020-04-15T09:03:43.170165Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c53f7d59a94675bcc13a6419674a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "Number of steps 28\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Number of steps 98\n"
     ]
    }
   ],
   "source": [
    "out = Output()\n",
    "display(out)\n",
    "for episode in range(2):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(max_steps):\n",
    "        # 获取每个状态下未来奖励的期望(ER)最大的动作\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            time.sleep(0.3)\n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "    print(\"Number of steps\", step)\n",
    "    time.sleep(1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:27:35.545421Z",
     "start_time": "2020-04-14T09:27:34.969235Z"
    }
   },
   "source": [
    "![frozenlake](../assets/frozenlake.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
