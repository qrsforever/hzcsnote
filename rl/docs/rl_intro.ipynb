{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align='center'> 深度增强学习Deep Reinforcement Learning </div>\n",
    "\n",
    "Perception --> Policy --> Action(Control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process(MDP)\n",
    "\n",
    "<img src=\"../assets/timg.jpeg\" height=\"100px\" width=\"400px\"/>\n",
    "\n",
    "| 符号 | 描述 |\n",
    "|------|------|\n",
    "|<img width='200'/> | <img width='600'/> |\n",
    "| $S$ | 状态集(the set of all valid states), 环境的状态空间(观察Observation) |\n",
    "| $A$| 动作集(the set of all valid actions), agent可选择的动作空间 |\n",
    "| $R(s，a)$ |  奖励函数，返回的值表示在**s**状态下执行**a**动作的奖励, $r_t = R(s_t, a_t, s_{t+1})$|\n",
    "| $T(s'|s,a)$ |  状态转移概率函数，表示从**s**状态执行**a**动作后环境转移至**s′**状态的概, $P(s'|s, a)$|\n",
    "| $\\rho_0$ |  起始状态分布 the starting state distribution |\n",
    "\n",
    "> 执行动作 a 的效果只与当前状态有关，与之前历史状态无关 $P(s_{t+1}|s_t) = P(s_{t+1}|s_1,...,s_t)$\n",
    "\n",
    "\n",
    "```                                           \n",
    "                                   Action\n",
    "                                     |\n",
    "                                     |\n",
    "                           +---------+----------+\n",
    "                           |                    |\n",
    "                           |                    |\n",
    "                           v                    v\n",
    "         Policy:     Deterministic           Stochastic\n",
    "\n",
    "                    T：S × A -> S        T：S × A -> Prob(S)\n",
    "                                                |\n",
    "                                                |\n",
    "                                      +---------+---------+\n",
    "                                      |                   |\n",
    "                                      v                   v\n",
    "                                 Categorical          Gaussian\n",
    "\n",
    "        Action Spaces:            Discrete             Continuous                               \n",
    "                                                                    \n",
    "```                                           \n",
    "\n",
    "Agent: $\\pi(a)$\n",
    "\n",
    "\n",
    "MDP过程中,可以有无数种策略(policy),找到最佳的路径实际上就是找到最佳的Policy 来最大化V函数(Value Function)或者Q函数(Action-Value Function).\n",
    "\n",
    "1. Determinstic: \n",
    "\n",
    "$$\n",
    "s_{t+1} = f(s_t, a_t) \\\\\n",
    "R(\\tau) = \\sum_0^T{r_t} \\text{       finite-horizon undiscounted return}\n",
    "$$\n",
    "\n",
    "\n",
    "2. Stochastic: \n",
    "\n",
    "$$\n",
    "s_{t+1} \\sim P(\\cdot|s_t, a_t) \\\\\n",
    "R(\\tau) = \\sum_0^{\\infty}\\gamma^t{r_t} \\text{        infinite-horizon discounted return}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T03:59:48.615705Z",
     "start_time": "2020-04-15T03:59:48.570863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"454pt\" height=\"223pt\"\n",
       " viewBox=\"0.00 0.00 454.03 223.48\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 219.4802)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-219.4802 450.028,-219.4802 450.028,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"189.7315\" cy=\"-197.4802\" rx=\"40.8928\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.7315\" y=\"-193.7802\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Action</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"82.7315\" cy=\"-116.6102\" rx=\"82.9636\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.7315\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Determinstic</text>\n",
       "<text text-anchor=\"middle\" x=\"82.7315\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">T：S × A &#45;&gt; S</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.1008,-181.8877C156.2909,-172.206 139.3763,-159.422 123.9134,-147.7352\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.5956,-144.6195 115.5075,-141.3821 121.3749,-150.2039 125.5956,-144.6195\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"297.7315\" cy=\"-116.6102\" rx=\"113.6886\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"297.7315\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Stochastic</text>\n",
       "<text text-anchor=\"middle\" x=\"297.7315\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">T：S × A &#45;&gt; Prob(S)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M210.555,-181.8877C223.1743,-172.4384 239.7402,-160.0339 255.0386,-148.5785\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.4673,-151.1324 263.3741,-142.3369 253.2716,-145.5291 257.4673,-151.1324\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"218.7315\" cy=\"-26.8701\" rx=\"70.922\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"218.7315\" y=\"-30.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Categorical</text>\n",
       "<text text-anchor=\"middle\" x=\"218.7315\" y=\"-15.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Discrete</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M274.4562,-90.1706C266.1961,-80.7876 256.7904,-70.1031 248.089,-60.2188\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"250.5318,-57.6967 241.297,-52.5034 245.2776,-62.322 250.5318,-57.6967\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"376.7315\" cy=\"-26.8701\" rx=\"69.0935\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"376.7315\" y=\"-30.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gaussian</text>\n",
       "<text text-anchor=\"middle\" x=\"376.7315\" y=\"-15.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Continuous</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M321.0068,-90.1706C329.2669,-80.7876 338.6726,-70.1031 347.374,-60.2188\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.1854,-62.322 354.166,-52.5034 344.9312,-57.6967 350.1854,-62.322\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f436c367f60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.node('1', label='Action')\n",
    "dot.node('2', label='Determinstic\\nT：S × A -> S')\n",
    "dot.node('3', label='Stochastic\\nT：S × A -> Prob(S)')\n",
    "dot.node('4', label='Categorical\\nDiscrete')\n",
    "dot.node('5', label='Gaussian\\nContinuous')\n",
    "\n",
    "dot.edges(['12', '13', '34', '35'])\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T08:41:35.403034Z",
     "start_time": "2020-04-14T08:41:35.391952Z"
    }
   },
   "source": [
    "## [songrotek](https://blog.csdn.net/songrotek/article/details/50580904)\n",
    "\n",
    "> Agent: 具备行为能力的物体 \n",
    "    \n",
    "> 增强学习的任务就是找到一个最优的策略policy从而使reward最多, 在轨迹上最大化累积奖励\n",
    "\n",
    "> 状态的好坏其实等价于对未来回报的期望 (Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T15:14:40.337816Z",
     "start_time": "2020-04-13T15:14:40.334352Z"
    }
   },
   "source": [
    "# The Bellman Equations\n",
    "\n",
    "## Reward & Return\n",
    "\n",
    "RL agents learn to maximize cumulative future **reward**. The word used to describe cumulative future reward is **return** and is often denoted with $R$.\n",
    "\n",
    "\n",
    "$$\n",
    "R_t = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + \\dots + = \\sum_{k=0}^{\\infty}r_{t + k + 1}  \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "## Episodic \n",
    "\n",
    "Tasks that always terminate are called episodic.\n",
    "\n",
    "\n",
    "## Discounted\n",
    "\n",
    "More common than using future cumulative reward as return is using future cumulative **discounted** reward:\n",
    "\n",
    "\n",
    "$$\n",
    "R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + \\dots + = \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 1}  \\tag{2}\n",
    "$$\n",
    "\n",
    "$\\gamma$: discount factor\n",
    "\n",
    "it gives a greater weight to sooner rewards, meaning that we care more about imminent rewards and less about rewards we will receive further in the future.\n",
    "\n",
    "\n",
    "[understanding-rl-the-bellman-equations](https://joshgreaves.com/reinforcement-learning/understanding-rl-the-bellman-equations/)\n",
    "\n",
    "\n",
    "## Policy $\\pi(s, a)$\n",
    "\n",
    "It is a function that takes in a state and an action and returns the **probability** of taking that action in that state. Therefore, for a given state, it must be true that $\\sum_a{\\pi(s, a)} = 1$\n",
    "\n",
    "Our goal in reinforcement learning is to learn an optimal policy $\\pi^*$ (An optimal policy is a policy which tells us how to act to maximize return in every state)\n",
    "\n",
    "## Value Functions\n",
    "\n",
    "There are two types of value functions that are used in reinforcement learning: the state value function, denoted $V(s)$, and the action value function, denoted $Q(s, a)$.\n",
    "\n",
    "\n",
    "$V(s)$: It is the expected return when starting from state s acting according to our policy $\\pi$\n",
    "\n",
    "$$\n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\big[R_t | s_t = s \\big ] \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "$Q(s, a)$: The action value function tells us the value of taking an action in some state when following a certain policy.\n",
    "\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E}_\\pi \\big[R_t | s_t = s, a_t = a \\big ] \\tag{2}\n",
    "$$\n",
    "\n",
    "$V^\\pi(s) = \\underset{a \\sim \\pi}{\\mathbb{E}} \\big[ Q^\\pi(s, a) \\big]$\n",
    "\n",
    "\n",
    "> we use an expectation is that there is some randomness in what happens after you arrive at a state. You may have a stochastic policy, which means we need to combine the results of all the different actions that we take. Also, the transition function can be stochastic, meaning that we may not end up in any state with 100% probability\n",
    "\n",
    "\n",
    "Optimal Value Function: \n",
    "\n",
    "$$\n",
    "V^* = \\underset{\\pi}{\\mathbf{max}}\\  \\mathbb{E}_\\pi \\big[R_t | s_t = s \\big ]\n",
    "$$\n",
    "\n",
    "\n",
    "Optimal Action-Value Function:\n",
    "\n",
    "$$\n",
    "Q^* = \\underset{\\pi}{\\mathbf{max}}\\  \\mathbb{E}_\\pi \\big[R_t | s_t = s, a_t = a \\big ]\n",
    "$$\n",
    "\n",
    "$V^*(s) = \\underset{a}{max}Q^*(s, a)$\n",
    "\n",
    "## The Bellman Equations\n",
    "\n",
    "\n",
    "$\\mathcal{P}$ is the transition probability. If we start at state s and take action a we end up in state $s'$ with probability $\\mathcal{P}_{s s'}^{a}$.\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{s s'}^a = Pr(s_{t+1} = s' | s_t = s, a_t = a)\n",
    "$$\n",
    "\n",
    "\n",
    "$\\mathcal{R}_{s s'}^{a}$ is another way of writing the expected (or mean) reward that we receive when starting in state s, taking action a, and moving into state $s'$.\n",
    "\n",
    "$$\n",
    "\\mathcal{R}_{s s'}^a = \\mathbb{E}\\big [ r_{t+1} | s_t = s, s_{t+1} = s', a_t = a \\big ]\n",
    "$$\n",
    "\n",
    "==> \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^\\pi(s) &= \\mathbb{E}_\\pi \\big[ r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\gamma^3 r_{t+4} + \\dots | s_t = s \\big ] \\\\\n",
    "&= \\mathbb{E}_\\pi \\big [ \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 1} | s_t = s \\big ] \\\\\n",
    "&= \\mathbb{E}_\\pi \\big[ r_{t+1} + \\gamma \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 2} | s_t = s \\big ] \\\\\n",
    "&= \\mathbb{E}_\\pi \\big [ r_{t+1} | s_t = s \\big ] +  \\mathbb{E}_\\pi \\big[ \\gamma \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 2} | s_t = s \\big ]  \\\\\n",
    "&= \\sum_{a}{\\pi(s, a)} \\sum_{s'}{\\mathcal P_{s s'}^{a} \\mathcal R_{s s'}^a } +  \\sum_{a}{\\pi(s, a)} \\sum_{s'}{\\mathcal P_{s s'}^{a} \\gamma \\mathbb{E}_\\pi \\big [ \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 2} | s_{t+1} = s' \\big] } \\\\\n",
    "&= \\sum_{a}{\\pi(s, a)} \\sum_{s'}{\\mathcal P_{s s'}^{a}} \\bigg [ \\mathcal R_{s s'}^a + \\gamma \\mathbb{E}_\\pi \\big [ \\sum_{k=0}^{\\infty}\\gamma^k r_{t + k + 2} | s_{t+1} = s' \\big] \\bigg] \\\\\n",
    "&= \\sum_{a}{\\pi(s, a)} \\sum_{s'}{\\mathcal P_{s s'}^{a}} \\bigg [ \\mathcal R_{s s'}^a + \\gamma V^{\\pi}(s') \\bigg] \\tag{3} \\\\\n",
    "Q^{\\pi}(s,a) &= \\sum_{s'} \\mathcal{P}_{ss'}^{a} \\bigg[ \\mathcal{R}_{ss'}^{a} + \\gamma \\sum_{a'} \\pi(s', a') Q^{\\pi}(s', a') \\bigg] \\tag{4}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    " v(s) & =  E[G_t|S_t = s] \\\\\\\\\n",
    "      & =  E[R_{t+1}+\\gamma R_{t+2} + \\gamma ^2R_{t+3} + ...|S_t = s] \\\\\\\\ \n",
    "      & =  E[R_{t+1}+\\gamma (R_{t+2} + \\gamma R_{t+3} + ...)|S_t = s] \\\\\\\\\n",
    "      & =  E[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\\\\\\\ \n",
    "      & =  E[R_{t+1} + \\gamma v(S_{t+1})|S_t = s]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^\\pi(s,a) & =   E[r_{t+1} + \\gamma r_{t+2} + \\gamma^2r_{t+3} + ... |s,a] \\\\\\\\\n",
    "& = E_{s^\\prime}[r+\\gamma Q^\\pi(s^\\prime,a^\\prime)|s,a]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "V^\\pi(s) &= E_\\pi \\{R_t | s_t = s\\} = E_\\pi \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s\\} \\\\\n",
    "Q^\\pi(s, a) &= E_\\pi \\{R_t | s_t = s, a_t = a\\} = E_\\pi \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s, a_t=a\\} \\\\\n",
    "V^\\pi(s) &= \\sum_{a \\in A} \\pi (a|s) * Q^\\pi(a,s)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "[a](https://blog.csdn.net/songrotek/article/details/50580904)\n",
    "[b](https://zhuanlan.zhihu.com/p/55481462)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies\n",
    "\n",
    "## exploration/exploitation\n",
    "\n",
    "1. exploration: picking an action of which the agent does not know whether it will perform well or not\n",
    "\n",
    "2. exploitation: picking the action of which the agent thinks it performs best\n",
    "\n",
    "### 1. Random\n",
    "\n",
    "This strategy represents pure **exploration** and will sample actions from a **uniform distribution** that representsthe action space. \n",
    "\n",
    "### 2. Greedy\n",
    "\n",
    "The greedy strategy always picks the action the policy thinks performs best, even when this policy has not converged yet. Therefore, this strategy embodies pure exploitation, and is the opposite of the Random strategy.\n",
    "\n",
    "\n",
    "### 3. Epsilon-greedy ($\\epsilon$-greedy)\n",
    "\n",
    "The agent picks a greedy approach, as explained above, with probability 1−$\\epsilon$ and picks an exploratory approach, like the Randomstrategy explained above, with probability $\\epsilon$.\n",
    "\n",
    "### 4. Decaying epsilon-greedy\n",
    "\n",
    "This strategy functions the sameas normal $\\epsilon$-greedy, however the epsilon value changes as a function of time, according to $\\gamma$ with $\\gamma^t$ as a function of time.\n",
    "\n",
    "\n",
    "### 5. Softmax\n",
    "\n",
    "[大地小神云盘](https://pan.baidu.com/s/1PJKtO8AukvLDEaIqwj8C0A \"提取码: rhnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "适用于: 状态和动作的组合是有限的\n",
    "\n",
    "每个状态以$\\pi$的概率进行探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States and Observations\n",
    "\n",
    "1. 状态(states)是对世界状况的完整描述, 没有隐藏信息\n",
    "\n",
    "2. 观察(observations)是对状态的部分描述, 有一些信息被略掉了\n",
    "\n",
    "视觉观察(像素矩阵RGB)\n",
    "\n",
    "机器人(关节角度和速度)\n",
    "\n",
    "\n",
    "1. 完全观察fully observed: 当agent能够观察到环境的完整状态\n",
    "\n",
    "2. 部分观察partially observed: 当agent只能看到部分环境\n",
    "\n",
    "> agent只能观察环境，不能直接得到环境的状态, 动作以观察为条件\n",
    "\n",
    "\n",
    "### Action Spaces\n",
    "\n",
    "给定环境中的所有有效行为集合通常称为行为空间\n",
    "\n",
    "离散的行为空间 (经典控制游戏)\n",
    " \n",
    "连续的行为空间 (机器人)\n",
    "\n",
    "\n",
    "### Policies\n",
    "\n",
    "agent用来决定行为的规则, 本质上是agent的大脑\n",
    "\n",
    "\n",
    "### Trajectories (Episodes)\n",
    "\n",
    "轨迹$\\tau$是在世界中的一系列状态和行为\n",
    "$$\n",
    "\\tau = (s_0, a_0, s_1, a_1, s_2, a_2, \\cdots) \\\\\n",
    "s_0 \\sim \\rho_0(\\cdot)\n",
    "$$\n",
    "\n",
    "### Reward and Return\n",
    "\n",
    "所有的奖励$R(\\tau)=$\n",
    "\n",
    "1. finite-horizon undiscounted return (有限期不折扣的回报)\n",
    "\n",
    "\n",
    "2. infinite-horizon discounted return (无限期折扣回报)\n",
    "\n",
    "> 数学上：无限期的奖励总和可能不会收敛到有限值，并且很难在方程中处理，但是在折扣因子以及一定的条件下，这个求和是收敛的。\n",
    "\n",
    "\n",
    "[RL中的关键概念](https://zhuanlan.zhihu.com/p/55481462)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellman方程, 基本思想: 起始点的值是从当前位置获得的奖励期望，在加上下次落脚点值的期望\n",
    "可以通过迭代来进行计算\n",
    "\n",
    "\n",
    "如果知道了每个动作的估值，那么就可以选择估值最好的一个动作去执行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T02:50:09.626558Z",
     "start_time": "2020-04-15T02:50:09.618773Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d858e725dfac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d858e725dfac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://blog.csdn.net/liweibin1994/article/details/79111536\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://blog.csdn.net/liweibin1994/article/details/79111536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{\\pi}(s) = \\sum_{a\\in A}\\pi(a|s)(R_s^a + \\gamma \\sum_{s' \\in S}P_{ss'}^a v_{\\pi}(s'))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T02:50:09.629813Z",
     "start_time": "2020-04-15T02:50:09.577Z"
    }
   },
   "outputs": [],
   "source": [
    "https://www.zhihu.com/question/26408259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T02:50:09.631463Z",
     "start_time": "2020-04-15T02:50:09.578Z"
    }
   },
   "outputs": [],
   "source": [
    "动作集合的数量将直接影响整个任务的求解难度"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
