{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T10:57:43.735608Z",
     "start_time": "2020-07-29T10:57:43.272627Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:21:43.534051Z",
     "start_time": "2020-07-29T11:21:43.390505Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Sequence\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import os, json\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import (Dataset, DataLoader)\n",
    "\n",
    "from k12libs.utils.nb_easy import K12AI_DATASETS_ROOT\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torchvision.transforms import ( # noqa\n",
    "        Compose,\n",
    "        ToTensor,\n",
    "        Normalize,\n",
    "        RandomOrder,\n",
    "        Resize,\n",
    "        ColorJitter,\n",
    "        RandomRotation,\n",
    "        RandomGrayscale,\n",
    "        RandomResizedCrop,\n",
    "        RandomHorizontalFlip,\n",
    "        RandomVerticalFlip)\n",
    "\n",
    "class IDataTransforms(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def compose_order(transforms: List):\n",
    "        return Compose(transforms)\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle_order(transforms: List):\n",
    "        return RandomOrder(transforms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_resize(size):\n",
    "        return Resize(size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def image_to_tensor():\n",
    "        return ToTensor()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_tensor(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "        return Normalize(mean, std)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_brightness(factor=0.25):\n",
    "        return ColorJitter(brightness=factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_contrast(factor=0.25):\n",
    "        return ColorJitter(contrast=factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_saturation(factor=0.25):\n",
    "        return ColorJitter(saturation=factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_hue(factor=0.25):\n",
    "        return ColorJitter(hue=factor)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_rotation(degrees=25):\n",
    "        return RandomRotation(degrees=degrees)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_grayscale(p=0.5):\n",
    "        return RandomGrayscale(p=p)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_resized_crop(size):\n",
    "        return RandomResizedCrop(size=size)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_horizontal_flip(p=0.5):\n",
    "        return RandomHorizontalFlip(p=p)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_vertical_flip(p=0.5):\n",
    "        return RandomVerticalFlip(p=p)\n",
    "\n",
    "\n",
    "class EasyaiDataset(ABC, Dataset):\n",
    "    def __init__(self, mean=None, std=None, **kwargs):\n",
    "        self.mean, self.std = mean, std\n",
    "        self.augtrans = None\n",
    "        if mean and std:\n",
    "            self.imgtrans = self.compose_order([\n",
    "                self.image_to_tensor(),\n",
    "                self.normalize_tensor(mean=mean, std=std)\n",
    "            ])\n",
    "        else:\n",
    "            self.imgtrans = self.image_to_tensor()\n",
    "\n",
    "        # reader\n",
    "        data = self.data_reader(**kwargs)\n",
    "        if isinstance(data, (tuple, list)) and len(data) == 2:\n",
    "            self.images, self.labels = data\n",
    "        elif isinstance(data, dict) and all([x in data.keys() for x in ('images', 'labels')]):\n",
    "            self.images, self.labels = data['images'], data['labels']\n",
    "        else:\n",
    "            raise ValueError('The return of data_reader must be List or Dict')\n",
    "\n",
    "    @abstractmethod\n",
    "    def data_reader(self, **kwargs) -> Union[Tuple[List, List, List], Dict[str, List]]:\n",
    "        \"\"\"\n",
    "        (M)\n",
    "        \"\"\"\n",
    "    \n",
    "    def set_aug_trans(self, transforms:Union[list, None], shuffle=False):\n",
    "        if transforms:\n",
    "            if any([not hasattr(x, '__call__') for x in transforms]):\n",
    "                raise ValueError(f'set_aug_trans: transforms params is invalid.')\n",
    "            if shuffle:\n",
    "                self.augtrans = self.shuffle_order(transforms)\n",
    "            else:\n",
    "                self.augtrans = self.compose_order(transforms)\n",
    "    \n",
    "    def set_img_trans(self, input_size:Union[Tuple[int, int], int, None], normalize=True):\n",
    "        trans = []\n",
    "        if input_size:\n",
    "            trans.append(self.image_resize(input_size))\n",
    "        trans.append(self.image_to_tensor())\n",
    "        if normalize and self.mean and self.std:\n",
    "            trans.append(self.normalize_tensor(mean=self.mean, std=self.std))\n",
    "        self.imgtrans = self.compose_order(trans)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.augtrans:\n",
    "            img = self.augtrans(img)\n",
    "        return self.imgtrans(img), self.labels[index], self.images[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "class EasyaiClassifier(pl.LightningModule, IDataTransforms):\n",
    "    def __init__(self):\n",
    "        super(EasyaiClassifier, self).__init__()\n",
    "        self.model = self.build_model()\n",
    "        self.criterion = None\n",
    "        self.datasets = {'train': None, 'val': None, 'test': None}\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    # Data\n",
    "    def load_presetting_dataset_(self, dataset_name):\n",
    "        class JsonfileDataset(EasyaiDataset):\n",
    "            def data_reader(self, path, phase):\n",
    "                \"\"\"\n",
    "                Args:\n",
    "                    path: the dataset root directory\n",
    "                    phase: the json file name (train.json / val.json / test.json)\n",
    "                \"\"\"\n",
    "                image_list = []\n",
    "                label_list = []\n",
    "                with open(os.path.join(path, f'{phase}.json')) as f:\n",
    "                    items = json.load(f)\n",
    "                    for item in items:\n",
    "                        image_list.append(os.path.join(path, item['image_path']))\n",
    "                        label_list.append(item['label'])\n",
    "                return image_list, label_list\n",
    "\n",
    "        root = f'{K12AI_DATASETS_ROOT}/cv/{dataset_name}'\n",
    "        datasets = {\n",
    "            'train': JsonfileDataset(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), path=root, phase='train'),\n",
    "            'val': JsonfileDataset(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), path=root, phase='val'),\n",
    "            'test': JsonfileDataset(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), path=root, phase='test'),\n",
    "        }\n",
    "        return datasets \n",
    "\n",
    "    def prepare_dataset(self) -> Union[EasyaiDataset, List[EasyaiDataset], Dict[str, EasyaiDataset]]:\n",
    "        return self.load_presetting_dataset_('rmnist')\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_delattr(cls, method):\n",
    "        try:\n",
    "            delattr(cls, method)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def prepare_data(self):\n",
    "        datasets = self.prepare_dataset()\n",
    "        if isinstance(datasets, EasyaiDataset):\n",
    "            self._safe_delattr(self.__class__, 'val_dataloader')\n",
    "            self._safe_delattr(self.__class__, 'validation_step')\n",
    "            self._safe_delattr(self.__class__, 'validation_epoch_end')\n",
    "            self._safe_delattr(self.__class__, 'test_dataloader')\n",
    "            self._safe_delattr(self.__class__, 'test_step')\n",
    "            self._safe_delattr(self.__class__, 'test_epoch_end')\n",
    "            self.datasets['train'] = datasets\n",
    "        elif isinstance(datasets, (list, tuple)) and len(datasets) <= 3:\n",
    "            self.datasets['train'] = datasets[0]\n",
    "            self.datasets['val'] = datasets[1]\n",
    "            if len(datasets) == 2:\n",
    "                self._safe_delattr(self.__class__, 'test_dataloader')\n",
    "                self._safe_delattr(self.__class__, 'test_step')\n",
    "                self._safe_delattr(self.__class__, 'test_epoch_end')\n",
    "            else:\n",
    "                self.datasets['test'] = datasets[2]\n",
    "\n",
    "        elif isinstance(datasets, dict) and \\\n",
    "                all([x in datasets.keys() for x in ('train', 'val', 'test')]):\n",
    "            self.datasets = datasets\n",
    "        else:\n",
    "            raise ValueError('the return of prepare_dataset is invalid.')\n",
    "\n",
    "    # Model\n",
    "    def load_pretrained_model_(self, model_name, num_classes=None, pretrained=True):\n",
    "        model = getattr(torchvision.models, model_name)(pretrained)\n",
    "        if num_classes:\n",
    "            if model_name.startswith('vgg'):\n",
    "                model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "            elif model_name.startswith('resnet'):\n",
    "                model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "            elif model_name.startswith('alexnet'):\n",
    "                model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "            elif model_name.startswith('mobilenet_v2'):\n",
    "                model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "            elif model_name.startswith('squeezenet'):\n",
    "                model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "            elif model_name.startswith('shufflenet'):\n",
    "                model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "            elif model_name.startswith('densenet'):\n",
    "                in_features = {\n",
    "                    \"densenet121\": 1024,\n",
    "                    \"densenet161\": 2208,\n",
    "                    \"densenet169\": 1664,\n",
    "                    \"densenet201\": 1920,\n",
    "                }\n",
    "                model.classifier = nn.Linear(in_features[model_name], num_classes)\n",
    "            else:\n",
    "                raise NotImplementedError(f'{model_name}')\n",
    "        return model\n",
    "\n",
    "    def build_model(self):\n",
    "        return self.load_pretrained_model_('resnet18', 10)\n",
    "\n",
    "    # Hypes\n",
    "    @property\n",
    "    def loss(self):\n",
    "        if self.criterion is None:\n",
    "            self.criterion = self.configure_criterion()\n",
    "        return self.criterion\n",
    "\n",
    "    def configure_criterion(self):\n",
    "        # default\n",
    "        loss = nn.CrossEntropyLoss(reduction='mean')\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizer(self):\n",
    "        # default\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=0.001)\n",
    "        return optimizer\n",
    "\n",
    "    def configure_scheduler(self, optimizer):\n",
    "        # default\n",
    "        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        return scheduler\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.configure_optimizer()\n",
    "        scheduler = self.configure_scheduler(optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    def calculate_acc_(self, y_pred, y_true):\n",
    "        return (torch.argmax(y_pred, axis=1) == y_true).float().mean()\n",
    "\n",
    "    def step_(self, batch):\n",
    "        x, y, _ = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return x, y, y_hat, loss\n",
    "\n",
    "    def get_dataloader(self, phase, \\\n",
    "                       trans_augment=None, trans_shuffle=False, \\\n",
    "                       normalize=False, input_size=None, \\\n",
    "                       batch_size=32, num_workers=1, \\\n",
    "                       drop_last=False, shuffle=False):\n",
    "        if phase not in self.datasets.keys():\n",
    "            raise RuntimeError(f'get {phase} data loader  error.')\n",
    "        dataset = self.datasets[phase]\n",
    "        dataset.set_aug_trans(transforms=trans_augment, shuffle=trans_shuffle)\n",
    "        dataset.set_img_trans(input_size=input_size, normalize=normalize)\n",
    "        return DataLoader(\n",
    "                dataset,\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                drop_last=drop_last,\n",
    "                shuffle=shuffle)\n",
    "\n",
    "    ## Train\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.get_dataloader(\n",
    "            phase='train',\n",
    "            trans_augment=[\n",
    "                self.random_resized_crop(size=(128, 128)),\n",
    "                self.random_brightness(factor=0.3),\n",
    "                self.random_rotation(degrees=30)\n",
    "            ],\n",
    "            trans_shuffle=False, \n",
    "            input_size=128,\n",
    "            normalize=True,\n",
    "            batch_size=32,\n",
    "            num_workers=1,\n",
    "            drop_last=False,\n",
    "            shuffle=False)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, y_hat, loss = self.step_(batch)\n",
    "        with torch.no_grad():\n",
    "            accuracy = self.calculate_acc_(y_hat, y)\n",
    "        log = {'train_loss': loss, 'train_acc': accuracy}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss,        # M\n",
    "            'acc': accuracy,     # O\n",
    "            'progress_bar': log, # O\n",
    "            \"log\": log           # O\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "        log = {'train_loss': avg_loss, 'train_acc': avg_acc}\n",
    "        return {'progress_bar': log, 'log': log}\n",
    "\n",
    "    ## Valid\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.get_dataloader('val',\n",
    "                batch_size=32,\n",
    "                num_workers=2,\n",
    "                drop_last=False,\n",
    "                shuffle=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, y_hat, loss = self.step_(batch)\n",
    "        accuracy = self.calculate_acc_(y_hat, y)\n",
    "        return {'val_loss': loss, 'val_acc': accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        log = {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "        return {'progress_bar': log, 'log': log}\n",
    "\n",
    "    ## Test\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.get_dataloader('test',\n",
    "                batch_size=32,\n",
    "                num_workers=1,\n",
    "                drop_last=False,\n",
    "                shuffle=False)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, y_hat, loss = self.step_(batch)\n",
    "        accuracy = self.calculate_acc_(y_hat, y)\n",
    "        return {'test_loss': loss, 'test_acc': accuracy}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        log = {'test_loss': avg_loss, 'test_acc': avg_acc}\n",
    "        return {'progress_bar': log, 'log': log}\n",
    "\n",
    "    \n",
    "class EasyaiTrainer(pl.Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(EasyaiTrainer, self).__init__(*args, **kwargs, gpus=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:47:12.642355Z",
     "start_time": "2020-07-29T11:46:46.925143Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                        | Type              | Params\n",
      "-------------------------------------------------------------------\n",
      "0  | model                       | ResNet            | 11 M  \n",
      "1  | model.conv1                 | Conv2d            | 9 K   \n",
      "2  | model.bn1                   | BatchNorm2d       | 128   \n",
      "3  | model.relu                  | ReLU              | 0     \n",
      "4  | model.maxpool               | MaxPool2d         | 0     \n",
      "5  | model.layer1                | Sequential        | 147 K \n",
      "6  | model.layer1.0              | BasicBlock        | 73 K  \n",
      "7  | model.layer1.0.conv1        | Conv2d            | 36 K  \n",
      "8  | model.layer1.0.bn1          | BatchNorm2d       | 128   \n",
      "9  | model.layer1.0.relu         | ReLU              | 0     \n",
      "10 | model.layer1.0.conv2        | Conv2d            | 36 K  \n",
      "11 | model.layer1.0.bn2          | BatchNorm2d       | 128   \n",
      "12 | model.layer1.1              | BasicBlock        | 73 K  \n",
      "13 | model.layer1.1.conv1        | Conv2d            | 36 K  \n",
      "14 | model.layer1.1.bn1          | BatchNorm2d       | 128   \n",
      "15 | model.layer1.1.relu         | ReLU              | 0     \n",
      "16 | model.layer1.1.conv2        | Conv2d            | 36 K  \n",
      "17 | model.layer1.1.bn2          | BatchNorm2d       | 128   \n",
      "18 | model.layer2                | Sequential        | 525 K \n",
      "19 | model.layer2.0              | BasicBlock        | 230 K \n",
      "20 | model.layer2.0.conv1        | Conv2d            | 73 K  \n",
      "21 | model.layer2.0.bn1          | BatchNorm2d       | 256   \n",
      "22 | model.layer2.0.relu         | ReLU              | 0     \n",
      "23 | model.layer2.0.conv2        | Conv2d            | 147 K \n",
      "24 | model.layer2.0.bn2          | BatchNorm2d       | 256   \n",
      "25 | model.layer2.0.downsample   | Sequential        | 8 K   \n",
      "26 | model.layer2.0.downsample.0 | Conv2d            | 8 K   \n",
      "27 | model.layer2.0.downsample.1 | BatchNorm2d       | 256   \n",
      "28 | model.layer2.1              | BasicBlock        | 295 K \n",
      "29 | model.layer2.1.conv1        | Conv2d            | 147 K \n",
      "30 | model.layer2.1.bn1          | BatchNorm2d       | 256   \n",
      "31 | model.layer2.1.relu         | ReLU              | 0     \n",
      "32 | model.layer2.1.conv2        | Conv2d            | 147 K \n",
      "33 | model.layer2.1.bn2          | BatchNorm2d       | 256   \n",
      "34 | model.layer3                | Sequential        | 2 M   \n",
      "35 | model.layer3.0              | BasicBlock        | 919 K \n",
      "36 | model.layer3.0.conv1        | Conv2d            | 294 K \n",
      "37 | model.layer3.0.bn1          | BatchNorm2d       | 512   \n",
      "38 | model.layer3.0.relu         | ReLU              | 0     \n",
      "39 | model.layer3.0.conv2        | Conv2d            | 589 K \n",
      "40 | model.layer3.0.bn2          | BatchNorm2d       | 512   \n",
      "41 | model.layer3.0.downsample   | Sequential        | 33 K  \n",
      "42 | model.layer3.0.downsample.0 | Conv2d            | 32 K  \n",
      "43 | model.layer3.0.downsample.1 | BatchNorm2d       | 512   \n",
      "44 | model.layer3.1              | BasicBlock        | 1 M   \n",
      "45 | model.layer3.1.conv1        | Conv2d            | 589 K \n",
      "46 | model.layer3.1.bn1          | BatchNorm2d       | 512   \n",
      "47 | model.layer3.1.relu         | ReLU              | 0     \n",
      "48 | model.layer3.1.conv2        | Conv2d            | 589 K \n",
      "49 | model.layer3.1.bn2          | BatchNorm2d       | 512   \n",
      "50 | model.layer4                | Sequential        | 8 M   \n",
      "51 | model.layer4.0              | BasicBlock        | 3 M   \n",
      "52 | model.layer4.0.conv1        | Conv2d            | 1 M   \n",
      "53 | model.layer4.0.bn1          | BatchNorm2d       | 1 K   \n",
      "54 | model.layer4.0.relu         | ReLU              | 0     \n",
      "55 | model.layer4.0.conv2        | Conv2d            | 2 M   \n",
      "56 | model.layer4.0.bn2          | BatchNorm2d       | 1 K   \n",
      "57 | model.layer4.0.downsample   | Sequential        | 132 K \n",
      "58 | model.layer4.0.downsample.0 | Conv2d            | 131 K \n",
      "59 | model.layer4.0.downsample.1 | BatchNorm2d       | 1 K   \n",
      "60 | model.layer4.1              | BasicBlock        | 4 M   \n",
      "61 | model.layer4.1.conv1        | Conv2d            | 2 M   \n",
      "62 | model.layer4.1.bn1          | BatchNorm2d       | 1 K   \n",
      "63 | model.layer4.1.relu         | ReLU              | 0     \n",
      "64 | model.layer4.1.conv2        | Conv2d            | 2 M   \n",
      "65 | model.layer4.1.bn2          | BatchNorm2d       | 1 K   \n",
      "66 | model.avgpool               | AdaptiveAvgPool2d | 0     \n",
      "67 | model.fc                    | Linear            | 5 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f657a9b8a8c34e168e5d3221b0f7f975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b08fdc73d0f4290a5b2d7729135342d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_acc': tensor(0.1364, device='cuda:0'),\n",
      " 'test_loss': tensor(2.6977, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.6976938247680664, 'test_acc': 0.136408731341362}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = EasyaiTrainer(max_epochs=2, logger=None, weights_summary='full')\n",
    "trainer.fit(EasyaiClassifier())\n",
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
