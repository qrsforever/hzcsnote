{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"> Pytorch Lightning: Classifier Letters </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:28.851500Z",
     "start_time": "2021-01-06T06:23:27.367707Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.9\n",
      "IPython 7.16.1\n",
      "\n",
      "numpy 1.18.5\n",
      "matplotlib 3.2.1\n",
      "torch 1.6.0.dev20200609+cu101\n",
      "torchvision 0.7.0.dev20200609+cu101\n",
      "pytorch_lightning 1.1.2\n",
      "PIL 7.1.2\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%watermark -v -p numpy,matplotlib,torch,torchvision,pytorch_lightning,PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:28.876477Z",
     "start_time": "2021-01-06T06:23:28.854542Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import os\n",
    "import json\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [数据集: emnist_letters: 8j58](https://pan.baidu.com/s/1COMbe7nuW7gS-hDLCT03Hw \"提取码: 8j58\")\n",
    "    \n",
    "   >原始图片是水平翻转并旋转90°的, 需要处理后使用\n",
    "    \n",
    "- 解压到`/data/datasets/cv/EMNIST_Letters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:28.907521Z",
     "start_time": "2021-01-06T06:23:28.879791Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_idx3_ubyte(idx3_ubyte_file):\n",
    "    with open(idx3_ubyte_file, 'rb') as fr:\n",
    "        bin_data = fr.read()\n",
    " \n",
    "    offset = 0\n",
    "    fmt_header = '>iiii'\n",
    "    magic_number, num_images, num_rows, num_cols = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('magic:%d, total: %d, size: %d*%d' % (magic_number, num_images, num_rows, num_cols))\n",
    " \n",
    "    image_size = num_rows * num_cols\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>' + str(image_size) + 'B'\n",
    "    images = np.empty((num_images, num_rows, num_cols))\n",
    "    for i in range(num_images):\n",
    "        # T: transpose\n",
    "        images[i] = np.array(struct.unpack_from(fmt_image, bin_data, offset)).reshape((num_rows, num_cols)).T\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return images\n",
    "\n",
    "def decode_idx1_ubyte(idx1_ubyte_file):\n",
    "    with open(idx1_ubyte_file, 'rb') as fr:\n",
    "        bin_data = fr.read()\n",
    "\n",
    "    offset = 0\n",
    "    fmt_header = '>ii'\n",
    "    magic_number, num_images = struct.unpack_from(fmt_header, bin_data, offset)\n",
    "    print('magic:%d, total: %d' % (magic_number, num_images))\n",
    "\n",
    "    offset += struct.calcsize(fmt_header)\n",
    "    fmt_image = '>B'\n",
    "    labels = np.empty(num_images, dtype=int)\n",
    "    for i in range(num_images):\n",
    "        labels[i] = struct.unpack_from(fmt_image, bin_data, offset)[0]\n",
    "        offset += struct.calcsize(fmt_image)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:28.963895Z",
     "start_time": "2021-01-06T06:23:28.910551Z"
    }
   },
   "outputs": [],
   "source": [
    "class EasyaiClassifier(pl.LightningModule):\n",
    "    def __init__(self, trainer, model, optimizer, scheduler):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.trainer = trainer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        # self.model.cuda()\n",
    "\n",
    "    def _get_lr(self):\n",
    "        return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.cuda()\n",
    "        return self.model(x)\n",
    "\n",
    "    def criterion(self, inputs, targets):\n",
    "        return torch.nn.functional.cross_entropy(inputs, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return {'monitor': 'val_loss', 'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, y_trues, paths = batch\n",
    "        y_preds = self(inputs)\n",
    "        loss = self.criterion(y_preds, y_trues)\n",
    "        acc = (torch.argmax(y_preds, dim=1) == y_trues).float().mean()\n",
    "        log = {'loss': loss, 'acc': acc}\n",
    "        return log\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        log = {\n",
    "            'lr': self._get_lr(),\n",
    "            'train_loss': torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        }\n",
    "        if 'acc' in outputs[0]:\n",
    "            log['train_acc'] = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, y_trues, paths = batch\n",
    "        y_preds = self(inputs)\n",
    "        loss = self.criterion(y_preds, y_trues)\n",
    "        acc = (torch.argmax(y_preds, dim=1) == y_trues).float().mean()\n",
    "        log = {'val_loss': loss, 'val_acc': acc}\n",
    "        return log\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        log = {\n",
    "            'val_loss': torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        }\n",
    "        if 'val_acc' in outputs[0]:\n",
    "            log['val_acc'] = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        self.log_dict(log, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, y_trues, paths = batch\n",
    "        y_preds = self(inputs)\n",
    "        acc = (torch.argmax(y_preds, dim=1) == y_trues).float().mean()\n",
    "        log = {'test_acc': acc}\n",
    "        return log\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        log = {}\n",
    "        if 'test_acc' in outputs[0]:\n",
    "            log['test_acc'] = torch.stack([x['test_acc'] for x in outputs]).mean().numpy()\n",
    "        self.log_dict(log)\n",
    "\n",
    "    def get_progress_bar_dict(self):\n",
    "        items = super().get_progress_bar_dict()\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "\n",
    "    def fit(self, train_loader, valid_loader):\n",
    "        return self.trainer.fit(self, train_loader, valid_loader)\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        return self.trainer.test(self, test_loader)\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        def predict_step(self, batch, batch_idx):\n",
    "            inputs, tags, paths = batch\n",
    "            y_preds = self(inputs)\n",
    "            return list(zip(paths, tags, F.softmax(y_preds, dim=1)))\n",
    "\n",
    "        def predict_epoch_end(self, outputs):\n",
    "            result = {'output':[]}\n",
    "            for item in outputs:\n",
    "                for path, tag, preds in item:\n",
    "                    probs = preds.cpu().numpy().astype(float).tolist()\n",
    "                    result['output'].append({'image_path': path, 'image_id': tag, 'probs': probs})\n",
    "            self.log_dict(result)\n",
    "        try:\n",
    "            _test_step = getattr(self.__class__, 'test_step', None)\n",
    "            _test_epoch_end = getattr(self.__class__, 'test_epoch_end', None)\n",
    "            setattr(self.__class__, 'test_step', predict_step)\n",
    "            setattr(self.__class__, 'test_epoch_end', predict_epoch_end)\n",
    "            return self.trainer.test(self, test_loader, verbose=True)\n",
    "        except Exception as err:\n",
    "            raise err\n",
    "        finally:\n",
    "            if _test_step:\n",
    "                setattr(self.__class__, 'test_step', _test_step)\n",
    "            if _test_epoch_end:\n",
    "                setattr(self.__class__, 'test_epoch_end', _test_epoch_end)\n",
    "\n",
    "class EasyaiTrainer(pl.Trainer):\n",
    "    default_root_dir = '/tmp/emnist_letters'\n",
    "    def __init__(self, logger, callbacks, pretrained=False, *args, **kwargs):\n",
    "        self.resume_path = f'{self.default_root_dir}/checkpoints/best.ckpt'\n",
    "        if pretrained:\n",
    "            if not os.path.exists(self.resume_path):\n",
    "                pretrained = False\n",
    "        os.makedirs(f'{self.default_root_dir}/checkpoints', exist_ok=True)\n",
    "        super().__init__(\n",
    "            logger=logger,\n",
    "            callbacks=callbacks,\n",
    "            num_sanity_val_steps=0,\n",
    "            resume_from_checkpoint=self.resume_path if pretrained else None,\n",
    "            default_root_dir=self.default_root_dir,\n",
    "            *args, **kwargs)\n",
    "        \n",
    "    def on_save_checkpoint(self):\n",
    "        states = super().on_save_checkpoint()\n",
    "        # for v in states.values():\n",
    "        #     if 'best_model_path' in v:\n",
    "        #         shutil.move(v['best_model_path'], self.resume_path)\n",
    "        #         break\n",
    "        return states\n",
    "\n",
    "    def save_checkpoint(self, filepath, weights_only: bool = False): \n",
    "        return super().save_checkpoint(self.resume_path, weights_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-04T10:58:04.731379Z",
     "start_time": "2021-01-04T10:58:04.706434Z"
    }
   },
   "source": [
    "## 数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:28.992385Z",
     "start_time": "2021-01-06T06:23:28.966489Z"
    }
   },
   "outputs": [],
   "source": [
    "class UbyteReaderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source, imgtrans, augtrans=[]):\n",
    "        self.images, self.labels = self.data_reader(source)\n",
    "        self.imgtrans = torchvision.transforms.Compose(imgtrans)\n",
    "        self.augtrans = torchvision.transforms.RandomOrder(augtrans)\n",
    "        \n",
    "    def data_reader(self, path):\n",
    "        image_list = decode_idx3_ubyte(f'{path}-images-idx3-ubyte')\n",
    "        label_list = decode_idx1_ubyte(f'{path}-labels-idx1-ubyte')\n",
    "        return image_list, label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.fromarray(self.images[index]).convert('RGB')\n",
    "        img = self.augtrans(img)\n",
    "        return self.imgtrans(img), self.labels[index] - 1, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:29.015746Z",
     "start_time": "2021-01-06T06:23:28.994863Z"
    }
   },
   "outputs": [],
   "source": [
    "# class JsonReaderDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, source, imgtrans, augtrans=[]):\n",
    "#         self.images, self.labels = self.data_reader(source)\n",
    "#         self.imgtrans = torchvision.transforms.Compose(imgtrans)\n",
    "#         self.augtrans = torchvision.transforms.RandomOrder(augtrans)\n",
    "#         \n",
    "#     def data_reader(self, path):\n",
    "#         image_list = []\n",
    "#         label_list = []\n",
    "#         root = os.path.dirname(path)\n",
    "#         with open(path, 'r') as f:\n",
    "#             items = json.load(f)\n",
    "#             for item in items:\n",
    "#                 image_list.append(os.path.join(root, item['image_path']))\n",
    "#                 label_list.append(item['label'])\n",
    "#         return image_list, label_list\n",
    "# \n",
    "#     def __getitem__(self, index):\n",
    "#         img = Image.open(self.images[index]).convert('RGB')\n",
    "#         img = self.augtrans(img)\n",
    "#         return self.imgtrans(img), self.labels[index], self.images[index]\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:52.257458Z",
     "start_time": "2021-01-06T06:23:29.018213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic:2051, total: 124800, size: 28*28\n",
      "magic:2049, total: 124800\n",
      "magic:2051, total: 20800, size: 28*28\n",
      "magic:2049, total: 20800\n"
     ]
    }
   ],
   "source": [
    "augtrans = [\n",
    "    torchvision.transforms.RandomRotation(degrees=75)\n",
    "]\n",
    "\n",
    "imgtrans = [\n",
    "    torchvision.transforms.Resize((28, 28)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.1362,), std=(0.2893,)),\n",
    "]\n",
    "\n",
    "IMG_ROOT='/data/datasets/cv/EMNIST_Letters'\n",
    "\n",
    "# train_dataset = JsonReaderDataset(\n",
    "#     f'{IMG_ROOT}/emnist-letters-train', # f'{IMG_ROOT}/train.json',\n",
    "#     imgtrans=imgtrans,\n",
    "#     # augtrans=augtrans\n",
    "# )\n",
    "\n",
    "# val_dataset = JsonReaderDataset(\n",
    "#     f'{IMG_ROOT}/val.json',\n",
    "#     imgtrans=imgtrans\n",
    "# )\n",
    "\n",
    "train_dataset = UbyteReaderDataset(\n",
    "    f'{IMG_ROOT}/emnist-letters-train',\n",
    "    imgtrans=imgtrans\n",
    ")\n",
    "\n",
    "val_dataset = UbyteReaderDataset(\n",
    "    f'{IMG_ROOT}/emnist-letters-test',\n",
    "    imgtrans=imgtrans\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=250, drop_last=True, num_workers=8,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=250, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:23:52.855306Z",
     "start_time": "2021-01-06T06:23:52.260701Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "class BBResnet18(torch.nn.Module):\n",
    "    def __init__(self, num_classes, use_gpu=True):\n",
    "        super().__init__()\n",
    "        self.bbmodel = torchvision.models.resnet18(pretrained=True)\n",
    "        self.bbmodel.fc = torch.nn.Linear(self.bbmodel.fc.in_features, num_classes) \n",
    "        # self.use_gpu = use_gpu\n",
    "        # if use_gpu:\n",
    "        #     self.bbmodel.cuda()\n",
    "    def forward(self, x):\n",
    "        # if self.use_gpu:\n",
    "        #     x = x.cuda()\n",
    "        return self.bbmodel(x)\n",
    "\n",
    "model = BBResnet18(26)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    threshold=1e-4,\n",
    "    verbose=False)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=23, gamma=0.1)\n",
    "\n",
    "cb_early_stop = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    min_delta=0.0001)\n",
    "\n",
    "cb_model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=1,\n",
    "    period=2,\n",
    "    save_weights_only=False,\n",
    "    filename=\"best\")\n",
    "\n",
    "trainer = EasyaiTrainer(\n",
    "    logger=False,\n",
    "    callbacks=[cb_early_stop, cb_model_checkpoint],\n",
    "    pretrained=True, \n",
    "    max_epochs=20,\n",
    "    gpus=[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T06:28:19.451347Z",
     "start_time": "2021-01-06T06:23:52.857949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | BBResnet18 | 11.2 M\n",
      "-------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f46772f36a499e9cf3642c55e2fd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifer = EasyaiClassifier(trainer, model, optimizer, scheduler)\n",
    "\n",
    "classifer.fit(train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
