{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T06:39:07.705262Z",
     "start_time": "2020-09-03T06:39:06.860511Z"
    },
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "! [ ! -L /datasets ] && ln -s /data/datasets/cv /datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T14:04:08.870675Z",
     "start_time": "2020-08-31T14:04:08.862741Z"
    }
   },
   "source": [
    "## 需掌握知识点\n",
    "\n",
    "1. 人工智能的应用与价值\n",
    "2. 人工智能作为一次科技浪潮的特点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T11:03:37.318126Z",
     "start_time": "2020-09-08T11:03:19.185513Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "{'label_names': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
      " 'mean': [0.1362, 0.1362, 0.1362],\n",
      " 'num_classes': 10,\n",
      " 'num_records': 10000,\n",
      " 'std': [0.2893, 0.2893, 0.2893]}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   | Name                        | Type              | Params\n",
      "-------------------------------------------------------------------\n",
      "0  | model                       | ResNet            | 11 M  \n",
      "1  | model.conv1                 | Conv2d            | 9 K   \n",
      "2  | model.bn1                   | BatchNorm2d       | 128   \n",
      "3  | model.relu                  | ReLU              | 0     \n",
      "4  | model.maxpool               | MaxPool2d         | 0     \n",
      "5  | model.layer1                | Sequential        | 147 K \n",
      "6  | model.layer1.0              | BasicBlock        | 73 K  \n",
      "7  | model.layer1.0.conv1        | Conv2d            | 36 K  \n",
      "8  | model.layer1.0.bn1          | BatchNorm2d       | 128   \n",
      "9  | model.layer1.0.relu         | ReLU              | 0     \n",
      "10 | model.layer1.0.conv2        | Conv2d            | 36 K  \n",
      "11 | model.layer1.0.bn2          | BatchNorm2d       | 128   \n",
      "12 | model.layer1.1              | BasicBlock        | 73 K  \n",
      "13 | model.layer1.1.conv1        | Conv2d            | 36 K  \n",
      "14 | model.layer1.1.bn1          | BatchNorm2d       | 128   \n",
      "15 | model.layer1.1.relu         | ReLU              | 0     \n",
      "16 | model.layer1.1.conv2        | Conv2d            | 36 K  \n",
      "17 | model.layer1.1.bn2          | BatchNorm2d       | 128   \n",
      "18 | model.layer2                | Sequential        | 525 K \n",
      "19 | model.layer2.0              | BasicBlock        | 230 K \n",
      "20 | model.layer2.0.conv1        | Conv2d            | 73 K  \n",
      "21 | model.layer2.0.bn1          | BatchNorm2d       | 256   \n",
      "22 | model.layer2.0.relu         | ReLU              | 0     \n",
      "23 | model.layer2.0.conv2        | Conv2d            | 147 K \n",
      "24 | model.layer2.0.bn2          | BatchNorm2d       | 256   \n",
      "25 | model.layer2.0.downsample   | Sequential        | 8 K   \n",
      "26 | model.layer2.0.downsample.0 | Conv2d            | 8 K   \n",
      "27 | model.layer2.0.downsample.1 | BatchNorm2d       | 256   \n",
      "28 | model.layer2.1              | BasicBlock        | 295 K \n",
      "29 | model.layer2.1.conv1        | Conv2d            | 147 K \n",
      "30 | model.layer2.1.bn1          | BatchNorm2d       | 256   \n",
      "31 | model.layer2.1.relu         | ReLU              | 0     \n",
      "32 | model.layer2.1.conv2        | Conv2d            | 147 K \n",
      "33 | model.layer2.1.bn2          | BatchNorm2d       | 256   \n",
      "34 | model.layer3                | Sequential        | 2 M   \n",
      "35 | model.layer3.0              | BasicBlock        | 919 K \n",
      "36 | model.layer3.0.conv1        | Conv2d            | 294 K \n",
      "37 | model.layer3.0.bn1          | BatchNorm2d       | 512   \n",
      "38 | model.layer3.0.relu         | ReLU              | 0     \n",
      "39 | model.layer3.0.conv2        | Conv2d            | 589 K \n",
      "40 | model.layer3.0.bn2          | BatchNorm2d       | 512   \n",
      "41 | model.layer3.0.downsample   | Sequential        | 33 K  \n",
      "42 | model.layer3.0.downsample.0 | Conv2d            | 32 K  \n",
      "43 | model.layer3.0.downsample.1 | BatchNorm2d       | 512   \n",
      "44 | model.layer3.1              | BasicBlock        | 1 M   \n",
      "45 | model.layer3.1.conv1        | Conv2d            | 589 K \n",
      "46 | model.layer3.1.bn1          | BatchNorm2d       | 512   \n",
      "47 | model.layer3.1.relu         | ReLU              | 0     \n",
      "48 | model.layer3.1.conv2        | Conv2d            | 589 K \n",
      "49 | model.layer3.1.bn2          | BatchNorm2d       | 512   \n",
      "50 | model.layer4                | Sequential        | 8 M   \n",
      "51 | model.layer4.0              | BasicBlock        | 3 M   \n",
      "52 | model.layer4.0.conv1        | Conv2d            | 1 M   \n",
      "53 | model.layer4.0.bn1          | BatchNorm2d       | 1 K   \n",
      "54 | model.layer4.0.relu         | ReLU              | 0     \n",
      "55 | model.layer4.0.conv2        | Conv2d            | 2 M   \n",
      "56 | model.layer4.0.bn2          | BatchNorm2d       | 1 K   \n",
      "57 | model.layer4.0.downsample   | Sequential        | 132 K \n",
      "58 | model.layer4.0.downsample.0 | Conv2d            | 131 K \n",
      "59 | model.layer4.0.downsample.1 | BatchNorm2d       | 1 K   \n",
      "60 | model.layer4.1              | BasicBlock        | 4 M   \n",
      "61 | model.layer4.1.conv1        | Conv2d            | 2 M   \n",
      "62 | model.layer4.1.bn1          | BatchNorm2d       | 1 K   \n",
      "63 | model.layer4.1.relu         | ReLU              | 0     \n",
      "64 | model.layer4.1.conv2        | Conv2d            | 2 M   \n",
      "65 | model.layer4.1.bn2          | BatchNorm2d       | 1 K   \n",
      "66 | model.avgpool               | AdaptiveAvgPool2d | 0     \n",
      "67 | model.fc                    | Linear            | 5 K   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b7731f82f540f3a5ce82afda7ce00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da7ecb7c6acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;31m# 训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;31m# 评估\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hzcsk12/hzcsnote/pyr/app/k12ai/__init__.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentering\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;31m# process epoch outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_accumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_accumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# checkpoint callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch_end\u001b[0;34m(self, epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# run training_epoch_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# a list with a result per optimizer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mepoch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hzcsk12/hzcsnote/pyr/app/k12ai/__init__.py\u001b[0m in \u001b[0;36mtraining_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'progress_bar'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hzcsk12/hzcsnote/pyr/app/k12ai/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mavg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'progress_bar'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "from pyr.app.k12ai import EasyaiClassifier, EasyaiTrainer\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CustomClassifier(EasyaiClassifier):\n",
    "\n",
    "    ##########################################################################\n",
    "    ####### Dataset ######\n",
    "    ##########################################################################\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        准备数据集, 从磁盘上加载数据集, 不同数据集的描述格式可能不一样, 一般有json/xml/csv等描述格式,\n",
    "        也可能直接是图片目录, 所有这些格式的处理可以在这个接口完成.\n",
    "        \n",
    "        预置数据集: mnist, cifar10, flowers, fruits, dogcat, chestxray\n",
    "            \n",
    "        返回:\n",
    "            以下几种方式任意一种:\n",
    "            1. EasyaiDataset实例, 表明只进行训练(只返回了训练数据集实例)\n",
    "            2. EasyaiDataset实例列表, 当列表长度为2时, 说明还要进行训练的校验, 当列表长度为3时, 说明还要进行测试评估.\n",
    "            3. EasyaiDataset实例字典, 如: {'train': EasyaiDataset, 'val': EasyaiDataset, 'test':EasyaiDataset}\n",
    "        \"\"\"\n",
    "        return self.load_mnist()\n",
    "    \n",
    "    ##########################################################################\n",
    "    ####### Model ######\n",
    "    ##########################################################################\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        构建模型\n",
    "        \n",
    "        返回:\n",
    "            模型实例\n",
    "        \"\"\"\n",
    "        return self.load_resnet18(num_classes=10)\n",
    "    \n",
    "    ##########################################################################\n",
    "    ####### Hypes Parameters ######\n",
    "    ##########################################################################\n",
    "    def configure_optimizer(self, model):\n",
    "        \"\"\"\n",
    "        配置优化器\n",
    "        \n",
    "        返回:\n",
    "            optimizer\n",
    "        \"\"\"\n",
    "        return self.adam(model.parameters(), base_lr=0.001)\n",
    "\n",
    "    def configure_scheduler(self, optimizer):\n",
    "        \"\"\"\n",
    "        配置学习率衰减策略\n",
    "        \n",
    "        参数:\n",
    "            optimizer: 优化器(通过configure_optimizer配置得到的)\n",
    "        \n",
    "        返回:\n",
    "            scheduler: 学习率策略实例或list\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.step_lr(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    ##########################################################################\n",
    "    ####### Trainer: Train ######\n",
    "    ##########################################################################\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        训练数据集批量控制加载器, 可以设置批量的大小, 是否对数据进行洗牌(shuffle)等\n",
    "        \n",
    "        返回:\n",
    "            DataLoader: 数据加载器\n",
    "        \"\"\"\n",
    "        return self.get_dataloader(\n",
    "            phase='train',  # [M] 训练DataLoader\n",
    "            input_size=28,  # [O] 输入到模型的图片大小\n",
    "            batch_size=32,  # [O] 输入到模型的最大批量数\n",
    "            # data_augment=[\n",
    "            #     self.random_brightness(factor=0.3),\n",
    "            #     self.random_rotation(degrees=30)\n",
    "            # ], # [O] 数据增强\n",
    "            random_order=False, # [O] 数据增强变换方法顺序是否随机\n",
    "            normalize=True,     # [O] 是否对输入的数据进行归一化\n",
    "            drop_last=False,    # [O] 一次epoch中最后一次批量数可能不足, 是否丢弃\n",
    "            shuffle=False)      # [O] 加载的数据是否随机洗牌\n",
    "     \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        训练过程中, 迭代一次batch数据, 就会触发一次training_step的调用,训练,统计metrics\n",
    "        \n",
    "        参数:\n",
    "            batch: 一个batch的数据内容, 一般包括图片(image), 图片标签(labels), 图片路径(path).\n",
    "                具体batch中内容受prepare_dataset接口的实现会有所不同\n",
    "            batch_idx: 本轮epoch批量迭代次数\n",
    "            \n",
    "        返回:\n",
    "            metrics: 必须包含loss关键字, log(日志模块)和progress_bar(进度条显示)是可选的\n",
    "        \"\"\"\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.cross_entropy(y_hat, y, reduction='mean') # 损失方法\n",
    "        with torch.no_grad():\n",
    "            accuracy = (torch.argmax(y_hat, axis=1) == y).float().mean() # 计算争取率\n",
    "        return {'loss': loss, 'progress_bar': {'acc': accuracy}}\n",
    "\n",
    "    ##########################################################################\n",
    "    ####### Trainer: validation ######\n",
    "    ##########################################################################\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        同train_dataloader\n",
    "        \"\"\"\n",
    "        return self.get_dataloader(\n",
    "            phase='val',\n",
    "            input_size=28,\n",
    "            batch_size=32,\n",
    "            normalize=True,\n",
    "            drop_last=False,\n",
    "            shuffle=False)\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        同train_step\n",
    "        \"\"\"\n",
    "        x, y, _ = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.cross_entropy(y_hat, y, reduction='mean')\n",
    "        accuracy = (torch.argmax(y_hat, axis=1) == y).float().mean()\n",
    "        return {'loss': loss, 'acc': accuracy}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "        return {'progress_bar': {'val_loss': avg_loss, 'val_acc': avg_acc}}\n",
    "    \n",
    "    ##########################################################################\n",
    "    ####### Trainer: test ######\n",
    "    ##########################################################################\n",
    "    def test_dataloader(self):\n",
    "        return self.get_dataloader(\n",
    "            phase='test',\n",
    "            input_size=28,\n",
    "            batch_size=32,\n",
    "            random_order=False,\n",
    "            normalize=True,\n",
    "            drop_last=False,\n",
    "            shuffle=False\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y, p = batch\n",
    "        y_hat = self(x)\n",
    "        accuracy = (torch.argmax(y_hat, axis=1) == y).float().mean()\n",
    "        return {'acc': accuracy}\n",
    "        \n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "        return {'test_acc': avg_acc}\n",
    "    \n",
    "    \n",
    "trainer = EasyaiTrainer(\n",
    "    max_epochs=10, # 训练过程遍历完整数据集的总次数(epoch)\n",
    "    resume=False,  # True: 模型继续上次训练(模型必须没有改变)\n",
    "    log_rate=2,    # 日志打印的频率, 单位是迭代次数(iteration step) \n",
    "    model_summary='full', # 打印模型顶层Memory信息\n",
    "    model_ckpt={'monitor': 'val_loss', 'period': 2, 'mode': 'min'},\n",
    "    early_stop={'monitor': 'val_acc', 'patience': 3, 'mode': 'max'}\n",
    ")\n",
    "\n",
    "model = CustomClassifier()\n",
    "\n",
    "# 训练\n",
    "trainer.fit(model)\n",
    "\n",
    "# 评估\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
